{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#custom-skill-developer-guide","title":"Custom Skill Developer Guide","text":"<p>The comprehensive guide to building production Agent Skills \u2014 Claude Code-first, cross-platform compatible via the agentskills.io open standard.</p> <p>17 chapters. 55,000+ words. From your first skill to meta-skill routers, quality scoring systems, and multi-skill pipeline orchestration.</p> <p>Built from the experience of creating 20+ production Agent Skills for business operations.</p> 17 Chapters 55K Words 200 Concepts 20+ Skills Built"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"Section Chapters What You'll Build Foundations 1-3 Your first working skill \u2014 from zero to production Skill Anatomy 4-7 Production-quality skill definitions with quality scoring Advanced Patterns 8-11 Meta-skill routers, token-efficient designs, pipelines Specialized Skills 12-14 Data transformation, code generation, and analysis skills Deployment 15-17 Installation, testing, and distribution to other users <p>Start Chapter 1</p>"},{"location":"#who-this-is-for","title":"Who This Is For","text":"<p>You already use AI agents and existing skills. Now you want to create your own \u2014 whether that's a skill for your specific domain, a meta-skill that routes to multiple sub-skills, or a full pipeline that chains skills together in dependency order.</p> <p>No traditional programming background required. If you can write markdown and think in systems, you can build skills.</p>"},{"location":"#the-cognify-ecosystem","title":"The Cognify Ecosystem","text":"<p>This guide is part of a three-layer stack for the Agent Skills ecosystem:</p> Layer Project Purpose Education Custom Skill Developer Teaches how to build production skills Tooling Skill Quality Analyzer Audits skills against the spec (100-point rubric) Product Cognify Skills 19 production business operations skills"},{"location":"#cross-platform-compatibility","title":"Cross-Platform Compatibility","text":"<p>Skills built with this guide work everywhere the agentskills.io standard is supported:</p> <p>Claude Code | Claude.ai | VS Code | Cursor | OpenAI Codex | Gemini CLI | and 20+ more</p>"},{"location":"#want-custom-skills-built-for-your-business","title":"Want Custom Skills Built for Your Business?","text":"<p>I build production Agent Skills for companies \u2014 workflow automation, sales enablement, operations, and domain-specific AI expertise packaged as reusable skills.</p> <p>Daniel Yarmoluk \u2014 GitHub | LinkedIn | daniel.yarmoluk@gmail.com</p> <p>Get Started</p>"},{"location":"course-description/","title":"Course Description","text":""},{"location":"course-description/#title","title":"Title","text":"<p>Custom Skill Developer: Building Autonomous Agents for Claude Code</p>"},{"location":"course-description/#audience","title":"Audience","text":"<p>Intermediate to advanced users of Claude Code who want to extend its capabilities by creating custom skills \u2014 autonomous agent definitions that Claude executes step-by-step to accomplish complex, repeatable tasks. Readers should be comfortable with markdown, YAML, and basic command-line operations. No traditional programming experience is required; this guide is designed for AI-first builders who work at the speed of thought.</p>"},{"location":"course-description/#prerequisites","title":"Prerequisites","text":"<ul> <li>Working installation of Claude Code CLI</li> <li>Familiarity with markdown syntax</li> <li>Basic understanding of YAML configuration files</li> <li>A GitHub account for publishing and distribution</li> <li>Experience using at least 2-3 existing Claude Code skills</li> </ul>"},{"location":"course-description/#course-summary","title":"Course Summary","text":"<p>This knowledge base teaches you to design, build, test, and deploy custom Claude Code skills. Skills are markdown-defined autonomous agents that give Claude structured workflows, quality standards, and domain expertise. You will learn the complete skill anatomy \u2014 from YAML frontmatter and trigger conditions to multi-step workflows with quality scoring rubrics. The guide covers advanced patterns including meta-skill routers (which consolidate multiple sub-skills under one entry point to work within Claude Code's 30-skill limit), token-efficient lazy loading, session logging for continuity across context windows, and pipeline orchestration where skills chain together in dependency order. By the end, you will be able to create production-quality skills for any domain and distribute them to other Claude Code users.</p>"},{"location":"course-description/#topics-covered","title":"Topics Covered","text":"<ol> <li>Skill Fundamentals \u2014 What skills are, how Claude Code loads and executes them, the relationship between skills and the system prompt</li> <li>The Skill Ecosystem \u2014 Surveying existing skill categories (book generation, analysis, specialized), understanding the 30-skill limit, skill discovery mechanisms</li> <li>Building Your First Skill \u2014 Creating a minimal SKILL.md, testing it locally, iterating on behavior</li> <li>SKILL.md Deep Dive \u2014 Complete anatomy of the skill definition file: frontmatter schema, section conventions, step numbering</li> <li>YAML Frontmatter \u2014 The <code>name</code>, <code>description</code>, <code>license</code>, and <code>allowed-tools</code> fields; how descriptions appear in Claude's system prompt</li> <li>Workflow Design \u2014 Step 0 environment setup, sequential vs. parallel steps, user dialog triggers, conditional branching, error handling</li> <li>Quality Scoring Systems \u2014 Designing 1-100 point rubrics, weighted sub-categories, threshold-based proceed/stop logic, quality gates</li> <li>Meta-Skill Routing \u2014 Consolidating related skills under a router, keyword-based routing tables, the <code>references/</code> directory pattern, decision trees for ambiguous requests</li> <li>Token Efficiency \u2014 Lazy loading of reference documents, tiered information retrieval (MCP \u2192 shell \u2192 file read), skip-if-complete detection, minimal context strategies</li> <li>Session Logging \u2014 Writing structured session logs, enabling cross-session continuity, log format conventions, state tracking with JSON files</li> <li>Pipeline Orchestration \u2014 Chaining skills in dependency order, checkpoint patterns, the 12-step intelligent textbook pipeline as a case study</li> <li>Data Format Skills \u2014 Skills that transform data (CSV \u2192 JSON, learning graphs, metadata schemas), working with Python helper scripts</li> <li>Code Generation Skills \u2014 Skills that produce executable code (MicroSims, scripts, configurations), template patterns, output validation</li> <li>Analysis &amp; Validation Skills \u2014 Skills that score, audit, or report on content quality, DAG validation, metrics generation</li> <li>Installation &amp; Registry \u2014 Global vs. project-local installation, symlink patterns, the skill listing system, MCP server integration</li> <li>Testing &amp; Debugging \u2014 Manual testing workflows, common failure modes, debugging skill behavior, iteration strategies</li> <li>Publishing &amp; Distribution \u2014 Packaging skills for sharing, GitHub-based distribution, versioning conventions, documentation requirements</li> </ol>"},{"location":"course-description/#learning-outcomes-blooms-taxonomy","title":"Learning Outcomes (Bloom's Taxonomy)","text":"<p>By completing this knowledge base, learners will be able to:</p> <ul> <li>Remember: Identify the required components of a SKILL.md file and list the standard sections</li> <li>Understand: Explain how Claude Code discovers, loads, and executes skill definitions during a session</li> <li>Apply: Create a complete, functional skill that follows all conventions and passes quality validation</li> <li>Analyze: Diagnose why a skill is not behaving as expected by tracing execution through its workflow steps</li> <li>Evaluate: Assess whether a skill design is token-efficient, maintainable, and follows best practices using the quality scoring framework</li> <li>Create: Design and publish a meta-skill router that consolidates multiple related sub-skills with lazy-loaded reference documents</li> </ul>"},{"location":"course-description/#estimated-scope","title":"Estimated Scope","text":"<ul> <li>17 chapters across 5 sections</li> <li>~200 concepts in the learning dependency graph</li> <li>Target reading level: professional/technical (accessible to non-engineers who build with AI)</li> <li>Interactive elements: MicroSims for skill workflow visualization, routing decision trees, quality score calculators</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#getting-started","title":"Getting Started","text":""},{"location":"faq/#what-is-an-agent-skill","title":"What is an Agent Skill?","text":"<p>A skill is a folder containing a <code>SKILL.md</code> file \u2014 a markdown document with YAML metadata and structured instructions that teaches an AI agent how to perform a specific task. Skills work across Claude Code, Claude.ai, VS Code, Cursor, OpenAI Codex, Gemini CLI, and any platform that supports the agentskills.io open standard.</p>"},{"location":"faq/#do-i-need-to-know-how-to-code-to-build-skills","title":"Do I need to know how to code to build skills?","text":"<p>No. Skills are written in markdown and YAML. If you can write structured text and think in sequences, you can build skills. Some advanced skills include Python scripts, but most production skills are pure markdown.</p>"},{"location":"faq/#how-is-a-skill-different-from-a-prompt","title":"How is a skill different from a prompt?","text":"<p>A prompt is ephemeral \u2014 it disappears after the conversation. A skill is persistent, structured, quality-gated, and reusable. Skills survive across sessions, enforce standards through scoring rubrics, and produce consistent output every time. See Chapter 1 for the full comparison.</p>"},{"location":"faq/#where-do-i-put-my-skill-files","title":"Where do I put my skill files?","text":"<p>Two options:</p> <ul> <li>Global: <code>~/.claude/skills/my-skill/SKILL.md</code> \u2014 available in every project</li> <li>Project-local: <code>.github/skills/my-skill/SKILL.md</code> \u2014 available only in that project (and portable across platforms via the open standard)</li> </ul>"},{"location":"faq/#building-skills","title":"Building Skills","text":""},{"location":"faq/#whats-the-minimum-viable-skill","title":"What's the minimum viable skill?","text":"<p>A directory with a single <code>SKILL.md</code> file containing YAML frontmatter with <code>name</code> and <code>description</code>. That's it. See Chapter 3 for a step-by-step walkthrough.</p> <pre><code>---\nname: my-skill\ndescription: Does X when the user asks for Y.\n---\n\n# My Skill\n\nStep 1: Do the thing.\n</code></pre>"},{"location":"faq/#how-long-should-my-skillmd-be","title":"How long should my SKILL.md be?","text":"<p>Under 500 lines. The agentskills.io spec recommends keeping the main file concise and moving detailed reference material to the <code>references/</code> directory. The progressive disclosure model means Claude only loads the full SKILL.md when the skill activates \u2014 but shorter files mean less context consumed.</p>"},{"location":"faq/#whats-the-difference-between-name-and-the-h1-title","title":"What's the difference between <code>name</code> and the H1 title?","text":"<p>The <code>name</code> field in frontmatter is machine-readable \u2014 Claude uses it for skill discovery and routing. It must be lowercase kebab-case and match the directory name. The H1 title is human-readable \u2014 it's what people see when they open the file. They serve different audiences.</p>"},{"location":"faq/#when-should-i-add-quality-scoring","title":"When should I add quality scoring?","text":"<p>Always, if the skill produces deliverable output. Quality scoring turns a skill from \"do the thing\" into \"do the thing and verify it meets standards.\" Skills without quality gates produce inconsistent results across invocations. See Chapter 7 for rubric design patterns.</p>"},{"location":"faq/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"faq/#what-is-a-meta-skill","title":"What is a meta-skill?","text":"<p>A meta-skill is a router that consolidates multiple related skills under one entry point. Instead of registering 14 separate visualization skills, you register one <code>microsim-generator</code> meta-skill that routes to the right sub-skill based on keywords in the request. See Chapter 8.</p>"},{"location":"faq/#why-does-the-30-skill-limit-exist","title":"Why does the 30-skill limit exist?","text":"<p>Every loaded skill's <code>name</code> and <code>description</code> are injected into the system prompt at session start. With 30 skills averaging ~100 tokens each, that's ~3,000 tokens of the context window consumed before you even send a message. The limit keeps the system prompt budget manageable.</p>"},{"location":"faq/#how-do-i-make-skills-work-across-sessions","title":"How do I make skills work across sessions?","text":"<p>Session logging. Write structured logs to a <code>logs/</code> directory recording what was completed, what's pending, and the current state. When a new session starts, the skill reads the log and resumes from where it left off. See Chapter 10.</p>"},{"location":"faq/#can-skills-call-other-skills","title":"Can skills call other skills?","text":"<p>Not directly. But skills can produce output files that serve as input to other skills \u2014 this is pipeline orchestration. The 12-step intelligent textbook pipeline is the canonical example of skills chaining through file contracts. See Chapter 11.</p>"},{"location":"faq/#token-efficiency","title":"Token Efficiency","text":""},{"location":"faq/#how-many-tokens-does-a-skill-cost","title":"How many tokens does a skill cost?","text":"<p>Three tiers:</p> <ul> <li>Tier 1 \u2014 Always loaded: ~100 tokens per skill (name + description in the registry)</li> <li>Tier 2 \u2014 On activation: &lt;5,000 tokens (full SKILL.md body)</li> <li>Tier 3 \u2014 On demand: Variable (reference files loaded only when a step needs them)</li> </ul>"},{"location":"faq/#whats-the-most-common-token-efficiency-mistake","title":"What's the most common token efficiency mistake?","text":"<p>Loading everything upfront. New skill authors put all reference material in the main SKILL.md instead of splitting it into <code>references/</code> files. A 2,000-line SKILL.md consumes ~8,000 tokens on every activation. Split it into a 200-line SKILL.md + 5 focused reference files, and you'll use ~1,000 tokens on activation plus ~500-800 tokens per reference file loaded on demand.</p>"},{"location":"faq/#deployment","title":"Deployment","text":""},{"location":"faq/#how-do-i-share-skills-with-others","title":"How do I share skills with others?","text":"<p>Package them in a GitHub repository with an install script. Users clone the repo and run the script, which creates symlinks from the skill directories to their local skill path. See Chapter 17.</p>"},{"location":"faq/#what-license-should-i-use","title":"What license should I use?","text":"<p>Apache 2.0 is the ecosystem standard \u2014 it's what Anthropic uses for their own skills. Use MIT if you want maximum permissiveness. Use a proprietary license if you're selling skill packs commercially.</p>"},{"location":"faq/#do-skills-work-outside-claude-code","title":"Do skills work outside Claude Code?","text":"<p>Yes. The agentskills.io open standard is supported by Claude Code, Claude.ai, VS Code, Cursor, OpenAI Codex, Gemini CLI, and 20+ other platforms. Skills placed in <code>.github/skills/</code> follow the cross-platform convention.</p>"},{"location":"faq/#this-guide","title":"This Guide","text":""},{"location":"faq/#who-wrote-this","title":"Who wrote this?","text":"<p>Daniel Yarmoluk \u2014 GitHub | LinkedIn | daniel.yarmoluk@gmail.com</p> <p>Built from the experience of creating 20+ production Agent Skills for business operations as part of the Cognify ecosystem.</p>"},{"location":"faq/#can-i-contribute","title":"Can I contribute?","text":"<p>Yes. The guide is Apache 2.0 licensed. Submit issues or pull requests on GitHub.</p>"},{"location":"faq/#i-want-custom-skills-built-for-my-business-how","title":"I want custom skills built for my business. How?","text":"<p>Reach out: daniel.yarmoluk@gmail.com</p>"},{"location":"glossary/","title":"Glossary","text":"<p>Definitions follow the ISO 11179 metadata registry standard: precise, concise, distinct, non-circular, and free of business rules.</p> Agent Skill A directory containing a SKILL.md file and optional supporting resources that teaches an AI agent how to perform a specific task through structured instructions. Activation The process by which an agent loads a skill's full SKILL.md body into context after determining relevance from the skill's description. allowed-tools An optional YAML frontmatter field specifying which tools a skill is permitted to use during execution. Experimental. Assets Directory An optional <code>assets/</code> subdirectory within a skill containing static files used in output \u2014 templates, images, schemas, fonts. Bloom's Taxonomy A six-level classification of cognitive objectives (Remember, Understand, Apply, Analyze, Evaluate, Create) used to design learning outcomes and quality scoring rubrics. Checkpoint A designated point in a multi-skill pipeline where execution pauses for user review before proceeding to the next stage. Context Window The maximum number of tokens an AI model can process in a single session, shared across system prompt, conversation history, skill instructions, and generated output. DAG (Directed Acyclic Graph) A graph structure where edges have direction and no cycles exist. Used for learning concept dependencies and skill pipeline ordering. Description Field The required YAML frontmatter field (max 1,024 characters) that tells agents what a skill does and when to use it. Loaded into the skill registry at session start. Discovery The startup phase where an agent scans skill directories and loads only the name and description of each available skill into its registry. Frontmatter The YAML metadata block at the top of a SKILL.md file, delimited by <code>---</code>, containing required fields (<code>name</code>, <code>description</code>) and optional fields (<code>license</code>, <code>compatibility</code>, <code>metadata</code>, <code>allowed-tools</code>). ISO 11179 An international standard for metadata registries that defines five qualities of good definitions: precise, concise, distinct, non-circular, and free of business rules. Lazy Loading A token efficiency pattern where reference documents are read into context only when a specific workflow step requires them, rather than at skill activation. MCP (Model Context Protocol) An open protocol that tells agents what tools exist and how to call them. Complementary to skills: MCP provides tool discovery, skills provide workflow knowledge. Meta-Skill A skill that routes requests to one of several sub-skills based on keyword matching. Uses a <code>references/</code> directory containing guide files for each variant. MicroSim An interactive browser-based simulation built with JavaScript libraries (p5.js, Chart.js, vis-network) for educational visualization. Name Field The required YAML frontmatter field (max 64 characters) that identifies a skill. Must be lowercase with hyphens only and match the parent directory name. Pipeline A sequence of skills executed in dependency order where each skill's output serves as the next skill's input. Progressive Disclosure The three-tier architecture for managing skill token costs: Tier 1 metadata (~100 tokens) loaded always, Tier 2 instructions (&lt;5,000 tokens) loaded on activation, Tier 3 resources loaded on demand. Quality Gate A threshold-based checkpoint within a skill where output is scored against a rubric and execution continues only if the score meets the minimum (typically 70-85 out of 100). Quality Scoring Rubric A structured evaluation framework within a skill that decomposes output quality into weighted criteria summing to 100 points, with defined thresholds for proceed/stop decisions. References Directory An optional <code>references/</code> subdirectory within a skill containing documentation that agents load on demand during execution. Routing Table A mapping within a meta-skill that associates keywords or request patterns with specific reference guide files in the <code>references/</code> directory. Scripts Directory An optional <code>scripts/</code> subdirectory within a skill containing executable code (Python, Bash, JavaScript) that agents run during workflow steps. Session Logging The practice of writing structured log files to a <code>logs/</code> directory recording timing, files created, quality scores, and decisions made \u2014 enabling cross-session continuity. Skill Collection A repository containing multiple related skills packaged together with a shared install script and documentation. Skill Registry The in-memory index an agent builds at startup from all available skill names and descriptions, used to match incoming requests to relevant skills. SKILL.md The required markdown file defining a skill's metadata, instructions, and workflow. Contains YAML frontmatter followed by structured markdown content. Skip-if-Complete A token efficiency pattern where a workflow step checks for existing output before executing, avoiding redundant work when resuming from a prior session. Step 0 The conventional first step in a skill workflow dedicated to environment setup \u2014 detecting project context, validating prerequisites, and prompting for missing information. System Prompt Budget The portion of the context window allocated to system-level instructions, including all loaded skill descriptions. The 30-skill limit exists to keep this budget manageable. 30-Skill Limit The maximum number of skills that can be loaded into a single Claude Code session, determined by the system prompt token budget. Token Efficiency The practice of minimizing unnecessary token consumption within skills through progressive disclosure, lazy loading, skip-if-complete detection, and concise instruction writing. User Dialog Trigger A condition defined within a skill workflow that causes the agent to pause and ask the user a question before proceeding \u2014 typically at quality gates, destructive actions, or ambiguous decision points. Workflow Step A numbered section within a SKILL.md body containing specific instructions for one logical unit of work within the skill's execution sequence."},{"location":"references/","title":"References","text":"<p>Curated references organized by chapter. Each entry includes a relevance note explaining why it matters for skill development.</p>"},{"location":"references/#foundational-reading","title":"Foundational Reading","text":"<p>These sources provide the intellectual and technical foundation for the Agent Skills ecosystem.</p>"},{"location":"references/#dario-amodei","title":"Dario Amodei","text":"<p>Machines of Loving Grace: How AI Could Transform the World for the Better Dario Amodei, October 2024</p> <p>A 14,000-word vision of how powerful AI compresses decades of scientific, economic, and governance progress into years. The essay's core argument \u2014 that AI systems become exponentially more valuable when they can be specialized for domains while maintaining general reasoning \u2014 is the philosophical foundation for why skills exist. Skills are the mechanism by which a general-purpose model becomes a domain expert.</p> <p>The Adolescence of Technology: Confronting and Overcoming the Risks of Powerful AI Dario Amodei, January 2026</p> <p>The sequel to \"Machines of Loving Grace\" catalogs five civilization-level risks from powerful AI and proposes concrete countermeasures. Relevant to skill development because it frames the safety argument for structured, quality-gated agent behavior \u2014 skills constrain AI output to validated workflows rather than unbounded generation.</p>"},{"location":"references/#the-agent-skills-standard","title":"The Agent Skills Standard","text":""},{"location":"references/#anthropic-official","title":"Anthropic Official","text":"<p>Introducing Agent Skills Anthropic, October 2025 (updated December 2025)</p> <p>The official product announcement for Agent Skills. Explains the product vision: skills as \"onboarding guides\" that transform a general-purpose agent into a domain specialist. Covers Claude.ai, Claude Code, the Agent SDK, and API integration.</p> <p>Equipping Agents for the Real World with Agent Skills Barry Zhang, Keith Lazuka, Mahesh Murag \u2014 Anthropic Engineering, October 2025</p> <p>The technical deep-dive into the skills architecture. Covers the SKILL.md format, progressive disclosure (the 3-tier token model), bundled scripts, security considerations, and the relationship between Skills and MCP. This is the most important technical reference for skill developers.</p> <p>Agent Skills Open Standard Specification Anthropic, December 2025</p> <p>The formal specification for the Agent Skills format. Defines frontmatter requirements (<code>name</code>, <code>description</code>, <code>license</code>, <code>compatibility</code>, <code>metadata</code>, <code>allowed-tools</code>), directory structure (<code>scripts/</code>, <code>references/</code>, <code>assets/</code>), progressive disclosure model, and validation rules. Cross-platform: Claude Code, Claude.ai, VS Code, Cursor, OpenAI Codex, Gemini CLI, and 20+ platforms.</p> <p>Agent Skills Documentation Anthropic</p> <p>Official documentation for using and creating skills. Covers installation, configuration, and authoring best practices.</p> <p>Agent Skills Best Practices Anthropic</p> <p>Anthropic's official authoring guidance for writing effective skills \u2014 conciseness, degrees of freedom, progressive disclosure, and common patterns.</p>"},{"location":"references/#community-analysis","title":"Community Analysis","text":"<p>Agent Skills: Anthropic's Next Bid to Define AI Standards The New Stack</p> <p>Strategic analysis of how Agent Skills parallels MCP as an industry standard play. Frames skills as the \"what to do\" complement to MCP's \"what tools exist.\"</p> <p>Agent Skills \u2014 Simon Willison Simon Willison, December 2025</p> <p>Technical breakdown of the open standard release. Willison's analysis is consistently the most precise technical commentary in the AI ecosystem.</p>"},{"location":"references/#by-chapter","title":"By Chapter","text":""},{"location":"references/#chapters-1-3-foundations","title":"Chapters 1-3: Foundations","text":"Reference Relevance Agent Skills Specification The formal format definition \u2014 required reading for all skill authors Anthropic Skills Repository Official example skills demonstrating patterns across creative, technical, and enterprise domains What Are Skills? The conceptual overview \u2014 discovery, activation, execution model Claude Code Documentation How Claude Code discovers and loads skills at session start"},{"location":"references/#chapters-4-7-skill-anatomy","title":"Chapters 4-7: Skill Anatomy","text":"Reference Relevance SKILL.md Format Specification Frontmatter field constraints, body content guidelines, validation rules Skill Creator \u2014 Anthropic Anthropic's own meta-skill for creating skills \u2014 demonstrates anatomy best practices Bloom's Taxonomy (2001 Revision) The six cognitive levels (Remember through Create) used in quality scoring rubric design ISO 11179 Metadata Registry Standard The standard for precise, concise, distinct, non-circular definitions \u2014 used in glossary and rubric design"},{"location":"references/#chapters-8-11-advanced-patterns","title":"Chapters 8-11: Advanced Patterns","text":"Reference Relevance Progressive Disclosure in Skills The 3-tier token model: metadata (~100 tokens), instructions (&lt;5,000 tokens), resources (as needed) MCP \u2014 Model Context Protocol The complementary protocol to skills \u2014 MCP provides tool discovery, skills provide workflow knowledge Dan McCreary \u2014 Intelligent Textbook Methodology The 12-step pipeline that demonstrated multi-skill orchestration at scale (19 skills, 200+ concepts per textbook) Directed Acyclic Graphs \u2014 Wikipedia The data structure underlying learning graphs, dependency chains, and pipeline orchestration"},{"location":"references/#chapters-12-14-specialized-skills","title":"Chapters 12-14: Specialized Skills","text":"Reference Relevance vis-network.js Documentation The library used for learning graph visualization in data format skills Chart.js Documentation The charting library used in code generation skills for data visualization MicroSims p5.js Reference The creative coding library used for interactive simulation MicroSims JSON Schema Specification Used for metadata validation in analysis and code generation skills"},{"location":"references/#chapters-15-17-deployment","title":"Chapters 15-17: Deployment","text":"Reference Relevance Integrate Skills into Your Agent How to build a skills-compatible client \u2014 the integration specification skills-ref Validation Library The official validation tool: <code>skills-ref validate ./my-skill</code> Apache 2.0 License The recommended license for open-source skills (used by Anthropic's own skills) Semantic Versioning 2.0.0 The versioning standard for published skill collections"},{"location":"references/#industry-context","title":"Industry Context","text":"Reference Relevance Anthropic Opens Agent Skills Standard \u2014 Unite.AI Analysis of Anthropic's pattern of publishing open standards (MCP, then Skills) as industry infrastructure Anthropic Launches Enterprise Agent Skills \u2014 VentureBeat Enterprise adoption angle and competitive positioning against OpenAI GitHub Copilot Skills Support Microsoft/GitHub adoption of the Agent Skills standard in Copilot and VS Code MkDocs Material Documentation The documentation framework used throughout this guide and the intelligent textbook methodology"},{"location":"chapters/01/","title":"Chapter 1: What Are Claude Code Skills?","text":"<p>Before you can build effective skills, you need a precise mental model of what they are and how they work. The word \"skill\" gets used loosely in AI conversations to mean anything from a prompt template to a fine-tuned model. In Claude Code, a skill has a specific, technical meaning \u2014 and that specificity is what makes skills powerful.</p> <p>This guide teaches skill development primarily for Claude Code, producing skills that are also compatible with the open agentskills.io specification. This chapter defines skills from first principles, explains how they execute, and establishes the conceptual vocabulary you will use throughout this guide.</p>"},{"location":"chapters/01/#11-the-precise-definition","title":"1.1 The Precise Definition","text":"<p>A Claude Code skill is a markdown file named <code>SKILL.md</code> that defines an autonomous workflow for Claude to execute. When you invoke a skill, Claude reads the file, interprets the workflow instructions, and executes them step by step \u2014 making decisions, reading files, writing files, running commands, and prompting you for input at defined checkpoints.</p> <p>Three properties distinguish a skill from any other text you might send to Claude:</p> <p>1. Persistence. A skill lives in the filesystem at a known path. You write it once and invoke it repeatedly. The instructions do not disappear when the conversation ends. You can version it, share it, and iterate on it over time.</p> <p>2. Structure. A skill defines a numbered workflow with explicit steps, prerequisites, decision points, and output specifications. Claude does not improvise the approach on each invocation \u2014 it follows the workflow you defined.</p> <p>3. Quality gates. A well-designed skill includes scoring rubrics, validation checklists, and output format specifications. Before completing, Claude evaluates its own output against those criteria. This means every execution meets the same baseline standard, whether Claude runs it today or six months from now.</p> <p>A skill is not:</p> <ul> <li>A saved chat prompt (those are stateless and unstructured)</li> <li>A fine-tuned model (those alter weights; skills alter behavior through instruction)</li> <li>A plugin or API integration (those require code; skills are pure markdown)</li> <li>A system prompt (those are global; skills are targeted, invocable, and composable)</li> </ul>"},{"location":"chapters/01/#12-the-standard-operating-procedure-analogy","title":"1.2 The Standard Operating Procedure Analogy","text":"<p>The clearest real-world analogy for a Claude Code skill is a Standard Operating Procedure (SOP). A hospital SOP does not say \"do surgery well\" \u2014 it specifies exactly what to do, in what order, with explicit checkpoints. Claude Code skills work the same way: the skill definition is the SOP, Claude is the executor, and the numbered steps ensure each invocation follows the same sequence. The key difference is that Claude brings reasoning capability to each step \u2014 the skill constrains the workflow; Claude applies judgment within it.</p>"},{"location":"chapters/01/#13-the-skill-execution-model","title":"1.3 The Skill Execution Model","text":"<p>Understanding what happens when you invoke a skill is essential for designing skills that work correctly.</p> <p>When you type <code>/skill my-skill-name</code> in Claude Code, the following sequence occurs:</p> <pre><code>flowchart TD\n    A[User invokes /skill my-skill-name] --&gt; B[Claude Code resolves skill path]\n    B --&gt; C{Skill found?}\n    C -- No --&gt; D[Error: skill not found]\n    C -- Yes --&gt; E[Claude reads SKILL.md into context]\n    E --&gt; F[Claude parses YAML frontmatter]\n    F --&gt; G[Claude reads workflow instructions in body]\n    G --&gt; H[Step 1: Claude executes first workflow step]\n    H --&gt; I{User dialog trigger?}\n    I -- Yes --&gt; J[Claude pauses and prompts user]\n    J --&gt; K[User responds]\n    K --&gt; L[Claude incorporates response and continues]\n    I -- No --&gt; M[Claude proceeds automatically]\n    L --&gt; N[Step 2...N: Continue through workflow]\n    M --&gt; N\n    N --&gt; O[Quality scoring: Claude evaluates output]\n    O --&gt; P{Score meets threshold?}\n    P -- No --&gt; Q[Claude flags gaps and requests review]\n    P -- Yes --&gt; R[Claude writes output files]\n    R --&gt; S[Claude confirms completion]</code></pre> <p>Several points in this diagram are worth examining closely.</p> <p>Path resolution. Claude Code looks for skills in two locations: <code>~/.claude/skills/</code> (global, available in all projects) and <code>.claude/skills/</code> (project-local, available only in the current project). For cross-platform compatibility with the agentskills.io open standard, skills can also live at <code>.github/skills/my-skill/SKILL.md</code> in a repository, making them accessible to any compatible agent runtime. Project-local skills take precedence over global skills with the same name. The skill directory must be named to match the skill's <code>name</code> field in the YAML frontmatter.</p> <p>YAML frontmatter parsing. The frontmatter at the top of <code>SKILL.md</code> tells Claude Code that this file is a skill definition. Without valid frontmatter, the file will not be recognized as a skill. Claude Code uses the <code>description</code> field from the frontmatter when generating the list of available skills for its own reference.</p> <p>Sequential execution. Claude executes workflow steps in the order they are numbered. This is not automatic parallelism \u2014 it is intentional sequential processing. Some skills explicitly instruct Claude to run tasks in parallel when the steps are independent. But the default is sequential, which makes behavior predictable and debuggable.</p> <p>User dialog triggers. Skills pause execution and prompt the user at defined points. A well-designed skill uses dialog triggers strategically: before taking irreversible actions (like overwriting files), when requiring information only the user can provide, and after presenting a plan that requires approval. Dialog triggers are how skills remain collaborative rather than fully autonomous.</p> <p>Quality scoring. At the end of execution, the skill instructs Claude to evaluate its own output against a rubric. This self-assessment catches obvious gaps and ensures the output matches the skill's stated purpose. Skills that score below a threshold produce a gap report rather than silently delivering substandard output.</p>"},{"location":"chapters/01/#14-the-skillmd-file-format","title":"1.4 The SKILL.md File Format","text":"<p>Every skill is defined by a single file named <code>SKILL.md</code>. The file has two sections: a YAML frontmatter block and a markdown body.</p>"},{"location":"chapters/01/#yaml-frontmatter","title":"YAML Frontmatter","text":"<p>The frontmatter occupies the top of the file, delimited by <code>---</code> markers. It contains metadata that Claude Code uses to register and describe the skill.</p> <pre><code>---\nname: my-skill-name\ndescription: &gt;\n  One to three sentences describing what this skill does and when to use it.\n  This description appears in Claude's system prompt as part of the skill registry.\n  Write it from Claude's perspective: \"Use this skill when...\"\nlicense: MIT\nallowed-tools: Bash(git:*) Read Write\ncompatibility:\n  claude-code: \"&gt;=1.0\"\nmetadata:\n  version: \"1.0.0\"\n  author: your-github-username\n---\n</code></pre> <p>The <code>name</code> field must match the directory name exactly. The <code>description</code> field is the most important \u2014 it is what appears in the skill registry that Claude sees at the start of every session. A poorly written description causes Claude to invoke the wrong skill or miss opportunities to use the right one.</p> <p>The <code>allowed-tools</code> field is optional and uses space-delimited format per the agentskills.io spec (e.g., <code>Bash(git:*) Read Write</code>). Omitting it grants the skill access to all available tools. Specifying it provides an explicit contract about what the skill will and will not do.</p> <p>The <code>compatibility</code> and <code>metadata</code> fields are also optional. <code>compatibility</code> specifies minimum runtime versions; <code>metadata</code> carries arbitrary key-value pairs like version numbers and author attribution. Both follow the agentskills.io open standard and are ignored by runtimes that do not recognize them.</p>"},{"location":"chapters/01/#the-markdown-body","title":"The Markdown Body","text":"<p>The body follows the frontmatter and contains the actual skill definition. A production-quality skill body includes these sections in this order:</p> <pre><code># Skill Name\n\n## Overview\nOne paragraph describing the skill's purpose and the output it produces.\n\n## When to Use This Skill\nExplicit conditions that trigger this skill. Include:\n- Positive conditions (when TO use it)\n- Negative conditions (when NOT to use it)\n- Prerequisites that must exist before invocation\n\n## Workflow\n\n### Step 1: [Action Name]\nDetailed instructions for what Claude should do in this step.\n\n### Step 2: [Action Name]\n...\n\n## Output Files\nWhat files the skill creates, their paths, and their formats.\n\n## Quality Scoring\nA rubric Claude uses to evaluate its own output before completing.\n\n## Example Session\nA sample invocation showing user input and expected behavior.\n\n## Common Pitfalls\nWhat goes wrong and how to prevent it.\n</code></pre> <p>The agentskills.io spec places no format restrictions on the body \u2014 any markdown structure is valid. This 10-section layout is a best practice convention, not a hard requirement enforced by Claude Code or the spec. Following it consistently makes skills easier to read, debug, and extend, especially when you are working with skills written by others.</p>"},{"location":"chapters/01/#15-ad-hoc-prompting-vs-skill-based-execution","title":"1.5 Ad-Hoc Prompting vs. Skill-Based Execution","text":"Dimension Ad-Hoc Prompting Skill-Based Execution Persistence Instructions exist only in the current conversation Instructions live in the filesystem, permanent until changed Consistency Output varies with each conversation and phrasing Output follows the same workflow every time Quality control Depends on the human to catch gaps Built-in scoring rubrics catch gaps before delivery Reusability Prompt must be rewritten or copy-pasted each time Invoked with a single command Shareability Must share the full prompt text Share the skill directory; others install and invoke it Iteration Improve by rephrasing the prompt each session Improve by editing SKILL.md; improvements persist Scope One conversation, one output Multi-step workflow producing multiple output files Delegation Human must supervise each step Human approves at defined checkpoints, Claude handles the rest Documentation The prompt is the only record SKILL.md is a complete specification and documentation artifact Composability Prompts do not chain automatically Skills can invoke other skills (meta-skills) <p>If you find yourself copying and pasting the same long prompt into Claude more than twice, you should probably write a skill. If the output needs to meet a consistent standard every time, you definitely should.</p>"},{"location":"chapters/01/#16-the-three-categories-of-skills","title":"1.6 The Three Categories of Skills","text":"<p>The claude-skills repository organizes skills into three broad categories. Understanding these categories helps you identify which type of skill to build for a given need.</p>"},{"location":"chapters/01/#book-generation-skills","title":"Book Generation Skills","text":"<p>These skills automate the creation of intelligent textbooks \u2014 structured educational content built on MkDocs Material with concept dependency graphs, interactive simulations (MicroSims), Bloom's Taxonomy-aligned quizzes, and ISO 11179-compliant glossaries.</p> <p>Book generation skills typically operate in a defined sequence, with each skill consuming the output of the previous one:</p> <ol> <li><code>course-description-analyzer</code> validates and structures the course description</li> <li><code>learning-graph-generator</code> produces a 200-concept dependency graph</li> <li><code>book-chapter-generator</code> designs chapter structure from the graph</li> <li><code>chapter-content-generator</code> writes detailed content for each chapter</li> <li><code>glossary-generator</code>, <code>faq-generator</code>, <code>quiz-generator</code>, <code>reference-generator</code> produce supporting content</li> <li><code>book-metrics-generator</code> analyzes quality across the full book</li> <li><code>readme-generator</code> and <code>linkedin-announcement-generator</code> handle publishing assets</li> </ol> <p>Each skill in this chain has defined prerequisites (what must exist before it runs) and defined outputs (what it produces for the next skill to consume). This tight coupling between skills is what makes the full pipeline reliable at scale.</p>"},{"location":"chapters/01/#analysis-skills","title":"Analysis Skills","text":"<p>Analysis skills examine existing content and produce structured reports. They are typically read-heavy (consuming many files) and produce markdown reports rather than creating new content from scratch.</p> <p>The <code>book-metrics-generator</code> is a good example: it reads all chapter files, counts concepts covered, identifies missing content, evaluates MicroSim coverage, and produces a quality metrics report with a composite score.</p> <p>Analysis skills often include scoring rubrics that produce numerical quality assessments. These scores make it possible to set objective thresholds \u2014 \"do not deploy a book with a metrics score below 70\" \u2014 and automate quality gates.</p>"},{"location":"chapters/01/#specialized-skills","title":"Specialized Skills","text":"<p>Specialized skills do not fit neatly into the book generation pipeline or the analysis category. They serve specific, high-value use cases:</p> <ul> <li><code>concept-classifier</code> creates interactive p5.js classification quiz MicroSims</li> <li><code>microsim-generator</code> is a meta-skill that routes to 14 different visualization generators (p5.js, Chart.js, Mermaid, vis-network, Leaflet, Plotly, and others)</li> <li><code>moving-rainbow</code> generates MicroPython code for Raspberry Pi Pico LED matrix hardware</li> <li><code>story-generator</code> produces narrative educational content following specific structural templates</li> </ul> <p>Specialized skills often push the boundaries of what skills can do: executing bash commands, generating working code in multiple languages, creating multi-file packages, and interfacing with external services.</p>"},{"location":"chapters/01/#17-why-skills-matter","title":"1.7 Why Skills Matter","text":"<p>Skills are worth building when consistency, quality, and delegation matter:</p> <ul> <li>Consistency \u2014 the same skill invoked on Monday and Friday follows the same workflow and produces outputs with the same structure, regardless of how you phrase the request</li> <li>Quality \u2014 quality criteria are embedded in the skill and enforced at completion, not left to memory or human review</li> <li>Reproducibility \u2014 complex multi-step workflows that once required manual orchestration run reliably from a single command</li> <li>Delegation \u2014 you define the checkpoints; Claude handles the steps between them</li> </ul>"},{"location":"chapters/01/#18-a-brief-history-dan-mccrearys-intelligent-textbook-methodology","title":"1.8 A Brief History: Dan McCreary's Intelligent Textbook Methodology","text":"<p>The skill system in its current form grew out of work by Dan McCreary on what he calls intelligent textbooks \u2014 educational resources built to be more adaptive, interactive, and learnable than traditional text.</p> <p>McCreary's methodology requires building several interdependent artifacts for each textbook: a concept dependency graph (the learning graph), Bloom's Taxonomy-aligned content, interactive simulations (MicroSims), quality-gated quizzes, and ISO 11179-compliant glossaries. Each artifact has specific structural requirements and quality standards.</p> <p>The challenge was that producing these artifacts consistently at scale required precise, repeatable workflows. Writing out those workflows in prose and following them manually was error-prone and slow. The natural solution was to encode the workflows as Claude Code skills \u2014 one skill per artifact type \u2014 and chain the skills together into a pipeline.</p> <p>The result was the <code>claude-skills</code> repository: a collection of skills that can take a course description as input and produce a complete, deployable intelligent textbook as output. Each skill in the collection is a crystallized version of McCreary's methodology for one specific artifact, validated against real textbook projects and refined through iteration.</p> <p>This guide teaches you to apply the same methodology to your own domains. The intelligent textbook use case provides many of the examples because it is the most fully developed skill ecosystem available. But the principles generalize: any complex, multi-step workflow that requires consistent quality and produces structured output is a candidate for a skill.</p>"},{"location":"chapters/01/#19-what-comes-next","title":"1.9 What Comes Next","text":"<p>You now have the conceptual foundation for the rest of this guide:</p> <ul> <li>Skills are persistent, structured, quality-gated workflow definitions</li> <li>They live in <code>SKILL.md</code> files in <code>~/.claude/skills/</code> or <code>.claude/skills/</code></li> <li>They execute through a defined sequence: read skill, parse frontmatter, execute steps, apply quality scoring, produce output</li> <li>They differ from ad-hoc prompting in persistence, consistency, quality, reusability, shareability, and composability</li> <li>They fall into three categories: book generation, analysis, and specialized</li> </ul> <p>Chapter 2 maps the full skill ecosystem \u2014 how skills are discovered, the taxonomy of skill types from individual skills to meta-skill routers, and Claude Code's 30-skill-per-session implementation limit (a Claude Code constraint, not a spec requirement).</p> <p>Before moving on</p> <p>Think about a workflow you currently execute through ad-hoc Claude conversations. Does it have defined steps? Does the output need to meet a consistent standard? Does it produce files you use downstream? If yes to two or more of these, it is a strong candidate for a skill. Keep it in mind as you read the next two chapters.</p> <p>Key terms from this chapter</p> <ul> <li>Skill: A markdown-defined autonomous workflow for Claude Code, stored as <code>SKILL.md</code></li> <li>YAML frontmatter: Metadata block at the top of <code>SKILL.md</code> containing <code>name</code>, <code>description</code>, <code>license</code>, <code>allowed-tools</code>, and optional <code>compatibility</code> and <code>metadata</code> fields</li> <li>allowed-tools: Space-delimited list of permitted tools per the agentskills.io spec (e.g., <code>Bash(git:*) Read Write</code>)</li> <li>Workflow steps: Numbered sequential instructions in the skill body that Claude executes in order</li> <li>User dialog trigger: A defined pause point where the skill prompts the user for input or approval</li> <li>Quality scoring: A rubric embedded in the skill that Claude uses to evaluate its own output before completing</li> <li>Meta-skill: A skill that routes to other skills based on context, consolidating multiple specialized skills into one entry point</li> <li>agentskills.io: Open standard for portable, cross-platform skill definitions; the format this guide produces</li> </ul>"},{"location":"chapters/02/","title":"Chapter 2: The Skill Ecosystem","text":"<p>A single skill is useful. A well-organized ecosystem of skills is transformative. This chapter maps the full landscape of Claude Code's skill system \u2014 how skills are stored, discovered, registered in Claude's context, and organized into categories. It also explains the constraints that shape skill design, most importantly the 30-skill limit that drove the creation of the meta-skill pattern.</p> <p>Understanding the ecosystem before you build your own skills prevents common architectural mistakes. The choices you make about how many skills to create, how to name them, and whether to consolidate them into meta-skills have lasting consequences for how effectively Claude uses them.</p>"},{"location":"chapters/02/#21-the-30-skill-limit","title":"2.1 The 30-Skill Limit","text":"<p>Claude Code enforces a hard limit of 30 active skills at any time. This is not an arbitrary policy \u2014 it reflects a real constraint in how skills integrate with Claude's context.</p> <p>When Claude Code starts a session, it scans the skill directories and injects a compact registry into Claude's system prompt. This registry includes the <code>name</code> and <code>description</code> fields from each skill's YAML frontmatter. Claude uses this registry to know what skills are available and when to invoke them.</p> <p>The system prompt has a finite token budget. Every skill description consumes tokens that would otherwise be available for conversation history, file contents, and instructions. At 30 skills with moderately detailed descriptions, the registry begins to consume a meaningful fraction of the available system prompt budget. Beyond 30 skills, the overhead becomes significant enough that Claude Code enforces the limit rather than allowing unbounded growth.</p> <p>The 30-skill limit is a design constraint, not a temporary limitation</p> <p>The limit exists because skills are loaded into the system prompt on every session, regardless of whether they are used. Unlike traditional software where you only pay for what you execute, every registered skill costs context tokens on every invocation. Design your skill ecosystem with this constraint in mind from the start.</p> <p>The practical implication is that as you build more skills, you will eventually face a choice: archive older skills to make room for new ones, or consolidate multiple related skills into a single meta-skill. The meta-skill pattern, described in section 2.4, is the preferred solution for mature skill ecosystems.</p>"},{"location":"chapters/02/#22-how-skills-are-discovered","title":"2.2 How Skills Are Discovered","text":"<p>Claude Code discovers skills by scanning two filesystem locations at session startup:</p> <p>Global skills live at <code>~/.claude/skills/</code>. They are available in every Claude Code session, across all projects. A global skill is installed once and works everywhere.</p> <p>Project-local skills live at <code>.claude/skills/</code> relative to the project root (the directory containing <code>mkdocs.yml</code>, <code>package.json</code>, or another project marker). They are only available when Claude Code is invoked from within that project. Project-local skills override global skills with the same name.</p> <p>The discovery process is straightforward: Claude Code lists all subdirectories in both locations and looks for a <code>SKILL.md</code> file in each subdirectory. A subdirectory without <code>SKILL.md</code> is ignored. The skill's registered name comes from the <code>name</code> field in the YAML frontmatter, not the directory name \u2014 though by strong convention these should always match.</p> <pre><code>~/.claude/skills/\n\u251c\u2500\u2500 glossary-generator/\n\u2502   \u2514\u2500\u2500 SKILL.md             \u2190 name: glossary-generator\n\u251c\u2500\u2500 book-chapter-generator/\n\u2502   \u2514\u2500\u2500 SKILL.md             \u2190 name: book-chapter-generator\n\u251c\u2500\u2500 microsim-generator/\n\u2502   \u251c\u2500\u2500 SKILL.md             \u2190 name: microsim-generator\n\u2502   \u2514\u2500\u2500 references/          \u2190 Sub-skill guides (loaded on demand)\n\u2502       \u251c\u2500\u2500 p5-guide.md\n\u2502       \u251c\u2500\u2500 chartjs-guide.md\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 my-custom-skill/\n    \u2514\u2500\u2500 SKILL.md             \u2190 name: my-custom-skill\n</code></pre> <p>The <code>references/</code> subdirectory is a pattern used exclusively by meta-skills. It contains guide files that are not skill definitions \u2014 they are additional instructions loaded by the parent skill during execution. These guide files do not appear in the skill registry and do not count toward the 30-skill limit.</p>"},{"location":"chapters/02/#23-how-the-skill-registry-works","title":"2.3 How the Skill Registry Works","text":"<p>Once Claude Code discovers all skills, it constructs a registry and injects it into Claude's system prompt. The registry entry for each skill looks approximately like this:</p> <pre><code>Available skill: glossary-generator\nDescription: Creates ISO 11179-compliant glossaries for intelligent textbooks.\nGenerates precise, concise, distinct, non-circular, and unencumbered definitions\nfor all concepts in the course. Use after the learning graph is finalized.\nInvoke with: /skill glossary-generator\n</code></pre> <p>Claude reads this registry at the start of every session. When you describe what you want to accomplish, Claude matches your intent against the registry descriptions. If a description accurately describes when and why to use a skill, Claude will suggest or automatically invoke it at the appropriate moment.</p> <p>This has two important implications for skill design:</p> <p>The description field is not documentation \u2014 it is routing logic. Claude uses descriptions to make invocation decisions. A description that is vague, too long, or poorly targeted causes Claude to miss opportunities to use the skill or to invoke it incorrectly. Write descriptions as if they are instructions to Claude for when to activate the skill.</p> <p>Short, precise descriptions outperform long, comprehensive ones. Because descriptions appear in the system prompt on every session, a 500-word description wastes far more tokens than a 50-word description. Aim for two to four sentences that specify the trigger condition, the primary output, and any critical prerequisites.</p> <p>Testing your skill description</p> <p>After installing a skill, start a new session and describe what you want to do in plain language \u2014 without mentioning the skill name. If Claude suggests the skill or invokes it automatically, your description is working. If Claude does not make the connection, the description needs revision.</p>"},{"location":"chapters/02/#24-the-three-tiers-of-skills","title":"2.4 The Three Tiers of Skills","text":"<p>Skills in the ecosystem fall into three tiers based on their structural complexity and their relationship to other skills.</p> <pre><code>graph TD\n    subgraph Tier3[\"Tier 3: Pipeline Skills\"]\n        direction LR\n        P1[Skill A] --&gt;|output| P2[Skill B]\n        P2 --&gt;|output| P3[Skill C]\n        P3 --&gt;|output| P4[Skill D]\n    end\n\n    subgraph Tier2[\"Tier 2: Meta-Skills\"]\n        direction TB\n        M1[Meta-Skill SKILL.md] --&gt; M2[references/variant-a.md]\n        M1 --&gt; M3[references/variant-b.md]\n        M1 --&gt; M4[references/variant-c.md]\n    end\n\n    subgraph Tier1[\"Tier 1: Individual Skills\"]\n        direction TB\n        S1[skill-one/SKILL.md]\n        S2[skill-two/SKILL.md]\n        S3[skill-three/SKILL.md]\n    end\n\n    Tier1 --&gt; Tier2\n    Tier2 --&gt; Tier3</code></pre>"},{"location":"chapters/02/#tier-1-individual-skills","title":"Tier 1: Individual Skills","text":"<p>An individual skill is a single <code>SKILL.md</code> file that executes one coherent workflow. It has no dependencies on other skills (though it may have prerequisites on files that other skills have previously created). Most skills start and remain at this tier.</p> <p>Individual skills are the right choice when:</p> <ul> <li>The workflow is clearly bounded and self-contained</li> <li>The output is a single type of artifact (one file, one report, one analysis)</li> <li>The number of skills in your ecosystem is well below 30</li> <li>The skill does not need to behave differently based on context</li> </ul> <p>Examples of well-designed individual skills: <code>glossary-generator</code>, <code>quiz-generator</code>, <code>course-description-analyzer</code>, <code>readme-generator</code>.</p>"},{"location":"chapters/02/#tier-2-meta-skills","title":"Tier 2: Meta-Skills","text":"<p>A meta-skill is an individual skill that acts as a router. When invoked, it analyzes the user's request and loads an appropriate guide from its <code>references/</code> directory to handle the specific variant. The meta-skill itself is a single registered skill; the guides it loads are unregistered and do not consume skill slots.</p> <p>Meta-skills solve the 30-skill limit problem by consolidating multiple specialized skills into one registry entry. Instead of 14 separate MicroSim generator skills (one per visualization library), there is one <code>microsim-generator</code> skill that routes to the appropriate library-specific guide.</p> <p>The <code>microsim-generator</code> skill in the claude-skills repository demonstrates this pattern at scale:</p> <pre><code>microsim-generator/\n\u251c\u2500\u2500 SKILL.md              \u2190 One registered skill, routes to 14 guides\n\u2514\u2500\u2500 references/\n    \u251c\u2500\u2500 routing-criteria.md     \u2190 How to select the right guide\n    \u251c\u2500\u2500 p5-guide.md             \u2190 p5.js simulations\n    \u251c\u2500\u2500 chartjs-guide.md        \u2190 Chart.js data visualizations\n    \u251c\u2500\u2500 timeline-guide.md       \u2190 vis-timeline chronological displays\n    \u251c\u2500\u2500 map-guide.md            \u2190 Leaflet geographic visualizations\n    \u251c\u2500\u2500 vis-network-guide.md    \u2190 Network/graph diagrams\n    \u251c\u2500\u2500 mermaid-guide.md        \u2190 Mermaid flowcharts and diagrams\n    \u251c\u2500\u2500 plotly-guide.md         \u2190 Plotly scientific charts\n    \u251c\u2500\u2500 venn-guide.md           \u2190 Venn.js set diagrams\n    \u251c\u2500\u2500 bubble-guide.md         \u2190 Bubble/scatter charts\n    \u251c\u2500\u2500 causal-loop-guide.md    \u2190 Causal loop diagrams\n    \u251c\u2500\u2500 comparison-table-guide.md \u2190 Comparison tables\n    \u251c\u2500\u2500 celebration-guide.md    \u2190 Celebration animations\n    \u2514\u2500\u2500 html-table.md           \u2190 HTML table visualizations\n</code></pre> <p>From the user's perspective, there is one skill to remember and invoke. Claude handles the routing internally based on what the user describes. The 14 specialized implementations are hidden behind a single interface.</p> <p>Meta-skills are the right choice when:</p> <ul> <li>You have more than 30 total skill definitions (or are approaching that limit)</li> <li>Multiple skills share a common invocation pattern but differ in implementation</li> <li>The selection logic between variants is deterministic and can be expressed as criteria</li> <li>Users should not need to remember which specific variant to invoke</li> </ul>"},{"location":"chapters/02/#tier-3-pipeline-skills","title":"Tier 3: Pipeline Skills","text":"<p>A pipeline is not a single skill type \u2014 it is a coordinated sequence of individual or meta-skills where each skill's output is consumed by the next skill's input. Pipelines are orchestrated manually (the user invokes each skill in sequence) or through a master pipeline skill that coordinates the sequence.</p> <p>The intelligent textbook creation workflow is an example of a pipeline:</p> <pre><code>course-description-analyzer\n    \u2192 learning-graph-generator\n        \u2192 book-chapter-generator\n            \u2192 chapter-content-generator\n                \u2192 glossary-generator + faq-generator + quiz-generator\n                    \u2192 book-metrics-generator\n                        \u2192 readme-generator\n</code></pre> <p>Each skill in this pipeline has defined input prerequisites (files that must exist) and defined outputs (files it creates for the next skill). The pipeline as a whole transforms a course description into a deployable textbook.</p> <p>Pipeline skills are the right choice when:</p> <ul> <li>The full workflow spans multiple distinct artifact types</li> <li>Individual steps are useful in isolation (not just as part of the pipeline)</li> <li>Different users may need different subsets of the pipeline</li> <li>Each step has meaningful quality gates that benefit from human review</li> </ul>"},{"location":"chapters/02/#25-survey-of-existing-skills","title":"2.5 Survey of Existing Skills","text":"<p>The following table catalogs all skills in the claude-skills repository, organized by category. Understanding what already exists helps you avoid duplicating existing functionality and helps you identify gaps worth filling.</p>"},{"location":"chapters/02/#book-generation-skills","title":"Book Generation Skills","text":"Skill Name Purpose Primary Input Primary Output <code>course-description-analyzer</code> Validates and structures course descriptions against quality criteria Draft course description Structured <code>course-description.md</code> <code>learning-graph-generator</code> Generates 200-concept dependency graphs as DAGs <code>course-description.md</code> <code>learning-graph.csv</code>, <code>learning-graph.json</code> <code>book-chapter-generator</code> Designs chapter structure from the learning graph <code>learning-graph.json</code> Chapter directories, <code>mkdocs.yml</code> nav entries <code>chapter-content-generator</code> Writes detailed educational content for each chapter Chapter structure, <code>learning-graph.json</code> Chapter <code>index.md</code> files with full content <code>glossary-generator</code> Produces ISO 11179-compliant definitions for all course concepts <code>course-description.md</code>, <code>learning-graph.json</code> <code>docs/glossary.md</code> <code>faq-generator</code> Creates frequently asked questions aligned to Bloom's Taxonomy Course content files <code>docs/faq.md</code> <code>quiz-generator</code> Generates Bloom's Taxonomy-aligned assessment questions Chapter content Quiz files per chapter <code>reference-generator</code> Compiles curated, annotated reference lists Course topic and content <code>docs/references.md</code> <code>book-metrics-generator</code> Analyzes full book quality and produces a composite score All book files Quality metrics report <code>readme-generator</code> Creates GitHub README for the textbook repository Course description, book structure <code>README.md</code> <code>linkedin-announcement-generator</code> Produces LinkedIn announcement posts for book launches Course description, book URL LinkedIn post draft <code>diagram-reports-generator</code> Audits MicroSim coverage and quality across all chapters All chapter files Diagram audit report"},{"location":"chapters/02/#meta-skills","title":"Meta-Skills","text":"Skill Name Sub-Skills Purpose <code>book-installer</code> 30 feature guides (mkdocs-template, learning-graph-viewer, skill-tracker, and more) Installs and configures project infrastructure for intelligent textbooks <code>microsim-generator</code> 14 visualization guides (p5, chartjs, timeline, map, vis-network, mermaid, plotly, venn, bubble, causal-loop, comparison-table, celebration, html-table) Creates interactive MicroSims using the best-matched JavaScript library <code>microsim-utils</code> 4 utility guides (standardization, screen-capture, add-icons, index-generator) Maintenance utilities for MicroSim quality and consistency"},{"location":"chapters/02/#specialized-skills","title":"Specialized Skills","text":"Skill Name Purpose Output Type <code>concept-classifier</code> Creates interactive p5.js classification quiz MicroSims HTML/JS quiz package <code>story-generator</code> Produces narrative educational content following structural templates Markdown story documents <code>moving-rainbow</code> Generates MicroPython code for Raspberry Pi Pico LED matrix hardware <code>.py</code> hardware script"},{"location":"chapters/02/#26-the-skill-listing-system","title":"2.6 The Skill Listing System","text":"<p>Knowing which skills are installed and available is an operational necessity. The claude-skills repository provides three methods for listing skills, each optimized for different use cases.</p>"},{"location":"chapters/02/#the-skills-command","title":"The /skills Command","text":"<p>The <code>/skills</code> command is a Claude Code slash command defined in <code>commands/skills.md</code>. When you type <code>/skills</code> in a Claude Code session, Claude executes the listing procedure defined in that file.</p> <p>The command uses a priority hierarchy:</p> <ol> <li>MCP server (zero tokens): If the <code>mcp__list_skills</code> tool is available, Claude calls it with <code>format: \"names-only\"</code>. This uses zero tokens because the MCP server handles the listing outside Claude's context window.</li> <li>Shell script (~67 tokens): If no MCP server is available, Claude executes <code>~/bin/list-skills.sh --names-only</code>. This costs approximately 67 tokens to display the output.</li> <li>Direct file reading (avoid): Reading <code>SKILL.md</code> files directly costs 5,000+ tokens and should never be used for listing purposes.</li> </ol> <p>Never list skills by reading SKILL.md files directly</p> <p>Reading all SKILL.md files to produce a skills list is technically possible but wastes thousands of tokens. Always use the shell script or MCP server for listing. Reserve direct SKILL.md reading for when you need the full workflow instructions of a specific skill.</p>"},{"location":"chapters/02/#the-list-skillssh-script","title":"The list-skills.sh Script","text":"<p>The shell script at <code>scripts/list-skills.sh</code> provides programmatic access to the skill registry. It supports multiple output formats:</p> <pre><code># Default text output\n./scripts/list-skills.sh\n\n# JSON output for programmatic use\n./scripts/list-skills-format.sh json\n\n# Markdown table output\n./scripts/list-skills-format.sh markdown\n</code></pre> <p>The script scans <code>~/.claude/skills/</code>, reads the <code>name</code> and <code>description</code> fields from each SKILL.md frontmatter, and formats the output. It is the fastest non-MCP method for producing the skill list.</p>"},{"location":"chapters/02/#the-mcp-server","title":"The MCP Server","text":"<p>The <code>mcp-servers/skills-lister</code> directory contains a Model Context Protocol server that exposes skill listing as a tool. When configured, Claude can invoke <code>mcp__list_skills</code> to get the current skill registry with zero token cost.</p> <p>The MCP server supports three output formats:</p> Format Output Use Case <code>names-only</code> Just skill names, ~267 characters Quick reference during conversation <code>json</code> Structured JSON with metadata, ~5,117 characters Programmatic skill management <code>full</code> Human-readable with descriptions, ~5,063 characters Detailed skill survey <p>The MCP server is the preferred listing method for heavy users of the skill system who invoke skills frequently. The zero-token cost is meaningful when you are managing a large skill ecosystem and need to reference the registry often.</p>"},{"location":"chapters/02/#27-the-skill-ecosystem-tiers-in-practice","title":"2.7 The Skill Ecosystem Tiers in Practice","text":"<p>To make the tier structure concrete, consider how the ecosystem evolved in the intelligent textbook domain.</p> <p>Phase 1: Individual skills. The first skills created were individual, purpose-built agents: one for generating learning graphs, one for generating chapters, one for generating glossaries. Each was independent and straightforward to build. At this phase, the 30-skill limit was not a concern.</p> <p>Phase 2: Growth and the 30-skill limit. As the ecosystem grew \u2014 more visualization types for MicroSims, more book infrastructure options, more analysis tools \u2014 the number of individual skills approached and eventually exceeded 30. Skills that were rarely used competed for registry slots with skills that were used daily.</p> <p>Phase 3: Meta-skill consolidation. Closely related skills were consolidated into meta-skills. The 14 MicroSim visualization skills became <code>microsim-generator</code> with a <code>references/</code> directory. The multiple book infrastructure setup skills became <code>book-installer</code> with its own references. This reduced the registry from 30+ individual skills to 18 registered skills, with the full capability preserved in the reference guides.</p> <p>Phase 4: Archived skills. The original individual skills that were consolidated into meta-skills were moved to <code>skills/archived/</code>. They remain available as reference material for understanding the full implementation, but they are no longer registered and do not consume skill slots.</p> <p>This evolution pattern is worth internalizing before you begin building your own skills. Start with individual skills. Build the meta-skill pattern only when you have a clear grouping of related skills that are approaching the limit, or when you have multiple skills that share a common invocation pattern.</p>"},{"location":"chapters/02/#28-installing-skills","title":"2.8 Installing Skills","text":"<p>Skills can be installed in two ways: by placing files directly in the skill directory, or by using symlinks.</p>"},{"location":"chapters/02/#direct-installation","title":"Direct Installation","text":"<p>Copy or create the skill directory in <code>~/.claude/skills/</code>:</p> <pre><code>mkdir -p ~/.claude/skills/my-skill\n# Then create SKILL.md in that directory\n</code></pre> <p>Direct installation is simple but means you maintain separate copies of the skill in each location where you have it.</p>"},{"location":"chapters/02/#symlink-installation","title":"Symlink Installation","text":"<p>The claude-skills repository uses symlinks to install skills globally from a central development location:</p> <pre><code># From the claude-skills repo root\n./scripts/install-claude-skills.sh\n</code></pre> <p>This script creates symlinks from <code>~/.claude/skills/</code> pointing to the skill directories in the repository. The advantage is that you develop skills in the repository, and any edits are immediately reflected in the global installation without copying files.</p> <p>For project-local installation, modify the script target:</p> <pre><code># Instead of ~/.claude/skills/, symlink to .claude/skills/\nln -s ~/projects/claude-skills/skills/my-skill .claude/skills/my-skill\n</code></pre> <p>Symlink installation is the recommended approach when you are actively developing skills, because it keeps a single source of truth and allows you to version skills with git.</p>"},{"location":"chapters/02/#29-what-the-ecosystem-tells-you-about-skill-design","title":"2.9 What the Ecosystem Tells You About Skill Design","text":"<p>Surveying the full ecosystem reveals several patterns that recur across well-designed skills:</p> <p>Skills are scoped to single artifact types. The <code>glossary-generator</code> produces glossaries. The <code>quiz-generator</code> produces quizzes. The <code>readme-generator</code> produces READMEs. This tight scoping makes it obvious when to invoke each skill and what to expect from it. Broad skills that try to do everything at once are harder to invoke correctly and harder to iterate on.</p> <p>Skills have defined prerequisites. Every book generation skill specifies what files must exist before it runs. The <code>book-chapter-generator</code> requires <code>course-description.md</code> and <code>learning-graph.json</code>. If you invoke it before those files exist, the skill instructs Claude to stop and report the missing prerequisites rather than proceeding with incomplete information.</p> <p>Skills produce predictable file layouts. Well-designed skills specify exactly which files they create, at which paths, in which formats. This makes it possible to chain skills together reliably \u2014 the output of one skill is a well-defined input for the next.</p> <p>Descriptions are optimized for routing. The most effective skill descriptions in the ecosystem are written from the perspective of \"when should Claude activate this skill?\" rather than \"what does this skill do in exhaustive detail?\" The distinction matters: a description written for humans differs from a description written to guide Claude's routing decisions.</p> <p>These patterns are the implicit design principles of the ecosystem. Chapter 4 makes them explicit in the context of the SKILL.md structure, and Chapter 6 develops them into a full workflow design methodology.</p>"},{"location":"chapters/02/#210-what-comes-next","title":"2.10 What Comes Next","text":"<p>You now have a complete map of the skill ecosystem:</p> <ul> <li>The 30-skill limit reflects the system prompt token budget and shapes all ecosystem design decisions</li> <li>Skills are discovered by scanning <code>~/.claude/skills/</code> and <code>.claude/skills/</code> for subdirectories containing <code>SKILL.md</code></li> <li>The skill registry is injected into Claude's system prompt with the name and description from each skill's frontmatter</li> <li>Skills fall into three tiers: individual, meta-skill, and pipeline</li> <li>The existing skill ecosystem covers book generation, meta-skill routing, and specialized domains</li> <li>Three methods exist for listing skills, optimized for different token budgets</li> </ul> <p>Chapter 3 moves from understanding the ecosystem to building within it. You will construct your first skill from scratch \u2014 a <code>changelog-generator</code> \u2014 iterating from a minimal viable definition to a production-quality skill with workflow steps, user dialog triggers, and quality scoring.</p> <p>Inventory your existing workflows</p> <p>Before reading Chapter 3, list three repetitive workflows you currently execute through ad-hoc Claude conversations. For each one, identify: What files does it read? What files does it produce? Where in the process do you need to provide input? These questions will become the raw material for your first skill definition.</p> <p>Key terms from this chapter</p> <ul> <li>30-skill limit: The maximum number of registered skills in Claude Code, enforced to protect the system prompt token budget</li> <li>Global skill: A skill installed in <code>~/.claude/skills/</code>, available in all Claude Code sessions</li> <li>Project-local skill: A skill installed in <code>.claude/skills/</code> within a project, only available in that project</li> <li>Skill registry: The compact list of skill names and descriptions injected into Claude's system prompt at session start</li> <li>Tier 1 (Individual skill): A single <code>SKILL.md</code> file executing one coherent workflow</li> <li>Tier 2 (Meta-skill): A skill that routes to sub-skill guides in a <code>references/</code> directory</li> <li>Tier 3 (Pipeline): A coordinated sequence of skills where each skill's output is the next skill's input</li> <li>references/ directory: A subdirectory within a meta-skill directory containing guide files that are loaded on demand and do not count toward the 30-skill limit</li> </ul>"},{"location":"chapters/03/","title":"Chapter 3: Your First Skill","text":"<p>The best way to understand how skills work is to build one. This chapter walks you through constructing a <code>changelog-generator</code> skill from scratch, starting with the absolute minimum that Claude Code will recognize as a skill, and iterating to a production-quality definition with a complete workflow, user dialog triggers, and a quality scoring rubric.</p> <p>By the end of this chapter, you will have:</p> <ul> <li>A working skill installed at <code>~/.claude/skills/changelog-generator/SKILL.md</code></li> <li>A clear understanding of how each component of the skill definition affects Claude's behavior</li> <li>A tested iteration loop you can apply to any skill you build</li> <li>A production-ready checklist for evaluating when a skill is finished</li> </ul>"},{"location":"chapters/03/#31-choosing-the-right-first-skill","title":"3.1 Choosing the Right First Skill","text":"<p>The <code>changelog-generator</code> is an ideal first skill for several reasons.</p> <p>It solves a real, recurring problem: tracking changes to a codebase or document collection over time. Every software project, every textbook, every content repository benefits from a clear, readable changelog. Writing changelogs manually is tedious, inconsistent, and easy to skip. This is exactly the profile of a task that benefits from a skill.</p> <p>It is also well-scoped. The input is clear (git history or a set of recent changes), the output is clear (a formatted <code>CHANGELOG.md</code> entry), and the workflow has a defined beginning and end. There are no hidden complexities that will derail your first attempt.</p> <p>Finally, it is realistic. You will be able to invoke and test this skill immediately, see the output, identify what needs improvement, and iterate. Skills that require complex infrastructure to test are poor choices for learning.</p>"},{"location":"chapters/03/#32-stage-1-the-minimal-viable-skill","title":"3.2 Stage 1: The Minimal Viable Skill","text":"<p>The minimum file that Claude Code will recognize as a skill contains only a valid YAML frontmatter block. No body content, no workflow steps, just the metadata that registers the skill.</p> <p>Create the directory and file:</p> <pre><code>mkdir -p ~/.claude/skills/changelog-generator\n</code></pre> <p>Then create <code>~/.claude/skills/changelog-generator/SKILL.md</code> with this content:</p> <pre><code>---\nname: changelog-generator\ndescription: &gt;\n  Generates a formatted CHANGELOG.md entry from recent git commits or\n  a description of recent changes. Use this skill when you need to document\n  what changed in a project between two points in time.\n---\n</code></pre> <p>That is a complete, valid skill definition. Claude Code will register it, and it will appear when you run <code>/skills</code>.</p> <p>What happens when you invoke it at this stage.</p> <p>Start a new Claude Code session and type <code>/skill changelog-generator</code>. Claude will read the SKILL.md file, find only the frontmatter and no workflow instructions, and... improvise. It will do something reasonable based on the description and its general knowledge of changelogs, but the output will be inconsistent. Some invocations will produce Keep a Changelog format; others will use a different format. Some will run <code>git log</code>; others will ask you what changed.</p> <p>This inconsistency is the problem that skills exist to solve. The minimal viable skill proves the registration mechanism works, but it does not yet provide the structured workflow that makes skills valuable.</p> <p>What to observe at Stage 1</p> <p>Invoke the skill twice in separate sessions without saying anything after the invocation. Observe whether the format of the output is the same both times. It will not be. This inconsistency is what the workflow steps in Stage 2 will fix.</p>"},{"location":"chapters/03/#33-stage-2-adding-the-basic-workflow","title":"3.3 Stage 2: Adding the Basic Workflow","text":"<p>The workflow section transforms a registered skill into a defined procedure. Each numbered step is an instruction to Claude that constrains what it does and in what order.</p> <p>Here is the Stage 2 version of the skill:</p> <pre><code>---\nname: changelog-generator\ndescription: &gt;\n  Generates a formatted CHANGELOG.md entry from recent git commits or\n  a description of recent changes. Use this skill when you need to document\n  what changed in a project between two points in time. Output follows\n  Keep a Changelog format (keepachangelog.com).\n---\n\n# Changelog Generator\n\n## Overview\n\nThis skill produces a properly formatted `CHANGELOG.md` entry documenting\nrecent changes to a project. It follows the Keep a Changelog format\n(keepachangelog.com) with sections for Added, Changed, Fixed, Removed,\nDeprecated, and Security changes. Output is ready to paste into your\nCHANGELOG.md file.\n\n## When to Use This Skill\n\nUse this skill when:\n\n- You are preparing a release and need to document what changed\n- You want to update CHANGELOG.md after a sprint or milestone\n- A collaborator asks what changed since the last version\n\nDo NOT use this skill if:\n\n- You need automated changelog generation from CI/CD (use a dedicated tool)\n- The project has no version history or semantic versioning scheme\n\n## Workflow\n\n### Step 1: Gather Change Information\n\nRun the following command to get recent git commits:\n\n```bash\ngit log --oneline --since=\"30 days ago\" --no-merges\n</code></pre> <p>If git is not available or the project is not a git repository, ask the user: \"Please describe the changes made since the last release.\"</p>"},{"location":"chapters/03/#step-2-categorize-changes","title":"Step 2: Categorize Changes","text":"<p>Review the commit messages or user-provided descriptions and categorize each change into the appropriate Keep a Changelog section:</p> <ul> <li>Added: New features or capabilities</li> <li>Changed: Changes to existing functionality</li> <li>Fixed: Bug fixes</li> <li>Removed: Removed features or capabilities</li> <li>Deprecated: Features marked for future removal</li> <li>Security: Security fixes or improvements</li> </ul> <p>Omit sections that have no entries.</p>"},{"location":"chapters/03/#step-3-format-the-changelog-entry","title":"Step 3: Format the Changelog Entry","text":"<p>Format the entry as follows:</p> <pre><code>## [Unreleased] - YYYY-MM-DD\n\n### Added\n- Item one\n- Item two\n\n### Fixed\n- Item three\n</code></pre> <p>Replace YYYY-MM-DD with today's date. Use \"Unreleased\" as the version unless the user specifies a version number.</p>"},{"location":"chapters/03/#step-4-present-output","title":"Step 4: Present Output","text":"<p>Display the formatted changelog entry to the user and ask: \"Does this changelog entry look correct? If you have a version number to use instead of 'Unreleased', provide it now.\" <pre><code>**What is different at Stage 2.**\n\nThe skill now has a defined structure: an Overview section, a When to Use section, and a four-step workflow. Invoke this skill in two separate sessions and you will see consistent behavior: Claude runs git log, categorizes the commits, formats the output in Keep a Changelog format, and presents it for review.\n\nThe description field in the frontmatter has also been improved. It now specifies the output format (Keep a Changelog) and the trigger condition more precisely. This helps Claude identify when to suggest the skill during normal conversation.\n\n!!! warning \"Common mistake at this stage\"\n    Beginning builders often write workflow steps that are too vague. \"Step 1: Get the changes\" is not actionable. \"Step 1: Run `git log --oneline --since='30 days ago' --no-merges` and parse the output\" is actionable. Every step should specify exactly what Claude should do, not just what the step is about.\n\n---\n\n## 3.4 Stage 3: Adding User Dialog and File Writing\n\nThe Stage 2 skill produces good output, but it does not save it anywhere. It also does not handle the case where the project already has a CHANGELOG.md that needs updating rather than creating from scratch. Stage 3 adds both.\n\n```markdown\n---\nname: changelog-generator\ndescription: &gt;\n  Generates a formatted CHANGELOG.md entry from recent git commits or\n  a description of recent changes. Use this skill when you need to document\n  what changed in a project between two points in time. Output follows\n  Keep a Changelog format (keepachangelog.com). Reads existing CHANGELOG.md\n  if present and prepends the new entry.\n---\n\n# Changelog Generator\n\n## Overview\n\nThis skill produces a properly formatted `CHANGELOG.md` entry documenting\nrecent changes to a project. It follows the Keep a Changelog format\n(keepachangelog.com) with sections for Added, Changed, Fixed, Removed,\nDeprecated, and Security changes.\n\nIf a CHANGELOG.md already exists in the project root, this skill reads\nthe existing file and prepends the new entry, preserving all prior history.\n\n## When to Use This Skill\n\nUse this skill when:\n\n- You are preparing a release and need to document what changed\n- You want to update CHANGELOG.md after a sprint or milestone\n- A collaborator asks what changed since the last version\n\n**Prerequisites:**\n\n- Project must be a git repository OR the user can describe changes manually\n- Must be invoked from the project root directory\n\nDo NOT use this skill if:\n\n- You need automated changelog generation from CI/CD (use a dedicated tool)\n- The project has no version history or semantic versioning scheme\n\n## Workflow\n\n### Step 1: Check for Existing Changelog\n\nCheck if a CHANGELOG.md file already exists in the current directory:\n\n```bash\nls -la CHANGELOG.md 2&gt;/dev/null\n</code></pre></p> <p>If CHANGELOG.md exists, read its current contents to preserve them when writing the updated file.</p>"},{"location":"chapters/03/#step-2-gather-change-information","title":"Step 2: Gather Change Information","text":"<p>Run the following command to get recent git commits:</p> <pre><code>git log --oneline --since=\"30 days ago\" --no-merges\n</code></pre> <p>If git is not available or the project is not a git repository, ask: \"This project does not appear to use git. Please describe the changes made since the last release, one change per line.\"</p> <p>Wait for the user response before proceeding to Step 3.</p> <p>If git IS available, proceed automatically to Step 3.</p>"},{"location":"chapters/03/#step-3-confirm-scope-with-user","title":"Step 3: Confirm Scope with User","text":"<p>Display the raw commit list to the user and ask: \"Here are the commits from the last 30 days. Should I use all of them, or would you like to specify a different date range or version boundary? Also, what version number should I use? (Press Enter to use 'Unreleased')\"</p> <p>Wait for user response before proceeding.</p>"},{"location":"chapters/03/#step-4-categorize-changes","title":"Step 4: Categorize Changes","text":"<p>Review the commits or user-provided descriptions and categorize each into the appropriate Keep a Changelog section:</p> <ul> <li>Added: New features or capabilities (feat: prefix in conventional commits)</li> <li>Changed: Changes to existing functionality (refactor:, perf: prefixes)</li> <li>Fixed: Bug fixes (fix: prefix)</li> <li>Removed: Removed features (feat: with removal language)</li> <li>Deprecated: Features marked for future removal</li> <li>Security: Security fixes (security: prefix or CVE mentions)</li> </ul> <p>Omit sections that have no entries.</p> <p>Rewrite commit messages for readability. Conventional commit prefixes (feat:, fix:, chore:) should be removed. Cryptic internal references should be paraphrased. Write entries as a human would read them, not as a developer wrote them at 2 AM.</p>"},{"location":"chapters/03/#step-5-format-the-changelog-entry","title":"Step 5: Format the Changelog Entry","text":"<p>Format the new entry:</p> <pre><code>## [VERSION] - YYYY-MM-DD\n\n### Added\n- Item one, written in plain language\n\n### Fixed\n- Item two, written in plain language\n</code></pre> <p>Replace VERSION with the user-provided version or \"Unreleased\". Replace YYYY-MM-DD with today's date.</p>"},{"location":"chapters/03/#step-6-write-the-file","title":"Step 6: Write the File","text":"<p>If CHANGELOG.md did not exist: write the new entry with a standard header.</p> <p>Standard header format: <pre><code># Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).\n\n## [VERSION] - YYYY-MM-DD\n\n...entries...\n</code></pre></p> <p>If CHANGELOG.md already existed: prepend the new entry after the header (the first <code>##</code> heading), preserving all existing entries below it.</p> <p>Confirm with the user before overwriting an existing file: \"I will prepend the new entry to your existing CHANGELOG.md. Does this look correct? (y/n)\"</p> <p>Wait for confirmation. If \"n\", ask what changes the user wants and revise before writing.</p>"},{"location":"chapters/03/#step-7-confirm-completion","title":"Step 7: Confirm Completion","text":"<p>After writing the file, display: \"CHANGELOG.md updated. New entry: [show the entry] Run <code>git diff CHANGELOG.md</code> to review the full change.\"</p>"},{"location":"chapters/03/#output-files","title":"Output Files","text":"<ul> <li><code>CHANGELOG.md</code> \u2014 Created or updated in the current directory <pre><code>**What is different at Stage 3.**\n\nThree significant additions appear in this version:\n\n**Prerequisite specification.** The \"When to Use This Skill\" section now lists explicit prerequisites. Claude will check these before proceeding and report missing prerequisites rather than failing mid-workflow.\n\n**Conditional user dialog.** Step 2 branches: if git is not available, Claude asks the user for changes. If git is available, Claude proceeds automatically. This conditional logic makes the skill robust to different project setups without requiring the user to tell Claude which case applies.\n\n**Destructive action confirmation.** Step 6 asks the user to confirm before overwriting an existing file. This is a critical pattern for any skill that writes files. Never overwrite without asking.\n\n---\n\n## 3.5 Stage 4: Adding Quality Scoring\n\nQuality scoring is what separates skills from scripts. A script either produces output or fails. A skill produces output, evaluates that output against defined criteria, and reports gaps before handing the result to the user.\n\nAdd the following sections to the end of your Stage 3 skill:\n\n```markdown\n## Quality Scoring\n\nBefore confirming completion, evaluate the changelog entry against this\nrubric. Score each item and calculate the total. If total score is below\n75/100, flag the specific gaps and offer to revise.\n\n| Criterion | Points | Evaluation |\n|-----------|--------|------------|\n| All commits accounted for | 20 | Every commit in the selected range appears in at least one section |\n| Correct categorization | 20 | Added/Changed/Fixed/Removed used per Keep a Changelog definitions |\n| Plain language | 20 | No commit hashes, branch names, internal jargon, or raw prefixes |\n| Completeness | 15 | No empty sections included; all populated sections present |\n| Date accuracy | 10 | Date matches today's date (verify before writing) |\n| Version format | 10 | Version is \"Unreleased\" or follows semver (X.Y.Z) |\n| File integrity | 5 | Existing CHANGELOG.md history preserved, no content removed |\n| **Total** | **100** | **Minimum passing score: 75** |\n\nIf score &lt; 75: Present the gap report with specific items that failed,\nask the user if they want revisions before writing the file.\n\n## Example Session\n\n**User:** `/skill changelog-generator`\n\n**Claude:** Checking for existing CHANGELOG.md... Found. Reading current contents.\n\nRunning git log for the last 30 days...\n\nFound 12 commits. Here they are:\n- a1b2c3d feat: add dark mode toggle\n- d4e5f6g fix: broken link in header nav\n- h7i8j9k chore: update dependencies\n- ... (9 more)\n\nShould I use all of these, or would you like a different date range?\nWhat version number? (Enter for \"Unreleased\")\n\n**User:** Use all of them. Version is 2.3.0.\n\n**Claude:** Categorizing and formatting...\n\nQuality check: 95/100. One dependency update commit categorized as\n\"Changed\" since it affects behavior \u2014 confirming this is correct.\n\nNew entry looks like:\n\n## [2.3.0] - 2025-09-14\n\n### Added\n- Dark mode toggle for user interface\n\n### Fixed\n- Broken navigation link in site header\n\n### Changed\n- Updated third-party library dependencies\n\nPrepending to CHANGELOG.md. Confirm? (y/n)\n\n**User:** y\n\n**Claude:** CHANGELOG.md updated. Run `git diff CHANGELOG.md` to review.\n\n## Common Pitfalls\n\n### Pitfall 1: Running git log from the wrong directory\n\nIf invoked from a subdirectory, git log may produce incorrect results or\nfail. Always verify you are in the project root before running git commands.\n\n**Prevention:** Add `pwd &amp;&amp; git rev-parse --show-toplevel` as the first\nbash command in Step 2 to confirm the current directory is the git root.\n\n### Pitfall 2: Including merge commits\n\nMerge commits appear in git log by default and produce changelog noise.\nAlways use `--no-merges` flag.\n\n### Pitfall 3: 30-day window misses release boundary\n\nThe 30-day window may include commits from before the last release or\nmiss commits that are part of this release. The Step 3 confirmation prompt\nexists to catch this \u2014 but users often skip it without reviewing.\n\n**Prevention:** The Step 3 prompt should display the oldest and newest\ncommit dates, making it obvious whether the window is correct.\n\n### Pitfall 4: Conventional commit prefix pollution\n\nCommits that follow conventional commit format (feat:, fix:, chore:)\nshould have their prefixes stripped in the output. \"feat: add dark mode\"\nbecomes \"Added dark mode\". The skill already instructs this in Step 4,\nbut review the output before confirming.\n</code></pre></li> </ul> <p>What is different at Stage 4.</p> <p>The quality scoring rubric gives Claude a framework for evaluating its own output. The numerical scoring creates a threshold (75/100) below which Claude must flag gaps rather than silently deliver substandard results. The example session shows exactly what a correct interaction looks like. The common pitfalls section encodes hard-won knowledge about what goes wrong.</p>"},{"location":"chapters/03/#36-where-to-save-the-skill","title":"3.6 Where to Save the Skill","text":"<p>The complete Stage 4 skill lives at:</p> <pre><code>~/.claude/skills/changelog-generator/SKILL.md\n</code></pre> <p>Create the directory if it does not exist:</p> <pre><code>mkdir -p ~/.claude/skills/changelog-generator\n</code></pre> <p>Then write the file with the Stage 4 content.</p> <p>To confirm the skill is registered, start a new Claude Code session and run <code>/skills</code>. You should see <code>changelog-generator</code> in the list.</p> <p>Project-local vs. global installation</p> <p>If the <code>changelog-generator</code> skill is useful across all your projects (it is), install it globally in <code>~/.claude/skills/</code>. If you are building a skill that is specific to one project \u2014 perhaps one that reads from that project's specific data files \u2014 install it project-locally in <code>.claude/skills/</code> within the project root.</p>"},{"location":"chapters/03/#37-testing-your-skill","title":"3.7 Testing Your Skill","text":"<p>Testing a skill means invoking it and observing whether it behaves as specified. This is different from testing software: you are not looking for deterministic output, you are looking for consistent, high-quality output that follows the workflow.</p> <p>Test 1: First invocation. Start a new session in a git project directory. Type <code>/skill changelog-generator</code>. Observe each step. Does Claude run git log? Does it ask for version confirmation? Does it format the output correctly?</p> <p>Test 2: Existing CHANGELOG.md. Create a minimal <code>CHANGELOG.md</code> in the project root. Invoke the skill again. Does Claude detect the existing file? Does it ask for confirmation before overwriting?</p> <p>Test 3: Non-git project. Invoke the skill from a directory with no git repository. Does Claude detect the missing git history and ask you to describe changes manually?</p> <p>Test 4: Quality scoring trigger. After Claude produces the changelog entry, examine it critically. Does the quality score seem accurate? Are the common pitfalls section's failure modes reflected in the rubric?</p> <p>Record what you observe. If behavior deviates from specification, the fix is in the skill definition, not in how you invoke it.</p>"},{"location":"chapters/03/#38-the-iteration-loop","title":"3.8 The Iteration Loop","text":"<p>Skills improve through a tight loop of invocation, observation, and refinement. The loop has four steps:</p> <pre><code>flowchart LR\n    A[Invoke the skill] --&gt; B[Observe actual behavior]\n    B --&gt; C[Compare to intended behavior]\n    C --&gt; D{Gap found?}\n    D -- Yes --&gt; E[Edit SKILL.md to close the gap]\n    E --&gt; A\n    D -- No --&gt; F[Skill is working as intended]</code></pre> <p>Common gaps and their fixes:</p> Observed behavior Likely cause Fix Output format varies between invocations Workflow step too vague Add explicit format specification Claude skips a step Step 2 does not follow from Step 1 clearly Add explicit \"then proceed to Step X\" instructions Claude asks unnecessary questions Step does not specify when to proceed automatically Add \"If X is true, proceed automatically\" Claude does not ask at critical decision points Missing user dialog trigger Add explicit \"Ask the user: '...'\" instruction Quality score never below 75 Rubric criteria too lenient Raise point values for critical criteria Wrong files written Output section not specific enough Add exact file paths and formats to Output section Skill invoked for wrong task Description too broad Narrow the description and add \"Do NOT use if\" criteria <p>Most gaps are in the workflow steps: either too vague, missing conditional logic, or not specifying when to pause for user input. The example session section of a skill is particularly valuable here \u2014 if you write a precise example session and Claude's actual behavior matches it, the skill is working correctly. If there is a divergence, the skill definition needs adjustment.</p> <p>Do not compensate for a bad skill definition by reprompting</p> <p>When a skill does not behave correctly, the instinct is to add clarifying instructions during the session: \"No, I meant for you to do X first.\" Resist this. Instead, identify what the skill definition lacks and add it there. The fix should be in the skill, not in the session. If you fix it in the session, the problem will recur on the next invocation.</p>"},{"location":"chapters/03/#39-the-complete-stage-4-skill","title":"3.9 The Complete Stage 4 Skill","text":"<p>Here is the complete, production-ready <code>changelog-generator</code> skill in a single code block for easy copying:</p> <pre><code>---\nname: changelog-generator\ndescription: &gt;\n  Generates a formatted CHANGELOG.md entry from recent git commits or\n  a description of recent changes. Use this skill when you need to document\n  what changed in a project between two points in time. Output follows\n  Keep a Changelog format (keepachangelog.com). Reads existing CHANGELOG.md\n  if present and prepends the new entry.\n---\n\n# Changelog Generator\n\n## Overview\n\nThis skill produces a properly formatted CHANGELOG.md entry documenting\nrecent changes to a project. It follows the Keep a Changelog format\n(keepachangelog.com) with sections for Added, Changed, Fixed, Removed,\nDeprecated, and Security changes.\n\nIf a CHANGELOG.md already exists in the project root, this skill reads\nthe existing file and prepends the new entry, preserving all prior history.\n\n## When to Use This Skill\n\nUse this skill when:\n\n- You are preparing a release and need to document what changed\n- You want to update CHANGELOG.md after a sprint or milestone\n- A collaborator asks what changed since the last version\n\n**Prerequisites:**\n\n- Project must be a git repository OR the user can describe changes manually\n- Must be invoked from the project root directory\n\nDo NOT use this skill if:\n\n- You need automated changelog generation from CI/CD (use a dedicated tool)\n- The project has no version history or semantic versioning scheme\n\n## Workflow\n\n### Step 1: Check for Existing Changelog\n\nCheck if a CHANGELOG.md file already exists in the current directory:\n\n```bash\nls -la CHANGELOG.md 2&gt;/dev/null\n</code></pre> <p>If CHANGELOG.md exists, read its current contents to preserve them when writing the updated file.</p>"},{"location":"chapters/03/#step-2-gather-change-information_1","title":"Step 2: Gather Change Information","text":"<p>Run the following command to get recent git commits:</p> <pre><code>git log --oneline --since=\"30 days ago\" --no-merges\n</code></pre> <p>If git is not available or the project is not a git repository, ask: \"This project does not appear to use git. Please describe the changes made since the last release, one change per line.\"</p> <p>Wait for the user response before proceeding to Step 3.</p> <p>If git IS available, proceed automatically to Step 3.</p>"},{"location":"chapters/03/#step-3-confirm-scope-with-user_1","title":"Step 3: Confirm Scope with User","text":"<p>Display the raw commit list to the user and ask: \"Here are the commits from the last 30 days. Should I use all of them, or would you like to specify a different date range or version boundary? Also, what version number should I use? (Press Enter to use 'Unreleased')\"</p> <p>Wait for user response before proceeding.</p>"},{"location":"chapters/03/#step-4-categorize-changes_1","title":"Step 4: Categorize Changes","text":"<p>Review the commits or user-provided descriptions and categorize each into the appropriate Keep a Changelog section:</p> <ul> <li>Added: New features or capabilities</li> <li>Changed: Changes to existing functionality</li> <li>Fixed: Bug fixes</li> <li>Removed: Removed features</li> <li>Deprecated: Features marked for future removal</li> <li>Security: Security fixes or CVE mentions</li> </ul> <p>Omit sections that have no entries.</p> <p>Rewrite commit messages for readability. Remove conventional commit prefixes. Paraphrase cryptic references. Write entries as a human would read them.</p>"},{"location":"chapters/03/#step-5-format-the-changelog-entry_1","title":"Step 5: Format the Changelog Entry","text":"<p>Format the new entry:</p> <pre><code>## [VERSION] - YYYY-MM-DD\n\n### Added\n- Item written in plain language\n\n### Fixed\n- Item written in plain language\n</code></pre> <p>Replace VERSION with the user-provided version or \"Unreleased\". Replace YYYY-MM-DD with today's date.</p>"},{"location":"chapters/03/#step-6-write-the-file_1","title":"Step 6: Write the File","text":"<p>If CHANGELOG.md did not exist: write the new entry with a standard header:</p> <pre><code># Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on Keep a Changelog (https://keepachangelog.com/en/1.0.0/).\n\n## [VERSION] - YYYY-MM-DD\n...entries...\n</code></pre> <p>If CHANGELOG.md already existed: prepend the new entry after the first line of the existing file header, before any existing version entries. Preserve all existing entries.</p> <p>Confirm with the user before writing: \"I will prepend the new entry to your existing CHANGELOG.md. Confirm? (y/n)\"</p> <p>Wait for confirmation. If \"n\", ask what revisions the user wants.</p>"},{"location":"chapters/03/#step-7-confirm-completion_1","title":"Step 7: Confirm Completion","text":"<p>After writing, display: \"CHANGELOG.md updated. Run <code>git diff CHANGELOG.md</code> to review.\"</p>"},{"location":"chapters/03/#output-files_1","title":"Output Files","text":"<ul> <li><code>CHANGELOG.md</code> \u2014 Created or updated in the current directory</li> </ul>"},{"location":"chapters/03/#quality-scoring","title":"Quality Scoring","text":"<p>Evaluate the changelog entry before writing. Score each criterion. If total is below 75/100, flag gaps and offer to revise.</p> Criterion Points All commits accounted for 20 Correct categorization 20 Plain language (no raw prefixes or hashes) 20 Completeness (no empty sections included) 15 Date accuracy 10 Version format (Unreleased or X.Y.Z) 10 File integrity (existing history preserved) 5 Total 100"},{"location":"chapters/03/#example-session","title":"Example Session","text":"<p>User: /skill changelog-generator</p> <p>Claude: Checking for CHANGELOG.md... Found. Reading current contents.</p> <p>Running git log for last 30 days... Found 12 commits.</p> <p>Here they are: [commit list]. Should I use all of them, or a different range? What version number? (Enter for Unreleased)</p> <p>User: All of them. Version 2.3.0.</p> <p>Claude: [formats entry, scores 95/100, confirms with user, writes file] \"CHANGELOG.md updated. Run git diff CHANGELOG.md to review.\"</p>"},{"location":"chapters/03/#common-pitfalls","title":"Common Pitfalls","text":"<ul> <li>Running from a subdirectory: git log may produce incorrect results.   Verify you are in the project root.</li> <li>Including merge commits: always use --no-merges.</li> <li>30-day window misses release boundary: the Step 3 prompt catches this,   but display oldest/newest commit dates to make the window obvious.</li> <li>Conventional commit prefix pollution: strip feat:, fix:, chore: prefixes   and rewrite entries in plain language. <pre><code>---\n\n## 3.10 Production-Readiness Checklist\n\nYour skill is production-ready when it satisfies all of the following:\n</code></pre> Frontmatter   [ ] name matches the directory name exactly   [ ] description is 2-4 sentences specifying trigger condition and output   [ ] description written from Claude's routing perspective, not documentation perspective</li> </ul> <p>Structure   [ ] Overview section explains purpose and primary output in 1-2 sentences   [ ] \"When to Use\" includes positive conditions, negative conditions, and prerequisites   [ ] Workflow steps are numbered and sequentially ordered   [ ] Each step specifies exactly what Claude should do (not just what the step is about)   [ ] Conditional logic is explicit (\"If X, do Y. Otherwise, do Z.\")   [ ] User dialog triggers are explicit (\"Ask the user: '...'\")   [ ] Destructive actions (file writes, overwrites) require explicit user confirmation</p> <p>Quality   [ ] Quality scoring rubric with specific criteria and point values   [ ] Minimum passing score defined (75/100 recommended)   [ ] Gap report behavior specified (what to do if score is below threshold)</p> <p>Documentation   [ ] Output files section lists every file the skill creates, with paths and formats   [ ] Example session shows correct interaction from invocation to completion   [ ] Common pitfalls section covers at least three known failure modes</p> <p>Testing   [ ] Skill has been invoked at least twice in separate sessions   [ ] Output is consistent between invocations   [ ] Conditional branches (e.g., git vs. no git) have each been tested   [ ] Quality scoring has been triggered at least once (introduce a flaw to test it)   [ ] Edge cases (empty input, existing files, user saying \"no\" at confirmations) tested ```</p> <p>This checklist is the answer to \"is my skill done?\" If every item is checked, the skill will behave consistently, produce quality-gated output, and handle the failure cases you know about. New failure cases will be discovered through use and added to the common pitfalls section over time.</p>"},{"location":"chapters/03/#311-what-comes-next","title":"3.11 What Comes Next","text":"<p>You have built a complete skill from scratch, understood how each component affects Claude's behavior, and established the iteration loop for improving skills over time. The <code>changelog-generator</code> demonstrates all the core patterns:</p> <ul> <li>Frontmatter for registration</li> <li>A \"When to Use\" section with positive conditions, negative conditions, and prerequisites</li> <li>Numbered workflow steps with explicit instructions</li> <li>Conditional logic for branching based on environment state</li> <li>User dialog triggers at decision points and before destructive actions</li> <li>Quality scoring with a rubric and a minimum threshold</li> <li>An example session showing correct behavior</li> <li>A common pitfalls section encoding known failure modes</li> </ul> <p>Chapter 4 goes deeper into the SKILL.md structure, examining each section in detail and explaining the design decisions behind the patterns you used in this chapter. Chapter 5 covers YAML frontmatter in depth, including the <code>allowed-tools</code> field and how to constrain skill capabilities. Chapter 6 addresses workflow design as a discipline \u2014 how to decompose complex tasks into workflow steps that produce consistent, high-quality output.</p> <p>Build your own first skill in parallel</p> <p>The <code>changelog-generator</code> is a learning vehicle. Your most valuable next step is to identify a real workflow in your own work and build a skill for it using the same four-stage process. Use the production-readiness checklist at Stage 4. The workflow you chose when reading the \"Before moving on\" tip at the end of Chapter 1 is your starting point.</p> <p>Key terms from this chapter</p> <ul> <li>Minimal viable skill: A SKILL.md containing only YAML frontmatter \u2014 the minimum for Claude Code to register the skill</li> <li>Workflow step: A numbered instruction in the skill body specifying exactly what Claude should do at that stage of execution</li> <li>User dialog trigger: An explicit instruction telling Claude to pause and ask the user a specific question before proceeding</li> <li>Conditional logic: Branching instructions in the skill body that tell Claude to take different actions based on the current state</li> <li>Destructive action confirmation: The pattern of requiring explicit user confirmation before writing, overwriting, or deleting files</li> <li>Quality scoring rubric: A table of criteria and point values that Claude uses to evaluate its own output before delivering it</li> <li>Minimum passing score: The threshold below which Claude must flag gaps and offer to revise rather than silently delivering substandard output</li> <li>Iteration loop: The cycle of invoke, observe, compare, edit that refines a skill toward correct and consistent behavior</li> </ul>"},{"location":"chapters/04/","title":"Chapter 4: SKILL.md Structure","text":""},{"location":"chapters/04/#the-complete-anatomy-of-a-skill-definition-file","title":"The Complete Anatomy of a Skill Definition File","text":"<p>Every skill in the Claude Code ecosystem lives in a single file: <code>SKILL.md</code>. Understanding every section of this file \u2014 what it does, why it exists, and what breaks without it \u2014 is the foundation of skill authorship.</p> <p>This chapter dissects the SKILL.md format completely. By the end, you will be able to read any skill file and understand exactly what Claude will do when it executes it. You will also understand the design decisions behind the format and be able to make intentional choices about every section you write.</p>"},{"location":"chapters/04/#what-a-skillmd-file-actually-is","title":"What a SKILL.md File Actually Is","text":"<p>A <code>SKILL.md</code> file is simultaneously three things:</p> <ol> <li>A configuration document \u2014 the YAML frontmatter tells Claude Code metadata about the skill: its name, what it does, what tools it is allowed to use, and its license.</li> <li>An instruction set \u2014 the markdown body is a workflow that Claude reads and executes step by step.</li> <li>A context injection \u2014 portions of the file, particularly the <code>description</code> field, are injected directly into Claude's context window as part of its operational instructions.</li> </ol> <p>The file format was designed to be human-readable first. A skill author should be able to read a SKILL.md file and immediately understand what the skill does, when to use it, and what it produces. Claude reads the same file. There is no separate \"machine-readable\" format \u2014 the human instructions and the machine instructions are identical.</p> <p>This design principle has a practical consequence: clarity in your SKILL.md is not a nicety, it is a functional requirement. Vague instructions produce inconsistent behavior. Precise instructions produce consistent, reproducible output.</p>"},{"location":"chapters/04/#document-structure-overview","title":"Document Structure Overview","text":"<pre><code>graph TD\n    A[SKILL.md] --&gt; B[YAML Frontmatter]\n    A --&gt; C[Markdown Body]\n\n    B --&gt; B1[name field]\n    B --&gt; B2[description field]\n    B --&gt; B3[license field]\n    B --&gt; B4[allowed-tools field]\n\n    C --&gt; C1[H1 Title]\n    C --&gt; C2[Version - optional]\n    C --&gt; C3[Overview / Purpose]\n    C --&gt; C4[When to Use This Skill]\n    C --&gt; C5[Workflow Steps]\n    C --&gt; C6[Output Files Summary]\n    C --&gt; C7[Example Session]\n    C --&gt; C8[Common Pitfalls]\n    C --&gt; C9[Quality Standards]\n\n    C5 --&gt; C5a[Step 0: Environment Setup]\n    C5 --&gt; C5b[Step 1: First Action]\n    C5 --&gt; C5c[Step N: Final Action]</code></pre>"},{"location":"chapters/04/#section-by-section-anatomy","title":"Section-by-Section Anatomy","text":""},{"location":"chapters/04/#the-yaml-frontmatter-block","title":"The YAML Frontmatter Block","text":"<p>The frontmatter is the first section of any SKILL.md file. It is enclosed between triple-dash delimiters (<code>---</code>) and written in YAML syntax.</p> <pre><code>---\nname: glossary-generator\ndescription: This skill automatically generates a comprehensive glossary of terms from a learning graph's concept list, ensuring each definition follows ISO 11179 metadata registry standards (precise, concise, distinct, non-circular, and free of business rules). Use this skill when creating a glossary for an intelligent textbook after the learning graph concept list has been finalized.\nlicense: MIT\nallowed-tools: Bash(~/.claude/skills/glossary-generator:*)\n---\n</code></pre> <p>The frontmatter serves as the skill's identity document. Claude Code reads this before loading the rest of the file and uses it to:</p> <ul> <li>Register the skill under its <code>name</code></li> <li>Surface the <code>description</code> when the user requests skill discovery</li> <li>Enforce tool permissions from <code>allowed-tools</code></li> <li>Record the <code>license</code> for distribution purposes</li> </ul> <p>What happens if frontmatter is missing or malformed?</p> <p>If the frontmatter is absent, Claude Code will not recognize the file as a valid skill. The skill will not appear in skill listings, and invoking it by name will fail. If the YAML is syntactically invalid (for example, incorrect indentation or unescaped special characters), the skill may load but with corrupted metadata.</p>"},{"location":"chapters/04/#the-h1-title","title":"The H1 Title","text":"<p>The first line of the markdown body should be an H1 heading that names the skill in plain English.</p> <pre><code># Glossary Generator\n</code></pre> <p>This is distinct from the <code>name</code> field in the frontmatter. The frontmatter <code>name</code> is the machine identifier (kebab-case, used for invocation). The H1 title is the human-readable name displayed in documentation, logs, and when Claude announces what it is about to do.</p> <p>Best practice: The H1 title should match the skill's purpose directly. If the frontmatter <code>name</code> is <code>course-description-analyzer</code>, the H1 title might be <code>Course Description Analyzer</code> or <code>Course Description Analyzer and Generator</code>. Avoid H1 titles that are clever or metaphorical \u2014 they reduce clarity without adding value.</p>"},{"location":"chapters/04/#the-version-line-optional","title":"The Version Line (Optional)","text":"<p>Immediately after the H1 title, you may include a version declaration:</p> <pre><code># Learning Graph Generator\n\n**Version:** 0.03\n</code></pre> <p>Versioning serves multiple purposes:</p> <ul> <li>It gives users a way to report which version they used when filing bug reports</li> <li>It gives Claude a way to announce its version at the start of a session, which helps users confirm the correct skill loaded</li> <li>It forces the skill author to think of skills as evolving artifacts, not static documents</li> </ul> <p>A convention used in the McCreary skill ecosystem is to have Claude announce its version in Step 0:</p> <pre><code>Tell the user that they are running Version 0.03 of the Learning Graph Generator Skill.\n</code></pre> <p>This creates an explicit handshake at session start that confirms skill identity and version.</p>"},{"location":"chapters/04/#overview-purpose","title":"Overview / Purpose","text":"<p>The overview section follows immediately after the version (or H1 if no version is used). It contains one to three paragraphs that explain:</p> <ol> <li>What the skill does \u2014 the core transformation it performs</li> <li>The domain context \u2014 why this matters and where it fits in a larger workflow</li> <li>The output quality target \u2014 what standards govern the output</li> </ol> <pre><code>## Purpose\n\nThis skill automates glossary creation for intelligent textbooks by converting concept labels\nfrom a learning graph into properly formatted glossary definitions. Each definition follows\nISO 11179 metadata registry standards: precise, concise, distinct, non-circular, and free of\nbusiness rules. The skill ensures consistency across terminology, validates cross-references,\nand produces alphabetically ordered entries with relevant examples.\n</code></pre> <p>The overview is important for two reasons. First, it gives Claude immediate context for interpreting every subsequent instruction. If Step 4 says \"generate a definition,\" Claude already knows from the overview that the definition must meet ISO 11179 standards \u2014 it does not need to be told again. Second, the overview is what a new user will read to determine whether this skill is appropriate for their current task.</p>"},{"location":"chapters/04/#when-to-use-this-skill","title":"When to Use This Skill","text":"<p>This section is a bulleted list of specific trigger conditions. It answers the question: \"How do I know whether to invoke this skill right now?\"</p> <pre><code>## When to Use This Skill\n\nUse this skill after the Learning Graph skill has completed and the concept list has been finalized.\nAll markdown content in the /docs area can also be scanned looking for words or phases that might\nnot be clear to the average high-school student.\n\nSpecifically, trigger this skill when:\n\n- A concept list file exists (typically `docs/learning-graph/02-concept-list-v1.md`)\n- The concept list has been reviewed and approved\n- The course description exists with clear learning outcomes\n- Ready to create or update the textbook's glossary\n</code></pre> <p>This section matters because skills in a pipeline have dependencies. A glossary generator depends on a finalized concept list. A chapter content generator depends on a finalized learning graph. The \"When to Use This Skill\" section makes those dependencies explicit as conditions the user can check before invoking.</p> <p>The section also helps with skill discovery. When a user describes what they want to do and asks Claude to suggest a skill, Claude reads the \"When to Use\" sections of available skills and matches them against the user's stated context.</p> <p>Design guidance: Write trigger conditions as things you can verify by checking the filesystem or the current project state. Avoid vague conditions like \"when you feel the project is ready.\" Use specific, checkable conditions like \"a concept list file exists at <code>docs/learning-graph/02-concept-list-v1.md</code>.\"</p>"},{"location":"chapters/04/#the-workflow-section","title":"The Workflow Section","text":"<p>The workflow is the most important section of any SKILL.md file. It is a numbered sequence of steps that Claude executes in order. Each step is a heading (<code>### Step N: Step Name</code>) followed by detailed instructions.</p> <p>The standard numbering convention starts at Step 0, not Step 1. Step 0 is conventionally reserved for environment setup \u2014 this is a pattern from the McCreary skill methodology, not an agentskills.io spec requirement, but it is widely used because it separates configuration concerns cleanly from execution.</p> <pre><code>## Workflow\n\n### Step 0: Setup\n\nTell the user that they are running the version graph generator and the version number above.\n\nThe default context is that the skill is run from claude code in the home directory of an\nintelligent textbook that has been checked out from GitHub.\n\nDetect the project root by looking for mkdocs.yml in the current directory or parent directories.\nSet the following path variables for use throughout this skill:\n\n- PROJECT_ROOT: directory containing mkdocs.yml\n- DOCS_DIR: PROJECT_ROOT/docs\n- LOG_DIR: PROJECT_ROOT/logs\n\nIf DOCS_DIR does not exist, halt and ask the user to confirm the working directory.\n</code></pre> <p>Each subsequent step performs one cohesive action or decision:</p> <pre><code>### Step 1: Validate Input Quality\n\nBefore generating definitions, assess the quality of the concept list:\n\n1. Read the concept list file (typically `docs/learning-graph/02-concept-list-v1.md`)\n2. Check for duplicate concept labels (target: 100% unique)\n3. Verify Title Case formatting (target: 95%+ compliance)\n4. Validate length constraints (target: 98% under 32 characters)\n5. Assess concept clarity (no ambiguous terms)\n\nCalculate a quality score (1-100 scale):\n\n- 90-100: All concepts unique, properly formatted, appropriate length\n- 70-89: Most concepts meet standards, minor formatting issues\n- 50-69: Some duplicate concepts or formatting inconsistencies\n- Below 50: Significant issues requiring manual review\n\n**User Dialog Triggers:**\n\n- If score &lt; 70: Ask \"The concept list has quality issues. Would you like to review and clean\n  it before generating the glossary?\"\n- If duplicates found: Ask \"Found [N] duplicate concepts. Should I remove duplicates\n  automatically or would you like to review?\"\n</code></pre> <p>Notice the structure within each step:</p> <ul> <li>Ordered sub-tasks (numbered list) for actions that must happen in sequence</li> <li>Decision tables for threshold-based logic</li> <li>User Dialog Triggers as a distinct labeled block so Claude recognizes when to pause and ask</li> </ul>"},{"location":"chapters/04/#output-files-summary","title":"Output Files Summary","text":"<p>This section documents every file the skill creates, organized by priority.</p> <pre><code>## Output Files Summary\n\n**Required:**\n\n1. `docs/glossary.md` \u2014 Complete glossary in alphabetical order with ISO 11179-compliant definitions\n\n**Recommended:**\n\n2. `docs/learning-graph/glossary-quality-report.md` \u2014 Quality assessment and recommendations\n\n**Optional:**\n\n3. `docs/learning-graph/glossary-cross-ref.json` \u2014 JSON mapping for semantic search\n4. Updates to `mkdocs.yml` navigation if glossary link is missing\n</code></pre> <p>The three-tier classification (Required / Recommended / Optional) serves a specific purpose. It tells Claude \u2014 and the user \u2014 which outputs are essential for success, which are valuable but skippable under time pressure, and which are extras that add value but do not block downstream steps.</p> <p>Without this section, Claude may produce different files on different runs depending on how it interprets the workflow steps. The Output Files Summary creates a contract: if the skill completes successfully, these files will exist. Downstream skills that depend on specific file paths from this skill can rely on that contract.</p>"},{"location":"chapters/04/#example-session","title":"Example Session","text":"<p>The example session shows a realistic interaction \u2014 what the user says, what Claude does, and what output is produced.</p> <pre><code>## Example Session\n\n**User:** \"Generate a glossary from my concept list\"\n\n**Claude (using this skill):**\n\n1. Reads `docs/learning-graph/02-concept-list-v1.md`\n2. Validates quality (checks for duplicates, formatting)\n3. Reads `docs/course-description.md` for context\n4. Generates ISO 11179-compliant definitions\n5. Adds examples to 70% of terms\n6. Sorts alphabetically\n7. Creates `docs/glossary.md`\n8. Generates quality report\n9. Reports: \"Created glossary with 187 terms. Overall quality score: 89/100.\n   Added examples to 71% of terms. No circular definitions found.\"\n</code></pre> <p>The example session accomplishes three things:</p> <ol> <li>It gives Claude a concrete model of the expected interaction pattern</li> <li>It gives users a preview of what to expect when they invoke the skill</li> <li>It serves as a quick smoke test \u2014 if a new version of the skill produces a wildly different interaction pattern than the example shows, something has changed</li> </ol>"},{"location":"chapters/04/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<p>This section documents recurring failure modes \u2014 things that go wrong in practice that are not obvious from reading the workflow steps.</p> <pre><code>## Common Pitfalls to Avoid\n\n**Circular Definitions:**\n- Bad: \"A Learning Graph is a graph that shows learning.\"\n- Good: \"A directed graph of concepts that reflects the order concepts should be learned.\"\n\n**Too Vague:**\n- Bad: \"A thing used in education.\"\n- Good: \"A directed graph of concepts that reflects prerequisite relationships.\"\n\n**Processing concepts before reading course context:**\nIf you skip Step 2 (Read Course Context) and go directly to generating definitions,\nthe definitions will be generic rather than tailored to this specific course's\nterminology and audience level.\n</code></pre> <p>The before/after format for anti-patterns is particularly effective because it gives Claude explicit negative examples. Claude learns to avoid the \"Bad\" patterns not through abstract rules but through concrete comparison.</p>"},{"location":"chapters/04/#quality-standards-and-scoring-rubric","title":"Quality Standards and Scoring Rubric","text":"<p>The quality scoring section defines the rubric Claude uses to assess its own output. This is covered in detail in Chapter 7. For now, understand its structural role:</p> <pre><code>## Quality Scoring Reference\n\nUse this rubric to score each definition (1-100 scale):\n\n**85-100: Excellent**\n- Meets all 4 ISO 11179 criteria (20+ pts each)\n- Appropriate length (20-50 words)\n- Includes relevant example\n- Clear, unambiguous language\n\n**70-84: Good**\n- Meets 3-4 ISO criteria\n- Acceptable length (15-60 words)\n- May lack example\n- Generally clear\n\n**Below 55: Needs Revision**\n- Fails multiple ISO criteria\n- Requires complete rewrite\n</code></pre> <p>The quality scoring section transforms Claude from a text generator into a self-auditing system. Before reporting completion, Claude compares its output against the rubric and either proceeds (if score &gt;= threshold) or flags the issue to the user (if score &lt; threshold).</p>"},{"location":"chapters/04/#a-fully-annotated-skillmd-example","title":"A Fully Annotated SKILL.md Example","text":"<p>The following is a complete, production-quality SKILL.md file with inline annotations explaining every design choice.</p> <pre><code>---\n# FRONTMATTER\n# name: Machine identifier. Must match the directory name. Kebab-case only.\nname: readme-generator\n\n# description: Single paragraph. Two sentences minimum.\n# Sentence 1: What the skill does.\n# Sentence 2: When to trigger it.\n# This exact text may appear in Claude's context window during skill selection.\ndescription: This skill generates a comprehensive GitHub README.md file for any\n  repository by analyzing existing code, documentation, and project structure.\n  Use this skill when a repository lacks a README or when the existing README\n  needs a complete overhaul to meet professional standards.\n\nlicense: MIT\n\n# allowed-tools: Restrict what Claude can do while running this skill.\n# Format: ToolName(path-glob:*) or just ToolName for no restriction.\n# Omitting this field allows all tools \u2014 risky for production skills.\nallowed-tools: Bash(~/.claude/skills/readme-generator:*), Bash(/tmp:*)\n---\n</code></pre> <pre><code># README Generator\n# H1 title: human-readable, matches skill purpose\n\n**Version:** 1.2\n# Version: enables announcements in Step 0 and helps with bug reports\n\n## Overview\n# Overview: 2-3 paragraphs. Establishes domain context for interpreting later steps.\n\nThis skill analyzes a repository's structure, code, documentation, and configuration\nfiles to produce a professional README.md. The output follows GitHub's best practices\nfor open-source documentation, including badges, installation instructions, usage\nexamples, and contribution guidelines.\n\nThe skill performs a read-only analysis pass before generating any output, ensuring\nit understands the project before making claims about it. Quality scoring validates\nthe output against a 100-point rubric before delivery.\n\n## When to Use This Skill\n# Trigger conditions: specific, checkable, filesystem-verifiable\n\n- No `README.md` exists in the repository root\n- Existing `README.md` is fewer than 200 words\n- Repository has been significantly updated but the README reflects old functionality\n- Preparing a repository for public release or portfolio presentation\n\n## Workflow\n\n### Step 0: Environment Setup\n# Step 0 is a McCreary methodology convention: always environment setup, always first.\n\nTell the user you are running README Generator version 1.2.\n\nDetect the repository root by looking for `.git/` in the current directory or\nparent directories. If not found, halt and ask: \"I cannot detect a git repository.\nPlease confirm your working directory.\"\n\nSet path variables:\n- REPO_ROOT: directory containing `.git/`\n- README_PATH: REPO_ROOT/README.md\n- LOG_PATH: REPO_ROOT/logs/readme-generator-[timestamp].md\n\nCreate the logs directory if it does not exist:\n```bash\nmkdir -p $REPO_ROOT/logs\n</code></pre>"},{"location":"chapters/04/#step-0-establishes-all-path-variables-once-all-later-steps-reference-these-variables","title":"Step 0 establishes all path variables once. All later steps reference these variables.","text":""},{"location":"chapters/04/#this-prevents-hardcoded-paths-that-break-when-the-skill-runs-in-different-projects","title":"This prevents hardcoded paths that break when the skill runs in different projects.","text":""},{"location":"chapters/04/#step-1-repository-analysis","title":"Step 1: Repository Analysis","text":""},{"location":"chapters/04/#each-step-has-a-single-cohesive-purpose","title":"Each step has a single, cohesive purpose","text":"<p>Perform a read-only scan of the repository structure:</p> <ol> <li>List all files in the root directory</li> <li>Detect the primary programming language from file extensions</li> <li>Check for <code>package.json</code>, <code>pyproject.toml</code>, <code>Cargo.toml</code>, <code>go.mod</code> \u2014 use whichever    exists to determine ecosystem and extract dependencies</li> <li>Read the first 50 lines of the main entry point if it can be detected</li> <li>Check for existing <code>docs/</code> directory or documentation files</li> </ol> <p>Record findings in a structured summary (do not write to disk yet).</p> <p>Skip-if-complete detection: If <code>README.md</code> exists and has more than 1,000 words, ask: \"A substantial README already exists. Would you like to overwrite it, or append sections that are missing?\"</p>"},{"location":"chapters/04/#skip-if-complete-detection-prevents-accidentally-overwriting-good-work","title":"Skip-if-complete detection prevents accidentally overwriting good work.","text":""},{"location":"chapters/04/#step-2-content-planning","title":"Step 2: Content Planning","text":"<p>Based on the analysis from Step 1, determine which README sections are applicable:</p> Section Include If Badges Public repo with CI/CD detected Installation Package or binary detected Usage Examples Code examples or CLI args detected API Reference Library or SDK project detected Configuration Config files detected Contributing Open-source intent detected License License file exists <p>Plan the section order before writing. Record the plan as a comment in the log.</p>"},{"location":"chapters/04/#step-3-draft-readme","title":"Step 3: Draft README","text":""},{"location":"chapters/04/#generation-step-uses-the-plan-from-step-2","title":"Generation step. Uses the plan from Step 2.","text":"<p>Generate <code>README.md</code> using the section plan from Step 2. Follow these standards:</p> <ul> <li>Use H1 for the project name (single occurrence, at the top)</li> <li>Use H2 for all main sections</li> <li>Use H3 for sub-sections within sections</li> <li>Code blocks must specify their language (e.g., <code>bash,</code>python)</li> <li>Every code example must be executable \u2014 no placeholder pseudocode</li> </ul> <p>User Dialog Trigger: If the skill cannot determine the project's primary purpose from the file scan, ask: \"I could not determine what this project does. Can you describe it in 1-2 sentences?\"</p>"},{"location":"chapters/04/#dialog-triggers-pause-execution-and-collect-information-that-cannot-be-inferred","title":"Dialog triggers pause execution and collect information that cannot be inferred.","text":""},{"location":"chapters/04/#step-4-quality-scoring","title":"Step 4: Quality Scoring","text":"<p>Score the draft README against the 100-point rubric (see Quality Scoring section below).</p> <p>If score &gt;= 80: Proceed to Step 5. If score 65-79: Proceed with a note to the user about improvement areas. If score &lt; 65: Ask the user: \"The draft scores [N]/100. Key issues: [list].   Would you like me to revise before saving, or save and address issues later?\"</p>"},{"location":"chapters/04/#step-5-write-output","title":"Step 5: Write Output","text":"<p>Write the approved README to <code>README_PATH</code>.</p> <p>Append a session log to <code>LOG_PATH</code>: <pre><code>Session: README Generator v1.2\nDate: [timestamp]\nRepository: [REPO_ROOT]\nQuality Score: [N]/100\nSections Generated: [list]\nWords: [count]\n</code></pre></p> <p>Report to the user: \"README.md created. [N] words, quality score [N]/100. Sections: [list].\"</p>"},{"location":"chapters/04/#output-files-summary_1","title":"Output Files Summary","text":"<p>Required: 1. <code>README.md</code> \u2014 Complete project documentation in the repository root</p> <p>Recommended: 2. <code>logs/readme-generator-[timestamp].md</code> \u2014 Session log with quality score and decisions</p> <p>Optional: 3. Updates to <code>mkdocs.yml</code> if the project uses MkDocs and README should link to docs</p>"},{"location":"chapters/04/#example-session_1","title":"Example Session","text":"<p>User: \"Generate a README for this project\"</p> <p>Claude (using this skill): 1. Detects repository root at <code>/Users/dan/projects/my-api/</code> 2. Scans structure \u2014 detects Python, <code>pyproject.toml</code>, FastAPI dependency 3. Plans sections: badges, installation, usage, API reference, contributing, license 4. Drafts README with FastAPI-specific examples 5. Scores draft: 84/100 (strong structure, good examples, missing badges) 6. Writes README.md 7. Reports: \"README.md created. 847 words, quality score 84/100. Missing: CI badge    (no CI config detected). Recommend adding GitHub Actions workflow.\"</p>"},{"location":"chapters/04/#common-pitfalls-to-avoid_1","title":"Common Pitfalls to Avoid","text":"<p>Inventing functionality: Only document what actually exists in the repository. Do not write \"coming soon\" sections or describe features that are not yet implemented.</p> <p>Generic installation instructions: Bad: \"Install the dependencies.\" Good: \"Run <code>pip install -e .</code> to install the package and all dependencies in development mode.\"</p> <p>Placeholder code examples: Bad: <code>my_function(your_data)</code> Good: <code>result = transform_csv(\"input.csv\", delimiter=\",\")</code></p>"},{"location":"chapters/04/#quality-scoring-reference","title":"Quality Scoring Reference","text":"Category Weight Criteria Completeness 30 pts All relevant sections present Accuracy 30 pts All claims verifiable from codebase Usability 20 pts New user can install and run from README alone Code Quality 20 pts All examples syntactically correct and executable <p>Thresholds: - 80-100: Excellent \u2014 deliver immediately - 65-79: Good \u2014 deliver with improvement notes - Below 65: Needs revision \u2014 ask user before saving <pre><code>---\n\n## Why These Sections Belong in Production Skills\n\nThe agentskills.io spec imposes no body format restrictions \u2014 these sections are conventions that produce reliable, high-quality skills. The spec's exact language: \"There are no format restrictions. Write whatever helps agents perform the task effectively.\"\n\nThat said, each section exists because it prevents a specific class of failure:\n\n| Section | What It Prevents |\n|---------|-----------------|\n| Frontmatter | Skill cannot be loaded or invoked without it \u2014 this is a spec requirement |\n| H1 Title | Without it, Claude has no named reference point; logs become ambiguous |\n| Overview | Without context, ambiguous step instructions are interpreted inconsistently |\n| When to Use | Without it, users invoke at wrong time in workflow; dependencies not met |\n| Workflow Steps | Without instructions, the skill produces arbitrary output \u2014 also a spec requirement |\n| Step 0 | Without it, path variables are undefined; filesystem operations fail |\n| Output Files Summary | Without a contract on deliverables, downstream skills break unpredictably |\n| Example Session | Without a reference, expected interaction patterns drift across runs |\n| Common Pitfalls | Without explicit anti-patterns, known failure modes repeat |\n| Quality Scoring | Without self-validation, output quality is unpredictable |\n\nThe sections are not decorative. The spec gives you total freedom over the body \u2014 these conventions are how experienced skill authors use that freedom to produce consistent, reproducible behavior.\n\n---\n\n## The Directory Structure for a Skill\n\nA SKILL.md file lives inside a directory that may contain supporting files. Skills can be installed in two locations:\n\n- `~/.claude/skills/` \u2014 user-level installation, available across all projects on the machine\n- `.github/skills/` \u2014 repository-level installation, committed to the project and portable across teams\n</code></pre> ~/.claude/skills/          # or .github/skills/ for repo-level installs \u2514\u2500\u2500 my-skill/     \u251c\u2500\u2500 SKILL.md              \u2190 Required. The skill definition.     \u251c\u2500\u2500 assets/     \u2502   \u2514\u2500\u2500 templates/        \u2190 Optional. Templates referenced in workflow steps.     \u251c\u2500\u2500 references/           \u2190 Optional. Sub-skill guides for meta-skills.     \u2502   \u251c\u2500\u2500 variant-a.md     \u2502   \u2514\u2500\u2500 variant-b.md     \u2514\u2500\u2500 scripts/              \u2190 Optional. Python or bash utilities.         \u2514\u2500\u2500 validate.py ```</p> <p>The SKILL.md file is the only required file. Supporting assets extend the skill's capabilities without bloating the SKILL.md file itself. A skill that generates Python validation scripts, for example, can store those scripts in <code>scripts/</code> and reference them in the workflow steps.</p>"},{"location":"chapters/04/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Every SKILL.md contains two logical sections: YAML frontmatter and a markdown body</li> <li>The frontmatter is Claude Code's configuration layer \u2014 name, description, license, allowed-tools</li> <li>The markdown body is Claude's instruction set \u2014 every section shapes how it interprets and executes the workflow</li> <li>Step 0 is a McCreary methodology convention for environment setup \u2014 widely used but not an agentskills.io spec requirement</li> <li>The Output Files Summary creates a binding contract on what the skill produces</li> <li>Quality scoring sections transform the skill from a generator into a self-validating system</li> <li>The agentskills.io spec imposes no body format restrictions \u2014 the 10-section convention is best practice, not a requirement</li> <li>Skills install to <code>~/.claude/skills/</code> (user-level) or <code>.github/skills/</code> (repo-level, portable across teams)</li> </ul> <p>In Chapter 5, we examine the YAML frontmatter in depth, including how the <code>description</code> field becomes part of Claude's system prompt and how <code>allowed-tools</code> glob patterns control execution permissions.</p>"},{"location":"chapters/05/","title":"Chapter 5: YAML Frontmatter","text":""},{"location":"chapters/05/#the-four-fields-that-define-every-skill","title":"The Four Fields That Define Every Skill","text":"<p>The YAML frontmatter is the first thing Claude Code reads when it loads a skill. Before executing a single workflow step, before reading the overview, before processing any instruction in the markdown body \u2014 Claude reads the four fields in the frontmatter block. Those fields determine whether the skill loads at all, what name it registers under, what permissions it has, and how Claude will describe it when asked about available skills.</p> <p>This chapter covers each field in depth: what it does mechanically, how it influences Claude's behavior, and the failure modes that result from common mistakes.</p>"},{"location":"chapters/05/#the-frontmatter-block-structure","title":"The Frontmatter Block Structure","text":"<p>A frontmatter block is valid YAML enclosed between two triple-dash lines at the very start of the file:</p> <pre><code>---\nname: skill-name-in-kebab-case\ndescription: Single paragraph describing what the skill does and WHEN to use it.\nlicense: MIT\nallowed-tools: Bash(some-path:*)\n---\n</code></pre> <p>Strict requirements:</p> <ul> <li>The opening <code>---</code> must be the very first line of the file \u2014 no blank lines, no comments, no byte-order marks before it</li> <li>The closing <code>---</code> must appear on its own line immediately after the last field</li> <li>Field names are case-sensitive (<code>name</code>, not <code>Name</code> or <code>NAME</code>)</li> <li>Values containing colons, special characters, or line breaks must be quoted or use YAML block scalar syntax</li> </ul> <p>If the frontmatter is malformed, Claude Code will either fail to load the skill entirely or load it with corrupted metadata. There is no graceful degradation \u2014 invalid YAML is a hard failure.</p>"},{"location":"chapters/05/#field-1-name","title":"Field 1: <code>name</code>","text":""},{"location":"chapters/05/#what-it-does","title":"What It Does","text":"<p>The <code>name</code> field is the machine identifier for your skill. It is the string users type when invoking the skill, the string that appears in skill registry listings, and the string that must match the directory name containing the SKILL.md file.</p> <pre><code>name: glossary-generator\n</code></pre>"},{"location":"chapters/05/#format-requirements","title":"Format Requirements","text":"<p>The <code>name</code> field must be in kebab-case: all lowercase, words separated by hyphens, no spaces, no underscores, no special characters.</p> Value Valid? Reason <code>glossary-generator</code> Yes Correct kebab-case <code>glossaryGenerator</code> No camelCase not allowed <code>glossary_generator</code> No Underscores not allowed <code>Glossary Generator</code> No Spaces not allowed <code>GLOSSARY-GENERATOR</code> No Must be lowercase <code>glossary-generator-v2</code> Yes Version suffix acceptable"},{"location":"chapters/05/#the-name-directory-relationship","title":"The Name-Directory Relationship","text":"<p>The <code>name</code> field must match the directory name that contains the SKILL.md file. If the directory is <code>~/.claude/skills/readme-generator/</code>, the <code>name</code> field must be <code>readme-generator</code>. A mismatch between directory name and <code>name</code> field creates ambiguity in skill resolution and should be treated as a configuration error.</p> <pre><code>~/.claude/skills/\n\u251c\u2500\u2500 readme-generator/        \u2190 Directory name\n\u2502   \u2514\u2500\u2500 SKILL.md             \u2190 name: readme-generator (must match)\n\u251c\u2500\u2500 glossary-generator/\n\u2502   \u2514\u2500\u2500 SKILL.md             \u2190 name: glossary-generator (must match)\n</code></pre>"},{"location":"chapters/05/#choosing-a-good-name","title":"Choosing a Good Name","text":"<p>A well-chosen skill name communicates what the skill does in two to four words. The noun-verb or verb-noun pattern works best.</p> <pre><code>verb-noun pattern:    generate-glossary, analyze-course, validate-graph\nnoun-verb pattern:    glossary-generator, course-analyzer, graph-validator\n</code></pre> <p>The noun-verb pattern (ending in <code>-er</code> or <code>-or</code>) is more common in the McCreary ecosystem and reads more naturally as a tool name: <code>glossary-generator</code>, <code>readme-generator</code>, <code>learning-graph-generator</code>.</p> <p>Avoid names that are too generic (<code>content-creator</code>, <code>file-processor</code>) or that describe the technology rather than the function (<code>markdown-writer</code>, <code>json-builder</code>). The name should answer \"what does it produce?\" not \"how does it work?\"</p>"},{"location":"chapters/05/#field-2-description","title":"Field 2: <code>description</code>","text":""},{"location":"chapters/05/#what-it-does-the-system-prompt-connection","title":"What It Does \u2014 The System Prompt Connection","text":"<p>The <code>description</code> field is more than metadata. When Claude selects or is instructed to use a skill, the description field is injected into Claude's context as part of its operational instructions. It is not merely a label \u2014 it is Claude's primary reference for understanding what it is supposed to do and when.</p> <p>This has a critical implication: your description is a prompt. Every word you write in the description field is part of the instructions Claude operates under while executing that skill. Vague descriptions produce vague behavior. Precise descriptions produce precise behavior.</p>"},{"location":"chapters/05/#the-two-sentence-structure","title":"The Two-Sentence Structure","text":"<p>The most effective descriptions follow a two-sentence structure:</p> <p>Sentence 1: What the skill does (the transformation it performs) Sentence 2: When to use it (the trigger condition)</p> <pre><code>description: This skill automatically generates a comprehensive glossary of terms\n  from a learning graph's concept list, ensuring each definition follows ISO 11179\n  metadata registry standards (precise, concise, distinct, non-circular, and free\n  of business rules). Use this skill when creating a glossary for an intelligent\n  textbook after the learning graph concept list has been finalized.\n</code></pre> <p>Let's break down why this works:</p> <ul> <li>\"automatically generates a comprehensive glossary\" \u2014 tells Claude the scope and nature of the output</li> <li>\"from a learning graph's concept list\" \u2014 specifies the required input</li> <li>\"ensuring each definition follows ISO 11179 standards\" \u2014 embeds the quality standard directly</li> <li>\"Use this skill when creating a glossary for an intelligent textbook after the learning graph concept list has been finalized\" \u2014 the trigger condition is specific and checkable</li> </ul>"},{"location":"chapters/05/#descriptions-as-skill-selection-signals","title":"Descriptions as Skill-Selection Signals","text":"<p>When a user asks \"which skill should I use to validate my course description?\", Claude reads the descriptions of available skills and performs a semantic matching operation. The description with the closest match to the user's stated need wins.</p> <p>This means your description must contain the vocabulary a user would naturally use when describing their problem. If users will say \"I need to check my course description,\" your description should include phrases like \"analyze,\" \"validate,\" \"course description,\" and \"quality check.\"</p> <pre><code># Good: contains user-natural vocabulary\ndescription: This skill analyzes or creates course descriptions for intelligent\n  textbooks by checking for completeness of required elements (title, audience,\n  prerequisites, topics, Bloom's Taxonomy outcomes) and providing quality scores\n  with improvement suggestions. Use this skill when working with course descriptions\n  in /docs/course-description.md that need validation or creation for learning\n  graph generation.\n\n# Bad: describes mechanism instead of user need\ndescription: This skill reads markdown files containing course description data\n  and parses their contents to produce a structured JSON report. Use when you\n  have a markdown file you want analyzed.\n</code></pre> <p>The \"bad\" example fails because it describes what the skill does technically rather than what user problem it solves. A user thinking \"I need to check if my course description is complete\" will not search for \"read markdown files.\"</p>"},{"location":"chapters/05/#length-constraints","title":"Length Constraints","text":"<p>The description should be one paragraph \u2014 typically 50 to 100 words. There is no hard maximum, but descriptions beyond 150 words begin to lose effectiveness for two reasons:</p> <ol> <li>Context window cost: Long descriptions consume context space that could be used for executing workflow steps.</li> <li>Trigger precision: Longer descriptions include more vocabulary, which can cause accidental matching against unrelated user requests.</li> </ol> <p>If you find yourself writing a description longer than 150 words, it is usually a sign that either your skill is doing too many things (consider splitting it) or you are including information that belongs in the Overview section of the markdown body.</p>"},{"location":"chapters/05/#the-use-this-skill-when-clause","title":"The \"Use this skill when...\" Clause","text":"<p>Every description should end with a \"Use this skill when...\" clause. This clause is the trigger condition \u2014 the specific circumstances under which this skill is the correct choice.</p> <pre><code># Too vague \u2014 matches everything\nUse this skill when you need to generate content.\n\n# Too narrow \u2014 misses valid use cases\nUse this skill when you have exactly 200 concepts in a file named\n02-concept-list-v1.md and your course is on computer science.\n\n# Well-calibrated \u2014 specific but not over-specified\nUse this skill when creating a glossary for an intelligent textbook\nafter the learning graph concept list has been finalized.\n</code></pre> <p>The trigger condition should be specific enough to distinguish this skill from similar skills, but not so narrow that valid use cases are excluded.</p>"},{"location":"chapters/05/#common-description-mistakes","title":"Common Description Mistakes","text":"<p>Mistake 1: Describing implementation instead of function</p> <pre><code># Wrong: tells Claude how the skill works, not what it does for the user\ndescription: This skill uses a multi-step process to iterate through concept labels,\n  apply ISO 11179 rules programmatically, and output formatted markdown entries.\n\n# Right: tells Claude what problem it solves\ndescription: This skill generates ISO 11179-compliant glossary definitions for all\n  concepts in a learning graph. Use this skill when your textbook needs a glossary\n  after the concept list is finalized.\n</code></pre> <p>Mistake 2: No trigger condition</p> <pre><code># Wrong: no \"when to use\" signal \u2014 Claude cannot distinguish use cases\ndescription: This skill generates glossary definitions for concept lists.\n\n# Right: trigger condition makes selection unambiguous\ndescription: This skill generates glossary definitions for concept lists. Use\n  this skill after the learning graph concept list has been reviewed and approved.\n</code></pre> <p>Mistake 3: Vague quality claims</p> <pre><code># Wrong: \"high-quality\" is meaningless without criteria\ndescription: This skill generates high-quality glossary definitions.\n\n# Right: specific quality standard named\ndescription: This skill generates ISO 11179-compliant glossary definitions\n  (precise, concise, distinct, non-circular, free of business rules).\n</code></pre> <p>Mistake 4: First-person reference</p> <pre><code># Wrong: skill descriptions should not use first person\ndescription: I will analyze your course description and tell you what's missing.\n\n# Right: objective, third-person description\ndescription: This skill analyzes course descriptions and reports on missing or\n  incomplete elements required for learning graph generation.\n</code></pre>"},{"location":"chapters/05/#field-3-license","title":"Field 3: <code>license</code>","text":""},{"location":"chapters/05/#what-it-does_1","title":"What It Does","text":"<p>The <code>license</code> field declares the open-source license under which the skill is distributed. For skills in the McCreary ecosystem and most custom skill development, the standard value is <code>MIT</code>.</p> <pre><code>license: MIT\n</code></pre>"},{"location":"chapters/05/#why-it-exists","title":"Why It Exists","text":"<p>The <code>license</code> field enables skill sharing and distribution. When a skill is published to a shared registry, other users need to know whether they can:</p> <ul> <li>Use the skill for commercial projects</li> <li>Modify the skill and redistribute it</li> <li>Include it in products without attribution requirements</li> </ul> <p>The MIT license is the most permissive common open-source license and is appropriate for most public skills. It allows unrestricted use, modification, and distribution with attribution.</p>"},{"location":"chapters/05/#license-options","title":"License Options","text":"License Use Case <code>MIT</code> Maximum permissibility, standard for public skills <code>Apache-2.0</code> Enterprise-safe with patent protections <code>GPL-3.0</code> Requires derivative works to be open-sourced <code>Proprietary</code> Skills that should not be redistributed <code>CC-BY-4.0</code> Creative Commons for documentation-heavy skills <p>For skills you intend to keep private (personal workflow automations, proprietary business logic), using <code>Proprietary</code> signals clearly that the skill should not be shared.</p>"},{"location":"chapters/05/#license-field-in-practice","title":"License Field in Practice","text":"<p>For most skill authors reading this guide, the license field will always be <code>MIT</code>. The important thing is to include it \u2014 a missing license field forces anyone who wants to share or build on your skill to make assumptions about redistribution rights.</p>"},{"location":"chapters/05/#field-4-allowed-tools","title":"Field 4: <code>allowed-tools</code>","text":""},{"location":"chapters/05/#what-it-does_2","title":"What It Does","text":"<p>The <code>allowed-tools</code> field restricts which tools Claude can use while executing the skill. It is a security and consistency mechanism: it prevents Claude from taking actions outside the defined scope of the skill, and it creates predictable, auditable behavior.</p> <pre><code>allowed-tools: Bash(~/.claude/skills/glossary-generator:*)\n</code></pre>"},{"location":"chapters/05/#why-tool-restriction-matters","title":"Why Tool Restriction Matters","text":"<p>Without <code>allowed-tools</code>, a skill running in Claude Code has access to every tool Claude can use: file reads, file writes, bash commands in any directory, network requests, and more. This is appropriate for open-ended conversations but dangerous for structured skills.</p> <p>Consider a skill designed to analyze a course description. Without tool restrictions, Claude could hypothetically: - Write to unintended directories - Execute shell commands outside the project scope - Make network requests to external services</p> <p>The <code>allowed-tools</code> field eliminates these possibilities by explicitly declaring what Claude is permitted to do.</p>"},{"location":"chapters/05/#glob-pattern-syntax","title":"Glob Pattern Syntax","text":"<p>The <code>allowed-tools</code> field uses glob patterns to specify which tools are allowed and what scopes are permitted.</p> <p>Basic syntax:</p> <pre><code>ToolName(path-pattern:operation)\n</code></pre> <p>Common patterns:</p> <pre><code># Allow bash commands only within the skill's own directory\nallowed-tools: Bash(~/.claude/skills/my-skill:*)\n\n# Allow bash commands in the skill directory AND a temp directory\nallowed-tools: Bash(~/.claude/skills/my-skill:*), Bash(/tmp:*)\n\n# Allow bash commands anywhere in the project docs folder\nallowed-tools: Bash(~/projects/my-project/docs:*)\n\n# Allow file reads anywhere (no path restriction on Read tool)\nallowed-tools: Read(*), Bash(~/.claude/skills/my-skill:*)\n\n# Allow all tools (equivalent to no allowed-tools field - use with caution)\nallowed-tools: \"*\"\n</code></pre> <p>Multiple tools:</p> <p>Multiple tool permissions are separated by commas:</p> <pre><code>allowed-tools: Bash(~/.claude/skills/my-skill:*), Bash(/tmp:*), Read(~/projects:*)\n</code></pre>"},{"location":"chapters/05/#designing-your-tool-permission-set","title":"Designing Your Tool Permission Set","text":"<p>When designing <code>allowed-tools</code> for a new skill, ask four questions:</p> <ol> <li>Does this skill need to run shell commands? If yes, which directories need bash access?</li> <li>Does this skill read files outside the project? If yes, which paths?</li> <li>Does this skill write files? If yes, which directories?</li> <li>Does this skill need network access? (Rare for most skills \u2014 most skills read and write local files)</li> </ol> <p>A well-designed skill has the minimum permissions necessary to accomplish its workflow. If a skill only needs to read files in <code>docs/</code> and write to <code>docs/</code>, there is no reason to allow bash access to the entire filesystem.</p>"},{"location":"chapters/05/#common-allowed-tools-patterns-by-skill-type","title":"Common allowed-tools Patterns by Skill Type","text":"<p>Documentation generator (reads project, writes to docs/): <pre><code>allowed-tools: Bash(~/projects:*), Read(~/projects:*)\n</code></pre></p> <p>Validator (reads only, no writes): <pre><code>allowed-tools: Read(*)\n</code></pre></p> <p>MkDocs skill (needs to run mkdocs CLI): <pre><code>allowed-tools: Bash(~/projects:*), Bash(/usr/local/bin:*)\n</code></pre></p> <p>Skill that runs Python scripts from the skill package: <pre><code>allowed-tools: Bash(~/.claude/skills/learning-graph-generator:*), Bash(~/projects:*)\n</code></pre></p>"},{"location":"chapters/05/#what-happens-without-allowed-tools","title":"What Happens Without allowed-tools","text":"<p>If the <code>allowed-tools</code> field is omitted, Claude operates without tool restrictions. This is appropriate during skill development and prototyping, but is not recommended for production skills that will be shared or used in automated workflows.</p> <p>The risk is not necessarily malicious behavior \u2014 it is unpredictable behavior. A skill without tool restrictions may work correctly in your environment and fail unexpectedly in someone else's environment because Claude makes different tool choices when not constrained.</p> <p>```admonition note Development vs. Production Practice</p> <p>During development, it is reasonable to omit <code>allowed-tools</code> to avoid debugging permission issues while iterating on the workflow. Once the skill is working correctly, add explicit <code>allowed-tools</code> restrictions before sharing or deploying the skill. <pre><code>---\n\n## The Frontmatter as a Whole\n\nUnderstanding each field individually is useful, but the real insight is how they work together as a system.\n\n```yaml\n---\nname: course-description-analyzer     # 1. Registration key\ndescription: This skill analyzes or creates course descriptions for intelligent\n  textbooks by checking for completeness of required elements and providing\n  quality scores with improvement suggestions. Use this skill when working with\n  course descriptions in /docs/course-description.md that need validation or\n  creation for learning graph generation.   # 2. Context injection + selection signal\nlicense: MIT                           # 3. Distribution rights\nallowed-tools: Bash(~/.claude/skills/course-description-analyzer:*)   # 4. Permission boundary\n---\n</code></pre></p> <p>When a user invokes this skill:</p> <ol> <li>Claude Code finds the skill directory <code>course-description-analyzer/</code> in <code>~/.claude/skills/</code></li> <li>It reads the <code>name</code> field to confirm the skill identity matches the invocation</li> <li>It reads the <code>description</code> and injects it into Claude's context</li> <li>It applies the <code>allowed-tools</code> restrictions for the duration of the skill session</li> <li>It records the <code>license</code> field in any distributed artifacts</li> </ol> <p>This four-step sequence happens before any workflow step runs. The frontmatter is not documentation \u2014 it is the initialization phase of every skill execution.</p>"},{"location":"chapters/05/#frontmatter-validation-checklist","title":"Frontmatter Validation Checklist","text":"<p>Before publishing or sharing a skill, verify all four fields against this checklist:</p> Field Check Pass Condition <code>name</code> Is it kebab-case? All lowercase, hyphens only, no spaces <code>name</code> Does it match the directory name? Exact string match <code>name</code> Is it descriptive? Communicates function in 2-4 words <code>description</code> Is it one paragraph? 50-100 words recommended <code>description</code> Does it contain a \"Use this skill when...\" clause? Explicit trigger condition present <code>description</code> Does it name the output? Clear statement of what is produced <code>description</code> Does it use user-natural vocabulary? Matches how users would describe their need <code>license</code> Is it present? Field exists with a valid license identifier <code>allowed-tools</code> Are permissions minimal? No broader than required by the workflow <code>allowed-tools</code> Are all required tools included? Skill can execute all workflow steps"},{"location":"chapters/05/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>The frontmatter is not decoration \u2014 it is the initialization layer that runs before any workflow step</li> <li>The <code>description</code> field functions as a prompt that shapes Claude's behavior throughout the skill session; treat it as instructions, not a label</li> <li>The two-sentence description structure (what it does + when to use it) is the most effective format</li> <li>The <code>name</code> field must match the directory name exactly and must be in kebab-case</li> <li><code>allowed-tools</code> creates predictable, auditable behavior by restricting Claude to the permissions the skill actually needs</li> <li>Omitting any field is never safe \u2014 each field either enables core functionality or prevents a class of errors</li> </ul> <p>In Chapter 6, we move into the workflow body \u2014 how to structure multi-step workflows, the Step 0 environment setup pattern, user dialog triggers, and conditional branching.</p>"},{"location":"chapters/06/","title":"Chapter 6: Workflow Design","text":""},{"location":"chapters/06/#building-multi-step-workflows-that-execute-reliably","title":"Building Multi-Step Workflows That Execute Reliably","text":"<p>The workflow is the engine of a skill. Everything else in the SKILL.md file \u2014 frontmatter, overview, output summary \u2014 exists to support it. A well-designed workflow executes identically whether Claude runs it the first time or the hundredth time, whether the project is large or small, whether the user is experienced or a first-timer.</p> <p>This chapter covers the full architecture of skill workflows: the Step 0 environment setup pattern, sequential vs. parallel execution, user dialog triggers, conditional branching, error handling, and the quality gate pattern. Three worked examples of increasing complexity close the chapter.</p>"},{"location":"chapters/06/#the-anatomy-of-a-workflow-step","title":"The Anatomy of a Workflow Step","text":"<p>Every step in a skill workflow follows the same structural pattern:</p> <pre><code>### Step N: Descriptive Step Name\n\n[One sentence explaining the purpose of this step]\n\n[Detailed instructions for what Claude should do]\n\n[Optional: User Dialog Triggers]\n[Optional: Skip-if-complete detection]\n[Optional: Decision table or threshold logic]\n</code></pre> <p>The step heading (<code>### Step N:</code>) is not just a label \u2014 it is a semantic anchor. When Claude is mid-execution and needs to reference another step, or when a user asks \"what step are you on?\", the step heading provides a clear, unambiguous reference point. Use consistent step numbering starting from Step 0.</p>"},{"location":"chapters/06/#what-makes-a-good-step","title":"What Makes a Good Step","text":"<p>A well-designed step has three properties:</p> <ol> <li>Single cohesive purpose \u2014 one step, one goal. \"Validate input and generate content and write output\" is three steps masquerading as one.</li> <li>Clear completion criteria \u2014 Claude should know unambiguously when the step is done.</li> <li>Explicit error handling \u2014 what should happen if the step fails or the inputs are not as expected?</li> </ol> <p>A poorly designed step is either too small (\"Read the file\") or too large (\"Do everything\"). The right granularity is a step that corresponds to a meaningful checkpoint \u2014 a point where you would naturally want to verify progress before continuing.</p>"},{"location":"chapters/06/#step-0-the-environment-setup-pattern","title":"Step 0: The Environment Setup Pattern","text":"<p>Step 0 is the most important step in any skill workflow. It is always the first step, always named \"Setup\" or \"Environment Setup,\" and always performs the same core function: establishing the filesystem context that all subsequent steps depend on.</p> <p>Without Step 0, every subsequent step that references a file path is a potential failure. With Step 0, paths are established once, verified once, and all subsequent steps operate with confidence.</p>"},{"location":"chapters/06/#the-three-jobs-of-step-0","title":"The Three Jobs of Step 0","text":"<p>Job 1: Announce the skill and version</p> <pre><code>### Step 0: Environment Setup\n\nTell the user you are running [Skill Name] version [X.Y].\n</code></pre> <p>This announcement serves as an explicit confirmation that the correct skill loaded. If the wrong skill loaded, the user can stop execution immediately rather than after several steps of incorrect work.</p> <p>Job 2: Detect the project root</p> <pre><code>Detect the project root by looking for `mkdocs.yml` in the current directory.\nIf not found, check parent directories up to three levels up.\n\nIf `mkdocs.yml` is still not found, halt and ask:\n\"I cannot detect an MkDocs project. Are you running this skill from the project's\nroot directory? The project root should contain mkdocs.yml.\"\n</code></pre> <p>The project root detection check is important because skills run from wherever the user starts Claude Code \u2014 which may not be the project root. A skill that assumes it is already in the project root will silently write files to the wrong location.</p> <p>Job 3: Establish path variables</p> <p><pre><code>Set the following path variables for use throughout this skill:\n\n- PROJECT_ROOT: directory containing mkdocs.yml\n- DOCS_DIR: PROJECT_ROOT/docs\n- LEARNING_GRAPH_DIR: DOCS_DIR/learning-graph\n- LOG_DIR: PROJECT_ROOT/logs\n\nCreate directories that do not exist:\n```bash\nmkdir -p $DOCS_DIR/learning-graph\nmkdir -p $LOG_DIR\n</code></pre> <pre><code>Setting path variables in Step 0 gives every subsequent step a consistent reference. Step 3 that says \"write to `$DOCS_DIR/glossary.md`\" will always write to the same place regardless of where the user invoked Claude Code.\n\n### A Complete Step 0 Template\n\n```markdown\n### Step 0: Environment Setup\n\nTell the user you are running [Skill Name] version [X.Y].\n\nDetect the project root by locating `mkdocs.yml` in the current directory or parent\ndirectories. If not found, halt and ask the user to navigate to the project root before\ncontinuing.\n\nSet path variables:\n- PROJECT_ROOT: directory containing mkdocs.yml\n- DOCS_DIR: PROJECT_ROOT/docs\n- LOG_DIR: PROJECT_ROOT/logs\n\nCreate required directories if they do not exist:\n```bash\nmkdir -p $LOG_DIR\n</code></pre></p> <p>Open the session log at <code>$LOG_DIR/[skill-name]-[timestamp].md</code> and record: - Skill name and version - Execution start timestamp - PROJECT_ROOT path - User's current working directory <pre><code>---\n\n## Sequential Execution\n\nThe default execution model for skill workflows is sequential: Step 0 runs, then Step 1, then Step 2, and so on. Each step completes before the next begins.\n\nSequential execution is appropriate when:\n- Each step depends on the output of the previous step\n- The order of operations matters for correctness\n- You want clear checkpoints between operations\n\n```mermaid\ngraph LR\n    S0[Step 0: Setup] --&gt; S1[Step 1: Read Input]\n    S1 --&gt; S2[Step 2: Validate]\n    S2 --&gt; S3[Step 3: Generate]\n    S3 --&gt; S4[Step 4: Write Output]\n    S4 --&gt; S5[Step 5: Report]\n</code></pre></p> <p>Most skill workflows are sequential. The dependencies between steps \u2014 Step 3 needs the validated input from Step 2, Step 4 needs the generated content from Step 3 \u2014 enforce a natural ordering that sequential execution respects.</p>"},{"location":"chapters/06/#parallel-execution","title":"Parallel Execution","text":"<p>Some steps can run simultaneously because they are independent of each other. You can instruct Claude to execute these steps in parallel:</p> <pre><code>### Step 2: Gather Context (Parallel)\n\nExecute the following reads simultaneously:\n\n- Read `docs/course-description.md` (course context for audience and vocabulary)\n- Read `docs/learning-graph/02-concept-list-v1.md` (concept list to define)\n- Read `mkdocs.yml` (navigation structure for output placement)\n\nProceed to Step 3 once all three files are read.\n</code></pre> <p>Parallel execution is appropriate when: - Steps access different files with no overlapping writes - Steps are read-only operations - Time is a concern and steps are independent</p> <p>Use the word \"simultaneously\" or \"in parallel\" explicitly to signal to Claude that these steps can overlap. Do not use parallel execution for steps that write to the same file or that depend on each other's output.</p>"},{"location":"chapters/06/#caution-token-cost-of-parallelism","title":"Caution: Token Cost of Parallelism","text":"<p>Reading multiple large files in parallel consumes context window space faster than reading them sequentially. For skills that run against large projects, consider whether parallel reads are necessary or whether sequential reads with explicit scope limits are more efficient.</p>"},{"location":"chapters/06/#user-dialog-triggers","title":"User Dialog Triggers","text":"<p>User dialog triggers are conditional pause points \u2014 moments where the skill stops, presents information to the user, and waits for input before continuing.</p> <p>The golden rule for user dialog triggers: ask only when you cannot proceed without user input, or when proceeding without asking would risk significant unwanted changes.</p>"},{"location":"chapters/06/#when-to-trigger-a-dialog","title":"When to Trigger a Dialog","text":"<pre><code>**User Dialog Triggers:**\n\n- If quality score &lt; 70: Ask \"The input quality score is [N]/100. Issues found:\n  [list]. Would you like to fix these issues before proceeding, or continue anyway?\"\n\n- If output file already exists: Ask \"A glossary already exists at docs/glossary.md\n  ([N] words). Overwrite, append missing terms, or skip?\"\n\n- If required input file is missing: Ask \"I cannot find docs/learning-graph/\n  02-concept-list-v1.md. Would you like me to look for a concept list with a\n  different filename, or should we create one first?\"\n</code></pre>"},{"location":"chapters/06/#three-categories-of-dialog-triggers","title":"Three Categories of Dialog Triggers","text":"<p>Category 1: Quality gate triggers</p> <p>These trigger when input quality falls below a threshold. The user is informed of the specific issue and given a choice about how to proceed.</p> <pre><code>If the concept list quality score is below 70/100, halt and report:\n\"Quality score: [N]/100. Issues found:\n- [N] duplicate concepts\n- [N] concepts exceed 32 characters\n- [N] concepts not in Title Case\n\nOptions:\n1. Fix issues automatically and continue\n2. Show me the problem concepts (I will fix manually)\n3. Continue anyway (glossary may have quality issues)\"\n\nWait for user selection before proceeding.\n</code></pre> <p>Category 2: Destructive action triggers</p> <p>These trigger before any operation that could overwrite or delete existing work.</p> <pre><code>Before writing docs/glossary.md, check if the file exists.\n\nIf it exists and contains more than 100 lines:\nAsk: \"docs/glossary.md already exists with [N] terms. How should I proceed?\n1. Overwrite completely\n2. Update existing terms and add missing ones\n3. Skip (keep the existing glossary)\"\n</code></pre> <p>Category 3: Ambiguity resolution triggers</p> <p>These trigger when the skill cannot determine the correct action from the available information.</p> <pre><code>If the project contains multiple course description files (e.g., course-description.md\nand course-description-v2.md), ask:\n\"I found multiple course description files. Which should I use?\n- course-description.md (last modified: [date])\n- course-description-v2.md (last modified: [date])\"\n</code></pre>"},{"location":"chapters/06/#what-to-avoid-in-dialog-triggers","title":"What to Avoid in Dialog Triggers","text":"<p>Avoid asking for information you can determine yourself: <pre><code># Wrong: Claude can detect the language from file extensions\nAsk: \"What programming language does this project use?\"\n\n# Right: Check file extensions first, only ask if ambiguous\nRead the file extensions in the repository root. If .py files dominate, assume Python.\nIf unclear (mixed languages or no clear majority), ask the user.\n</code></pre></p> <p>Avoid over-checking with the user:</p> <p>A skill that asks for permission at every step is friction, not safety. Dialog triggers should be reserved for genuinely uncertain or high-stakes decisions. A skill that asks five questions before doing any work will be abandoned.</p> <p>Avoid open-ended questions:</p> <pre><code># Wrong: Too open-ended, forces user to think about implementation\nAsk: \"How do you want me to handle the glossary?\"\n\n# Right: Specific options with clear implications\nAsk: \"The glossary already exists. Options:\n1. Overwrite (replace all existing definitions)\n2. Update (keep existing, fill in missing concepts)\n3. Skip\"\n</code></pre>"},{"location":"chapters/06/#conditional-branching","title":"Conditional Branching","text":"<p>Conditional branching allows a skill to follow different execution paths based on conditions it detects. The most common patterns are:</p>"},{"location":"chapters/06/#pattern-1-file-existence-branch","title":"Pattern 1: File Existence Branch","text":"<pre><code>### Step 1: Detect Project State\n\nCheck if `docs/course-description.md` exists.\n\n**If the file does NOT exist:** Follow the Creation Workflow starting at Step 2a.\n**If the file DOES exist:** Follow the Analysis Workflow starting at Step 2b.\n</code></pre> <p>This pattern is used extensively in the course-description-analyzer skill, which can either create a course description from scratch or validate an existing one. The branch point is a clear binary check.</p>"},{"location":"chapters/06/#pattern-2-quality-score-branch","title":"Pattern 2: Quality Score Branch","text":"<pre><code>### Step 3: Quality Gate\n\nCalculate the input quality score (see Quality Scoring section).\n\n**Score 85-100:** Proceed directly to Step 4. No user dialog required.\n**Score 70-84:** Proceed to Step 4 with a note: \"Input quality is good but not\n  excellent. [Specific issue]. Continuing.\"\n**Score 50-69:** Trigger User Dialog \u2014 present issues and ask how to proceed.\n**Score &lt; 50:** Halt. Report: \"Input quality is too low to produce acceptable\n  output. Quality score: [N]/100. Critical issues: [list]. Please address these\n  issues and re-run the skill.\"\n</code></pre> <p>The quality score branch creates four tiers of response rather than a binary pass/fail. This graduated response is more useful \u2014 it does not interrupt the user unnecessarily for minor issues, but it does halt for serious problems.</p>"},{"location":"chapters/06/#pattern-3-skip-if-complete-detection","title":"Pattern 3: Skip-if-Complete Detection","text":"<p>Skip-if-complete detection checks whether a step's output already exists before executing it. This prevents redundant work when a skill is partially re-run.</p> <pre><code>### Step 5: Generate Concept List\n\nBefore generating, check for existing concept list files:\n\n**If `docs/learning-graph/02-concept-list-v1.md` exists:**\n  - Read the file and count concepts\n  - If concept count &gt;= 180: Ask \"A concept list with [N] concepts already exists.\n    Skip this step and use the existing list?\" (Default: yes)\n  - If concept count &lt; 180: Note \"Existing concept list has only [N] concepts\n    (target: 200). Regenerating.\" and proceed.\n\n**If no concept list exists:** Generate the concept list now.\n</code></pre> <p>Skip-if-complete detection is especially valuable in long-running skills (10+ steps) where earlier steps may have already completed in a previous session. Re-running the entire skill from the beginning when only the last two steps need to be redone wastes tokens and time.</p>"},{"location":"chapters/06/#pattern-4-content-dependent-branch","title":"Pattern 4: Content-Dependent Branch","text":"<pre><code>### Step 4: Determine Output Format\n\nRead the `site_url` field from `mkdocs.yml`.\n\n**If `site_url` is set:** Assume public deployment. Include badges and live-site links\n  in the README.\n**If `site_url` is not set:** Assume private/development project. Generate a minimal\n  README without deployment links.\n</code></pre>"},{"location":"chapters/06/#error-handling-patterns","title":"Error Handling Patterns","text":"<p>Errors in skill workflows fall into three categories: missing inputs, unexpected file states, and execution failures. Each requires a different handling pattern.</p>"},{"location":"chapters/06/#missing-input-handling","title":"Missing Input Handling","text":"<pre><code>### Step 1: Validate Required Inputs\n\nCheck that the following required files exist:\n\n| File | Required For |\n|------|-------------|\n| `docs/course-description.md` | Context for all definitions |\n| `docs/learning-graph/02-concept-list-v1.md` | The concepts to define |\n\n**If any required file is missing:**\nReport clearly which file is missing and why it is needed:\n\"Required file not found: docs/learning-graph/02-concept-list-v1.md\nThis file contains the concept list that the glossary generator will define.\nTo create it, run the learning-graph-generator skill first, then return to\nthis skill.\"\n\nHalt. Do not proceed to Step 2 without all required inputs.\n</code></pre> <p>The error message should: 1. State exactly what is missing 2. Explain what the missing thing is used for 3. Tell the user how to get it</p>"},{"location":"chapters/06/#unexpected-file-state-handling","title":"Unexpected File State Handling","text":"<pre><code>If `docs/learning-graph/02-concept-list-v1.md` exists but is empty (0 bytes):\nReport: \"Found docs/learning-graph/02-concept-list-v1.md but it is empty. This\nfile may have been created but not populated. Please check if the learning graph\ngenerator completed successfully.\"\nHalt.\n\nIf the file exists but contains fewer than 10 lines:\nReport: \"Found docs/learning-graph/02-concept-list-v1.md but it contains only\n[N] lines, which is too few to be a complete concept list. Minimum expected: 50 lines.\"\nTrigger User Dialog: \"Continue with partial concept list, or halt to investigate?\"\n</code></pre>"},{"location":"chapters/06/#execution-failure-handling","title":"Execution Failure Handling","text":"<p>When a bash command or file operation fails, the skill should report the failure explicitly rather than silently continuing:</p> <pre><code>Run:\n```bash\nmkdir -p docs/learning-graph\n</code></pre> <p>If this command fails (e.g., due to permission errors), report: \"Failed to create docs/learning-graph/ directory. This may be a file permission issue. Error: [error message]. Please check that you have write permissions in this project directory.\" Halt. <pre><code>### The Halt vs. Ask Decision\n\nWhen an error occurs, the skill must decide: halt completely, or pause and ask the user how to proceed?\n\nUse **halt** when:\n- A required input is completely missing and cannot be substituted\n- A write operation would overwrite something irreplaceable\n- The error state suggests the project is in an unexpected configuration\n\nUse **ask** when:\n- There is a reasonable alternative path the user might prefer\n- The issue is recoverable without starting over\n- The severity is unclear (low quality vs. no quality)\n\n---\n\n## The Quality Gate Pattern\n\nThe quality gate is a specific pattern that appears at the end of generation steps. It evaluates the generated output against a rubric and makes a proceed/stop decision.\n\n```mermaid\ngraph TD\n    A[Generate Output] --&gt; B{Score Output}\n    B --&gt; |Score &gt;= 85| C[Proceed Immediately]\n    B --&gt; |Score 70-84| D[Proceed with Notes]\n    B --&gt; |Score 50-69| E[Ask User: Revise or Proceed?]\n    B --&gt; |Score &lt; 50| F[Halt: Too Low to Proceed]\n    E --&gt; |User: Revise| A\n    E --&gt; |User: Proceed| G[Write Output with Warning]\n    C --&gt; G\n    D --&gt; G\n</code></pre></p> <p>The quality gate sits between generation and output writing. It serves two purposes:</p> <ol> <li>Self-correction: If the score is low, Claude revises before asking \u2014 it does not immediately interrupt the user with low-quality output.</li> <li>User transparency: When the score is borderline, the user is informed and given agency over whether to proceed.</li> </ol> <p>A complete quality gate implementation looks like this:</p> <pre><code>### Step 4: Quality Gate\n\nAfter generating all definitions, calculate the overall quality score using the\nrubric in the Quality Scoring section below.\n\nApply the following threshold logic:\n\n**Score 85-100 (Excellent):**\nProceed to Step 5 without interruption. Note the score in the session log.\n\n**Score 70-84 (Good):**\nProceed to Step 5. Inform the user:\n\"Output quality score: [N]/100. [N] definitions could be improved. Continuing.\"\nLog the specific definitions that scored below 80.\n\n**Score 50-69 (Needs Work):**\nBefore writing output, present the issues to the user:\n\"Quality score: [N]/100. [N] definitions have significant issues:\n- [Definition 1]: [Issue]\n- [Definition 2]: [Issue]\nOptions:\n1. Auto-fix the flagged definitions and continue\n2. Show me each flagged definition for manual review\n3. Save as-is and address issues later\"\n\n**Score &lt; 50 (Insufficient):**\nHalt. Do not write output. Report:\n\"Output quality score: [N]/100 \u2014 below the minimum threshold of 50 to produce\nuseful output. Critical issues: [list]. This typically means the input concept\nlist has fundamental problems. Please review the concept list and re-run.\"\n</code></pre>"},{"location":"chapters/06/#worked-example-1-a-simple-single-purpose-skill","title":"Worked Example 1: A Simple Single-Purpose Skill","text":"<p>Context: A skill that checks whether all required files for a chapter exist before running the chapter content generator.</p> <p>This is a simple skill \u2014 three steps, no generation, read-only \u2014 that demonstrates clean workflow structure.</p> <pre><code>## Workflow\n\n### Step 0: Environment Setup\n\nTell the user you are running Chapter Prerequisite Checker version 1.0.\n\nDetect project root by locating mkdocs.yml. If not found, halt with:\n\"Cannot detect MkDocs project root. Please run from the project directory.\"\n\nSet path variables:\n- PROJECT_ROOT: directory containing mkdocs.yml\n- DOCS_DIR: PROJECT_ROOT/docs\n\n### Step 1: Read Chapter Configuration\n\nRead mkdocs.yml and extract the nav section.\nRead the list of chapters from the nav entries under the \"Chapters\" section.\n\nIf the nav section has no \"Chapters\" entries, halt with:\n\"No chapters found in mkdocs.yml nav. Has the book-chapter-generator been run?\"\n\n### Step 2: Check Prerequisites for Each Chapter\n\nFor each chapter in the nav:\n\n1. Check that the chapter's markdown file exists in DOCS_DIR\n2. Check that `docs/learning-graph/learning-graph.json` exists\n3. Check that `docs/course-description.md` exists\n\nBuild a report table:\n\n| Chapter | Markdown File | Status |\n|---------|--------------|--------|\n| Chapter 1: [title] | docs/chapters/01.md | Ready / Missing |\n| ... | ... | ... |\n\n### Step 3: Report Results\n\nIf all chapters are ready: Report \"All [N] chapters have prerequisite files.\nReady to run chapter-content-generator.\"\n\nIf any chapters are missing files: Report:\n\"[N] of [N] chapters are missing prerequisite files:\n[table of missing files]\nRun the book-chapter-generator skill to create the missing chapter files.\"\n</code></pre> <p>This example demonstrates: Step 0 environment setup, clean error handling, a simple linear flow with no branching, and a useful report output.</p>"},{"location":"chapters/06/#worked-example-2-a-branching-content-generation-skill","title":"Worked Example 2: A Branching Content-Generation Skill","text":"<p>Context: A skill that either creates or updates a FAQ document depending on whether one already exists. Moderate complexity \u2014 branching at Step 1, quality gate at Step 4.</p> <pre><code>## Workflow\n\n### Step 0: Environment Setup\n\nTell the user you are running FAQ Generator version 2.1.\n\nDetect project root by locating mkdocs.yml. Halt if not found.\n\nSet path variables:\n- PROJECT_ROOT: directory containing mkdocs.yml\n- DOCS_DIR: PROJECT_ROOT/docs\n- FAQ_PATH: DOCS_DIR/faq.md\n- LOG_DIR: PROJECT_ROOT/logs\n\nCreate LOG_DIR if it does not exist.\n\nOpen session log at LOG_DIR/faq-generator-[timestamp].md.\n\n### Step 1: Detect Project State\n\nCheck whether FAQ_PATH exists.\n\n**If FAQ_PATH does NOT exist:**\n\u2192 Follow the Creation Workflow (Steps 2a through 4a)\n\n**If FAQ_PATH DOES exist:**\nRead FAQ_PATH and count existing questions (count lines beginning with \"##\").\n\u2192 Follow the Update Workflow (Steps 2b through 4b)\n\n### CREATION WORKFLOW\n\n### Step 2a: Read Source Material\n\nRead the following files to gather content for FAQ generation:\n- docs/course-description.md\n- docs/glossary.md (if it exists)\n- docs/learning-graph/02-concept-list-v1.md (if it exists)\n\nIf course-description.md does not exist, halt:\n\"Cannot generate FAQ without docs/course-description.md. Run the\ncourse-description-analyzer skill first.\"\n\n### Step 3a: Generate FAQ\n\nGenerate 20-30 questions based on the source material. Organize questions into categories:\n\n1. Course Overview Questions (5-7 questions)\n2. Concept and Terminology Questions (8-10 questions)\n3. Learning Path Questions (4-6 questions)\n4. Practical Application Questions (4-6 questions)\n\nFormat each question as an H2 heading followed by a paragraph answer:\n```markdown\n## What is a learning graph?\n\nA learning graph is a directed acyclic graph of concepts that represents\nthe order in which concepts should be learned to achieve mastery...\n</code></pre>"},{"location":"chapters/06/#step-4a-quality-gate-creation","title":"Step 4a: Quality Gate (Creation)","text":"<p>Score the generated FAQ:</p> <ul> <li>Coverage: Do the questions address the most likely student questions? (40 pts)</li> <li>Accuracy: Are the answers consistent with source material? (30 pts)</li> <li>Clarity: Are questions and answers clearly written? (30 pts)</li> </ul> <p>If total score &gt;= 70: Proceed to write output. If score &lt; 70: Revise the lowest-scoring section before proceeding.</p> <p>Skip to Step 5.</p>"},{"location":"chapters/06/#update-workflow","title":"UPDATE WORKFLOW","text":""},{"location":"chapters/06/#step-2b-analyze-existing-faq","title":"Step 2b: Analyze Existing FAQ","text":"<p>Read FAQ_PATH and extract all existing questions. Read docs/course-description.md and docs/glossary.md for new content.</p> <p>Identify concepts or topics covered in the course description or glossary that are NOT addressed in the existing FAQ.</p> <p>If fewer than 5 new topics found: Report \"Existing FAQ appears comprehensive. Found [N] potential additions. Continue to add them? (yes/no)\"</p>"},{"location":"chapters/06/#step-3b-generate-additions","title":"Step 3b: Generate Additions","text":"<p>For each identified gap, generate 1-2 new questions and answers. Determine the appropriate category for each new question. Prepare additions without modifying existing content yet.</p>"},{"location":"chapters/06/#step-4b-quality-gate-update","title":"Step 4b: Quality Gate (Update)","text":"<p>Review proposed additions:</p> <ul> <li>Are the new questions non-redundant with existing questions? (50 pts)</li> <li>Are the answers accurate and consistent with existing content? (50 pts)</li> </ul> <p>If score &gt;= 70: Proceed. If score &lt; 70: Remove or revise the redundant/inaccurate additions.</p>"},{"location":"chapters/06/#step-5-write-output","title":"Step 5: Write Output","text":"<p>Creation: Write the complete FAQ to FAQ_PATH. Update: Append approved additions to the appropriate sections in FAQ_PATH.</p> <p>Update mkdocs.yml if FAQ is not already in the nav: Under the appropriate section, add: <code>- FAQ: faq.md</code></p> <p>Log session results: - Questions created/added: [N] - Quality score: [N]/100 - Timestamp</p> <p>Report to user: \"FAQ [created/updated]. Added [N] questions. Quality score: [N]/100. [Any notes on low-scoring sections or skipped topics]\" <pre><code>This example demonstrates: early branching at the workflow level, two distinct sub-workflows with appropriate skip points, quality gates adapted to each sub-workflow, and a unified output step.\n\n---\n\n## Worked Example 3: A Multi-Phase Pipeline Skill with Rich Branching\n\n**Context:** A comprehensive chapter content generator that orchestrates multiple phases \u2014 planning, generation, quality scoring, revision, and output \u2014 with complex branching and user dialog throughout.\n\nThis is a full-complexity skill representative of production-grade workflow design.\n\n```markdown\n## Workflow\n\n### Step 0: Environment Setup\n\nTell the user you are running Chapter Content Generator version 3.0.\n\nDetect project root by locating mkdocs.yml. If not found, halt.\n\nSet path variables:\n- PROJECT_ROOT: directory containing mkdocs.yml\n- DOCS_DIR: PROJECT_ROOT/docs\n- CHAPTERS_DIR: DOCS_DIR/chapters\n- LEARNING_GRAPH_DIR: DOCS_DIR/learning-graph\n- LOG_DIR: PROJECT_ROOT/logs\n\nCreate LOG_DIR and CHAPTERS_DIR if they do not exist.\n\nCheck for required dependencies:\n\n| Required File | Status |\n|--------------|--------|\n| docs/course-description.md | [Check: exists / missing] |\n| docs/learning-graph/learning-graph.json | [Check: exists / missing] |\n| docs/learning-graph/02-concept-list-v1.md | [Check: exists / missing] |\n\nIf any required file is missing, halt with specific instructions on which\nskill to run to create the missing file.\n\n### Step 1: Determine Scope\n\nAsk: \"Which chapters should I generate?\n1. All chapters listed in mkdocs.yml\n2. A specific chapter (I will ask which one)\n3. Only chapters with empty or missing content files\"\n\nWait for user selection.\n\n**If option 1 selected:**\nRead mkdocs.yml and extract all chapter file paths from the nav section.\nStore as CHAPTER_LIST.\n\n**If option 2 selected:**\nList all chapters from mkdocs.yml nav with their current word counts.\nAsk user to select which chapter(s) to generate.\n\n**If option 3 selected:**\nScan CHAPTERS_DIR for .md files. Count words in each.\nBuild CHAPTER_LIST from files with fewer than 200 words.\nReport: \"Found [N] chapters with insufficient content. Generating those [N] chapters.\"\n\n### Step 2: Load Context (Parallel)\n\nLoad the following files simultaneously:\n- docs/course-description.md (audience, objectives, vocabulary level)\n- docs/learning-graph/learning-graph.json (concept dependencies)\n- docs/learning-graph/02-concept-list-v1.md (complete concept list)\n- docs/glossary.md (if exists \u2014 for terminology consistency)\n\nExtract from the learning graph:\n- Total concepts: [N]\n- Taxonomy distribution by category\n- For each chapter: which concepts are introduced, which are prerequisites\n\n### Step 3: Generate Chapter Plan\n\nFor each chapter in CHAPTER_LIST, create a content plan:\n</code></pre> Chapter [N]: [Title] - Target concepts from learning graph: [list] - Prerequisite concepts (from earlier chapters): [list] - Bloom's Taxonomy levels to address: [Remember, Understand, Apply, ...] - Recommended structure:   - Introduction (~300 words)   - Section 1: [Concept group] (~500 words)   - Section 2: [Concept group] (~500 words)   - Worked Example (~400 words)   - Practice Exercises (5-8 questions)   - Summary (~200 words) - Estimated total: [N] words <pre><code>Present the plan summary to the user:\n\"Content plan complete. [N] chapters planned, estimated [N] words total.\nProceed with generation? (yes/no/adjust plan)\"\n\nWait for user response before proceeding to Step 4.\n\n### Step 4: Generate Content (Per-Chapter Loop)\n\nFor each chapter in CHAPTER_LIST:\n\n#### 4.1: Skip-if-Complete Check\n\nRead the existing chapter file if it exists.\nIf word count &gt; 800: Ask \"Chapter [N] already has [N] words. Skip, overwrite, or append?\"\n\n#### 4.2: Generate Chapter Draft\n\nGenerate the chapter following the plan from Step 3.\n\nRequirements:\n- Introduce each target concept before using it\n- Do not use concepts not yet introduced (reference learning graph for ordering)\n- Include at least 2 worked examples per section\n- Include 5-8 practice exercises at varying Bloom's levels\n- Use MkDocs Material admonitions for tips, warnings, and notes\n- Code examples (if applicable to course domain) must be runnable\n\n#### 4.3: Per-Chapter Quality Gate\n\nScore the draft using the Chapter Quality Rubric (see Quality Scoring section):\n\n**Score &gt;= 80:** Write to file. Continue to next chapter.\n**Score 70-79:** Write to file. Note improvement areas in session log.\n**Score &lt; 70:** Revise the specific failing sections. Re-score. If still &lt; 70\n  after revision, write with a warning comment at the top of the file:\n  &lt;!-- QUALITY SCORE: [N]/100 \u2014 needs revision --&gt;\n\n#### 4.4: Write Chapter\n\nWrite the chapter to CHAPTERS_DIR/[chapter-number]/index.md.\n\nVerify mkdocs.yml includes this chapter in the nav. If missing, add it.\n\nLog chapter completion:\n- Chapter: [title]\n- Words: [N]\n- Quality score: [N]/100\n- Concepts covered: [N]\n- Time: [timestamp]\n\n### Step 5: Final Report\n\nAfter all chapters are generated, produce a summary report:\n</code></pre> Chapter Content Generation Complete ==================================== Chapters generated: [N] Total words: [N] Average quality score: [N]/100 Lowest quality chapter: [title] ([N]/100) Highest quality chapter: [title] ([N]/100)</p> <p>Chapters needing attention (score &lt; 75): - [Chapter title]: [N]/100 \u2014 [Key issue]</p> <p>Next recommended step: Run book-metrics-generator to validate cross-chapter concept coverage. <pre><code>Write this report to LOG_DIR/chapter-generation-[timestamp].md.\n</code></pre></p> <p>This example demonstrates: multi-phase scope selection at the start, parallel context loading, a planning step with user approval gate, a per-chapter generation loop with skip-if-complete detection, per-item quality scoring with automated revision, and a comprehensive final report.</p>"},{"location":"chapters/06/#workflow-design-principles-summary","title":"Workflow Design Principles \u2014 Summary","text":"<p>The following principles apply across all three examples and should guide your own workflow design:</p> Principle Implementation Step 0 is always environment setup Detect root, set paths, create dirs, open log Steps have single cohesive purpose One step = one meaningful checkpoint Error messages name the solution \"Run skill X to create the missing file\" Dialog triggers are minimum-necessary Only ask when you cannot proceed without input Skip-if-complete before overwriting Always check before destructive operations Quality gates separate generation from output Score first, write second Branching is declared at the branch point Make the decision logic explicit and readable Session logs capture everything Score, decisions, timestamps, file paths"},{"location":"chapters/06/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Step 0 is the most important step in any skill \u2014 it establishes the environment that all subsequent steps depend on</li> <li>User dialog triggers should be minimum-necessary: ask only when proceeding without input would risk unwanted changes or produce incorrect results</li> <li>Conditional branching (if X exists, follow path A; else follow path B) is the primary mechanism for handling different project states</li> <li>Skip-if-complete detection prevents redundant work in multi-step skills</li> <li>The quality gate pattern separates generation from output writing, creating a self-evaluation checkpoint before any file is saved</li> <li>Error handling should name the problem, explain why it matters, and tell the user how to fix it</li> <li>Session logging makes skills auditable, debuggable, and reproducible</li> </ul> <p>In Chapter 7, we examine the quality scoring system in depth \u2014 how to design rubrics, decompose quality into weighted sub-categories, and calibrate thresholds that produce consistent, useful results.</p>"},{"location":"chapters/07/","title":"Chapter 7: Quality Scoring Systems","text":""},{"location":"chapters/07/#building-self-evaluating-skills","title":"Building Self-Evaluating Skills","text":"<p>A skill without a quality scoring system is a generator. It produces output, but it has no mechanism for determining whether that output is good. A skill with a quality scoring system is a self-evaluating agent \u2014 it generates output, assesses it against defined criteria, and makes a decision about whether to deliver, revise, or halt based on that assessment.</p> <p>This chapter covers everything you need to design quality scoring systems for your skills: the 100-point scale, how to decompose quality into weighted sub-categories, threshold logic, examples from production skills, and a complete template rubric you can adapt for your own work.</p>"},{"location":"chapters/07/#why-quality-scoring-exists","title":"Why Quality Scoring Exists","text":"<p>The fundamental problem quality scoring solves is this: Claude generates text and code fluently, but it has no built-in mechanism for determining whether its output meets your specific standards. Left to its own judgment, Claude will produce output that is superficially plausible \u2014 well-formatted, grammatically correct, structurally appropriate \u2014 but may miss the specific quality criteria your domain requires.</p> <p>Quality scoring externalizes those criteria. Instead of relying on Claude's implicit judgment, you define the criteria explicitly in the SKILL.md file. Claude then applies those criteria to evaluate its own output before delivering it.</p> <p>This has three practical benefits:</p> <ol> <li>Consistency: The same criteria are applied every run, regardless of context, session state, or how the conversation began.</li> <li>Transparency: Users see the score and understand why the output received it. A score of 74/100 with a note \"Missing examples for 3 of 8 terms\" is actionable. \"The output is okay\" is not.</li> <li>Automation of revision: A skill that scores its own output can automatically revise low-scoring sections without user involvement, delivering a better first draft.</li> </ol>"},{"location":"chapters/07/#the-100-point-scale","title":"The 100-Point Scale","text":"<p>The 100-point scale is the standard across the McCreary skill ecosystem. It is not arbitrary \u2014 it was chosen for three reasons:</p> <ol> <li>Familiar interpretation: Humans interpret 100-point scores intuitively. A score of 73 communicates \"this is passing but not great\" immediately, without explanation.</li> <li>Granularity for meaningful weighting: Sub-categories can be weighted in increments of 5 or 10 points, producing natural integer totals.</li> <li>Threshold alignment: Common thresholds (70, 80, 85) fall on round numbers that align with letter grades and standard quality tiers.</li> </ol>"},{"location":"chapters/07/#standard-threshold-definitions","title":"Standard Threshold Definitions","text":"Score Range Grade Standard Meaning 85-100 Excellent Deliver immediately, no caveats 70-84 Good Deliver with optional improvement notes 55-69 Adequate Ask user whether to deliver or revise 40-54 Needs Work Revise before asking user Below 40 Insufficient Halt \u2014 cannot produce acceptable output <p>These thresholds are defaults. For skills that generate high-stakes content (medical education, legal documentation, production code), you may raise the minimum acceptable threshold to 80. For skills that generate draft content intended for human revision, you may lower the interactive threshold to 60.</p> <p>The key is that thresholds must be defined explicitly in the SKILL.md. A skill that says \"if quality is acceptable, proceed\" leaves \"acceptable\" undefined and produces inconsistent behavior.</p>"},{"location":"chapters/07/#decomposing-quality-into-sub-categories","title":"Decomposing Quality into Sub-Categories","text":"<p>A single undifferentiated score is less useful than a score broken into sub-categories. Sub-categories do three things:</p> <ol> <li>They force you to think precisely about what \"quality\" means for this specific type of output</li> <li>They give Claude a structured framework for assessment rather than a vague instruction to \"assess quality\"</li> <li>They give users specific, actionable feedback: \"Your score was low due to Precision (8/25), not Conciseness (23/25)\"</li> </ol>"},{"location":"chapters/07/#the-weighting-principle","title":"The Weighting Principle","text":"<p>Sub-category weights should reflect the relative importance of each criterion to the final output quality. Not all criteria are equally important. For glossary definitions, Non-circularity is binary \u2014 a circular definition is broken regardless of how well it performs on other criteria \u2014 so you might weight it higher. For README files, Accuracy is paramount because false claims are actively harmful.</p> <p>Design rule: Weights should sum to 100. Use multiples of 5 for clean arithmetic.</p>"},{"location":"chapters/07/#common-weight-distributions","title":"Common Weight Distributions","text":"<p>Equal weighting (4 criteria): 25-25-25-25 Best for: when all four criteria are genuinely equal in importance. Used in: glossary-generator (Precision, Conciseness, Distinctiveness, Non-circularity)</p> <p>Dominant criterion: 40-20-20-20 Best for: when one criterion is primary and failure on it invalidates the output regardless of other scores. Used in: course validators where learning outcome completeness is the primary criterion.</p> <p>Tiered weighting: 35-30-20-15 Best for: when criteria form a priority hierarchy. Used in: chapter content generation (Conceptual Accuracy 35, Coverage 30, Clarity 20, Formatting 15).</p>"},{"location":"chapters/07/#real-example-glossary-generator-rubric","title":"Real Example: Glossary Generator Rubric","text":"<p>The glossary-generator skill uses a four-criterion rubric derived directly from ISO 11179 metadata registry standards. Each criterion is equally weighted at 25 points.</p>"},{"location":"chapters/07/#the-rubric","title":"The Rubric","text":"<pre><code>Score each definition on four criteria (25 points each, 100 points total):\n\n**Precision (25 points)**\nDoes the definition accurately capture the concept's meaning in the course context?\n\n- 25: Definition is exact, unambiguous, and precisely correct for this domain\n- 20: Definition is accurate but slightly broader or narrower than ideal\n- 15: Definition captures the concept but contains one inaccuracy or overstatement\n- 10: Definition is partially correct but misses the core meaning\n- 0-9: Definition is inaccurate or misleading\n\n**Conciseness (25 points)**\nIs the definition the shortest it can be while remaining complete?\n\n- 25: 20-50 words, every word necessary\n- 20: 15-60 words, one or two unnecessary qualifiers\n- 15: 60-80 words, could be shortened by 25%\n- 10: Over 80 words, contains redundant explanation\n- 0-9: Severely over-length or padded\n\n**Distinctiveness (25 points)**\nDoes the definition clearly distinguish this concept from related concepts?\n\n- 25: Definition uniquely identifies this concept; could not be applied to a similar concept\n- 20: Mostly distinctive; one related concept could be confused\n- 15: Some distinctiveness; student might confuse this with a closely related concept\n- 10: Low distinctiveness; definition could apply to multiple concepts\n- 0-9: Definition applies equally well to other concepts (not distinctive)\n\n**Non-circularity (25 points)**\nDoes the definition avoid circular reasoning?\n\n- 25: No circular references; does not use the term to define itself\n- 20: One indirect circularity (uses a near-synonym of the term)\n- 10: Direct circularity (defines term using itself)\n- 0: Severely circular (definition adds no information)\n</code></pre>"},{"location":"chapters/07/#applying-the-rubric","title":"Applying the Rubric","text":"<p>The glossary-generator applies this rubric in Step 7 (Generate Quality Report). For each definition, Claude scores each criterion and computes an overall score:</p> <pre><code>Learning Graph:\n  Precision: 25/25 \u2014 Exact definition, matches ISO standard precisely\n  Conciseness: 20/25 \u2014 47 words, slightly verbose\n  Distinctiveness: 25/25 \u2014 Clearly distinguishes from Linear Curriculum\n  Non-circularity: 25/25 \u2014 No self-reference\n  Total: 95/100\n\nConcept Dependency:\n  Precision: 20/25 \u2014 Slightly broader than the canonical definition\n  Conciseness: 25/25 \u2014 31 words, concise\n  Distinctiveness: 15/25 \u2014 Could be confused with Learning Graph\n  Non-circularity: 25/25 \u2014 No self-reference\n  Total: 85/100\n</code></pre> <p>The skill then aggregates across all definitions:</p> <pre><code>Overall Quality Score: 89/100\nDefinitions below 70: 3 (will list specific issues)\nCircular definitions: 0\nAverage length: 34 words\n</code></pre>"},{"location":"chapters/07/#threshold-logic-for-the-glossary-generator","title":"Threshold Logic for the Glossary Generator","text":"<pre><code>After scoring all definitions:\n\n**Overall Score &gt;= 85:** Report score and proceed to write output.\n**Overall Score 70-84:** Proceed with note about which definitions to review.\n**Overall Score &lt; 70:** Automatically revise definitions scoring below 60.\n  Re-score. If still &lt; 70 overall, ask user whether to proceed or stop.\n\n**For individual definitions:**\n**Score &gt;= 85:** Include as-is.\n**Score 70-84:** Include with a comment in the quality report.\n**Score &lt; 70:** Revise before including. Log original and revision.\n</code></pre>"},{"location":"chapters/07/#real-example-course-description-analyzer-rubric","title":"Real Example: Course Description Analyzer Rubric","text":"<p>The course-description-analyzer uses a completeness-based rubric. Instead of evaluating quality of writing, it evaluates presence and completeness of required structural elements.</p>"},{"location":"chapters/07/#the-rubric_1","title":"The Rubric","text":"<pre><code>| Element | Points | Criteria for Full Points |\n|---------|--------|--------------------------|\n| Title | 5 | Clear, descriptive course title present |\n| Target Audience | 5 | Specific audience identified |\n| Prerequisites | 5 | Prerequisites listed or explicitly \"None\" |\n| Main Topics Covered | 10 | Comprehensive list of 5-10 topics |\n| Topics Excluded | 5 | Clear boundaries on scope |\n| Learning Outcomes Header | 5 | \"After this course, students will be able to...\" present |\n| Remember Level | 10 | 3+ specific recall outcomes |\n| Understand Level | 10 | 3+ specific comprehension outcomes |\n| Apply Level | 10 | 3+ specific application outcomes |\n| Analyze Level | 10 | 3+ specific analysis outcomes |\n| Evaluate Level | 10 | 3+ specific evaluation outcomes |\n| Create Level | 10 | 3+ specific synthesis outcomes; includes capstone |\n| Descriptive Context | 5 | Context on course importance and relevance |\n| **Total** | **100** | |\n</code></pre>"},{"location":"chapters/07/#why-this-rubric-works","title":"Why This Rubric Works","text":"<p>This rubric is entirely structural. Each element is either present and complete (full points), present but incomplete (partial points), or missing (zero points). There is almost no subjective judgment required. This makes it:</p> <ul> <li>Consistent: Two runs against the same file produce the same score</li> <li>Actionable: A score of 55 with \"Create Level: 0\" tells you exactly what to add</li> <li>Automatable: Claude can check these conditions mechanically</li> </ul> <p>For content quality rubrics (like the glossary generator), some judgment is required. For completeness rubrics (like the course description analyzer), the rubric can be almost entirely mechanical.</p>"},{"location":"chapters/07/#threshold-logic-for-course-description-analysis","title":"Threshold Logic for Course Description Analysis","text":"<pre><code>**Score &gt;= 85:** Course description is ready for learning graph generation.\n  Proceed to recommend the learning-graph-generator skill.\n\n**Score 70-84:** Course description is adequate but incomplete.\n  Report: \"Score [N]/100. The following elements need strengthening: [list].\n  You may proceed to learning graph generation, but the graph quality will\n  be limited by these gaps.\"\n\n**Score &lt; 70:** Course description needs significant work before generating\n  a learning graph.\n  Report specific missing elements with examples of what should be added.\n  Do not recommend proceeding to learning graph generation yet.\n</code></pre>"},{"location":"chapters/07/#real-example-learning-graph-quality-metrics","title":"Real Example: Learning Graph Quality Metrics","text":"<p>The learning-graph-generator uses a structural validity rubric that evaluates the mathematical properties of a directed acyclic graph (DAG) rather than content quality.</p>"},{"location":"chapters/07/#the-rubric_2","title":"The Rubric","text":"<pre><code>**Learning Graph Quality Score (100 points)**\n\nStructural Integrity (40 points):\n- Zero circular dependencies (DAG constraint): 20 pts\n  - Any circular dependency: 0 pts (automatic failure indicator)\n- All concepts reachable (connected graph): 10 pts\n- No orphaned leaf nodes (every concept depends on or is depended on): 10 pts\n\nCoverage Balance (30 points):\n- Foundational concepts (zero dependencies) are 10-20% of total: 10 pts\n- No single taxonomy category exceeds 30% of concepts: 10 pts\n- Average dependencies per concept is 2-4: 10 pts\n\nVocabulary Quality (30 points):\n- All concept labels in Title Case: 10 pts\n- All concept labels under 32 characters: 10 pts\n- Zero duplicate concept labels: 10 pts\n</code></pre>"},{"location":"chapters/07/#the-critical-sub-category","title":"The Critical Sub-Category","text":"<p>Note that \"Zero circular dependencies\" receives 20 points but functions as a veto criterion: any circular dependency in the graph means the graph cannot be used, regardless of the other scores. The workflow handles this with special logic:</p> <pre><code>**Special Case: Circular Dependency Detected**\n\nIf analyze-graph.py reports any circular dependencies:\nHalt immediately with this message:\n\n\"CRITICAL: [N] circular dependencies detected in the learning graph.\nA circular dependency means concept A depends on B, which depends on A \u2014\ncreating an impossible learning order.\n\nCircular dependencies found:\n- [Concept A] \u2192 [Concept B] \u2192 [Concept A]\n- [Concept X] \u2192 [Concept Y] \u2192 [Concept Z] \u2192 [Concept X]\n\nThe learning graph CANNOT be used until these cycles are removed.\nPlease remove the listed dependency edges and re-run this skill.\"\n\nDo not calculate an overall quality score when circular dependencies exist.\n</code></pre> <p>This is an example of a sub-category that, when it fails, overrides the overall scoring calculation. Not every rubric needs this pattern, but for constraints that are mathematically required (like DAG acyclicity), it is appropriate.</p>"},{"location":"chapters/07/#scoring-vs-checklist-validation","title":"Scoring vs. Checklist Validation","text":"<p>Quality scoring is not always the right tool. Some outputs are better validated by a checklist \u2014 a list of binary yes/no conditions that must all be satisfied.</p>"},{"location":"chapters/07/#when-to-use-quality-scoring","title":"When to Use Quality Scoring","text":"<p>Use a numeric quality score when:</p> <ul> <li>Output quality exists on a spectrum (not just pass/fail)</li> <li>Different users might have different acceptable thresholds</li> <li>You want to produce graduated responses (proceed / proceed with notes / ask / halt)</li> <li>Sub-category scores provide useful diagnostic information</li> </ul> <p>Examples: glossary definitions, chapter content, README files, quiz questions</p>"},{"location":"chapters/07/#when-to-use-checklist-validation","title":"When to Use Checklist Validation","text":"<p>Use a checklist when:</p> <ul> <li>The conditions are binary (present or absent, valid or invalid)</li> <li>All conditions must be satisfied \u2014 partial compliance is not acceptable</li> <li>Failure on any single item is a blocker</li> </ul> <p>Examples: DAG cycle detection (a cycle exists or it does not), required file presence checks, syntax validation of generated JSON or YAML</p>"},{"location":"chapters/07/#hybrid-approach-checklist-gates-quality-score","title":"Hybrid Approach: Checklist Gates + Quality Score","text":"<p>Many production skills use both:</p> <ol> <li>Step 1: Run checklist validation. Halt if any blocker condition is triggered.</li> <li>Step 2: After passing validation, run quality scoring on the content.</li> </ol> <pre><code>### Step 2: Validate Structure\n\nRun checklist validation before quality scoring:\n\nRequired conditions (all must pass):\n- [ ] JSON file is syntactically valid (parseable)\n- [ ] All required fields present (id, label, dependencies, taxonomyID)\n- [ ] No duplicate ConceptIDs\n- [ ] No circular dependencies (run analyze-graph.py)\n\nIf any condition fails: Halt with specific error message.\nIf all conditions pass: Proceed to quality scoring.\n\n### Step 3: Quality Scoring\n\nOnly reached if Step 2 passes completely.\n\n[Apply quality rubric to concept labels, taxonomy distribution, dependency density...]\n</code></pre>"},{"location":"chapters/07/#designing-your-own-rubric","title":"Designing Your Own Rubric","text":"<p>The following template is a starting point for any new skill rubric. Fill in the blanks based on your skill's specific output.</p>"},{"location":"chapters/07/#template-4-category-100-point-rubric","title":"Template: 4-Category 100-Point Rubric","text":"<pre><code>## Quality Scoring Reference\n\nScore the [output type] on four criteria (total: 100 points):\n\n**[Category 1 Name] ([Weight] points)**\n[One sentence describing what this criterion measures]\n\nDefinition of scores:\n- [Weight] points: [Describe excellent performance]\n- [Weight * 0.8] points: [Describe good performance]\n- [Weight * 0.6] points: [Describe adequate performance]\n- [Weight * 0.4] points: [Describe poor performance]\n- 0-[Weight * 0.2] points: [Describe failing performance]\n\n**[Category 2 Name] ([Weight] points)**\n[One sentence describing what this criterion measures]\n\n[Scoring levels as above]\n\n**[Category 3 Name] ([Weight] points)**\n[One sentence describing what this criterion measures]\n\n[Scoring levels as above]\n\n**[Category 4 Name] ([Weight] points)**\n[One sentence describing what this criterion measures]\n\n[Scoring levels as above]\n\n**Overall Score Thresholds:**\n\n| Score | Action |\n|-------|--------|\n| [High threshold]-100 | [e.g., 85-100: Deliver immediately] |\n| [Mid threshold]-[High threshold-1] | [e.g., 70-84: Deliver with notes] |\n| [Low threshold]-[Mid threshold-1] | [e.g., 55-69: Ask user] |\n| Below [Low threshold] | [e.g., Below 55: Revise before delivering] |\n</code></pre>"},{"location":"chapters/07/#worked-template-application-quiz-question-rubric","title":"Worked Template Application: Quiz Question Rubric","text":"<p>Here is the template applied to a quiz question generator:</p> <pre><code>## Quality Scoring Reference\n\nScore each quiz question on four criteria (total: 100 points):\n\n**Alignment to Bloom's Level (30 points)**\nDoes the question genuinely require the cognitive operation specified by its\nassigned Bloom's Taxonomy level?\n\n- 30 points: Question clearly requires the specified cognitive operation; could not\n  be answered by a student operating at a lower level\n- 24 points: Question mostly aligns; minor ambiguity in required cognitive level\n- 18 points: Question is at the right level but the wording allows shortcuts\n  (e.g., a \"Analyze\" question that can be answered by recall)\n- 12 points: Question is at the wrong level (e.g., labeled \"Create\" but only\n  requires recall)\n- 0-9 points: Level assignment is incorrect and misleading\n\n**Concept Specificity (25 points)**\nDoes the question test a specific, identifiable concept from the learning graph?\n\n- 25 points: Question targets one specific concept; correct answer requires\n  mastery of exactly that concept\n- 20 points: Question targets one concept but has one minor ambiguity\n- 15 points: Question could be answered with knowledge of multiple unrelated concepts\n- 10 points: Question is too broad (tests general knowledge rather than specific concepts)\n- 0-9 points: Question does not map to any concept in the learning graph\n\n**Distractors Quality (25 points)**\nAre the incorrect answer choices plausible but unambiguously wrong?\n\n- 25 points: All distractors are plausible to a student who misunderstood the concept;\n  none are obviously wrong\n- 20 points: Most distractors are plausible; one is too easy to eliminate\n- 15 points: Some distractors are implausible; a student could guess without knowledge\n- 10 points: Multiple distractors are obviously wrong\n- 0-9 points: Distractors are trivially wrong; question is a giveaway\n\n**Clarity and Fairness (20 points)**\nIs the question unambiguous and fair to all students?\n\n- 20 points: Question has one unambiguous correct answer; no cultural or language bias\n- 16 points: Question is clear but slightly technical in phrasing\n- 12 points: Question has minor ambiguity that could cause debate about the answer\n- 8 points: Question is ambiguous; reasonable argument could be made for multiple answers\n- 0-7 points: Question is ambiguous, biased, or has multiple defensible answers\n\n**Overall Score Thresholds:**\n\n| Score | Action |\n|-------|--------|\n| 80-100 | Include in quiz as-is |\n| 65-79 | Include with a note in the quality log |\n| 50-64 | Revise before including |\n| Below 50 | Discard and regenerate |\n</code></pre>"},{"location":"chapters/07/#scoring-in-practice-what-claude-actually-does","title":"Scoring in Practice: What Claude Actually Does","text":"<p>When you include a quality scoring rubric in your SKILL.md, Claude does not merely output a score. It applies the rubric through a structured self-evaluation process:</p> <ol> <li>It reads the rubric criteria from the Quality Scoring section</li> <li>It evaluates the output against each criterion and assigns a score with reasoning</li> <li>It computes the total and applies the threshold logic</li> <li>It acts on the result \u2014 proceeding, noting, asking, or halting</li> </ol> <p>The scoring narration might look like this in practice:</p> <pre><code>Evaluating definition for \"Directed Acyclic Graph\":\n\nPrecision: 22/25 \u2014 Captures the mathematical concept accurately, though\n\"directed\" could be more clearly explained as \"one-directional edges.\"\n\nConciseness: 25/25 \u2014 28 words, no filler.\n\nDistinctiveness: 20/25 \u2014 Could be confused with \"Directed Graph\" by a\nstudent who does not yet know the difference.\n\nNon-circularity: 25/25 \u2014 No self-reference.\n\nTotal: 92/100 \u2014 Excellent. Proceeding.\n</code></pre> <p>This narration is valuable even if the user never explicitly requests it, because it makes the scoring transparent. If the score is unexpectedly low, the user can see exactly why.</p>"},{"location":"chapters/07/#calibrating-thresholds-for-your-domain","title":"Calibrating Thresholds for Your Domain","text":"<p>The standard thresholds (85/70/55) are starting points. Adjust them based on the stakes and revision cost of your output:</p> <p>Raise thresholds when: - Output will be published without human review - Errors in the output are difficult or costly to fix - The audience requires high precision (medical, legal, financial domains)</p> <p>Lower thresholds when: - Output is a draft intended for human revision - The cost of re-running the skill is low - The user explicitly wants to see lower-quality output for reference or iteration</p> <p>Example: Medical education content</p> <pre><code>**Overall Score Thresholds:**\n\n| Score | Action |\n|-------|--------|\n| 90-100 | Deliver immediately |\n| 80-89 | Deliver with specific improvement notes |\n| 70-79 | Ask: \"Score [N]/100. Recommend revision. Deliver or revise?\" |\n| Below 70 | Halt. Medical content with score below 70 should not be delivered. |\n</code></pre> <p>Example: First-draft blog content</p> <pre><code>**Overall Score Thresholds:**\n\n| Score | Action |\n|-------|--------|\n| 75-100 | Deliver immediately |\n| 60-74 | Deliver with notes about weakest sections |\n| Below 60 | Ask: \"Score [N]/100. Continue or revise?\" |\n</code></pre>"},{"location":"chapters/07/#common-rubric-design-mistakes","title":"Common Rubric Design Mistakes","text":"<p>Mistake 1: Criteria that overlap</p> <p>If two criteria measure the same thing, scores will be inflated and the diagnostic value is lost.</p> <pre><code># Wrong: Precision and Accuracy measure the same thing\nPrecision (25 pts): Is the definition correct?\nAccuracy (25 pts): Does the definition accurately describe the concept?\n\n# Right: Each criterion measures a distinct dimension\nPrecision (25 pts): Does the definition target the specific concept exactly,\n  without being too broad or too narrow?\nAccuracy (25 pts): Are all claims in the definition factually correct?\n</code></pre> <p>Mistake 2: Subjective criteria without anchor descriptions</p> <p>A criterion like \"Is the definition good?\" gives Claude nothing to evaluate against. Every criterion needs level descriptions that anchor the score to observable properties.</p> <pre><code># Wrong: No anchor descriptions\nClarity (25 pts): Is the definition clear?\n\n# Right: Anchored to observable properties\nClarity (25 pts):\n- 25: An intelligent reader unfamiliar with the domain can understand the definition\n  without additional context\n- 15: The definition is understandable to someone with general domain familiarity\n- 5: The definition requires domain expertise to interpret correctly\n</code></pre> <p>Mistake 3: Weights that do not reflect actual importance</p> <p>If you use 25-25-25-25 but actually care far more about one criterion, the score will not reflect real quality. Be honest about the relative importance of your criteria.</p> <p>Mistake 4: A threshold of exactly 70 as both \"proceed\" and \"ask\"</p> <p>Define thresholds as ranges with clear boundaries. \"Score &gt;= 70: proceed. Score &lt; 70: ask.\" is clear. \"Score = 70: unclear.\" is ambiguous. Use <code>&gt;=</code> and <code>&lt;</code> with clear boundaries.</p>"},{"location":"chapters/07/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Quality scoring transforms skills from generators into self-evaluating agents</li> <li>The 100-point scale is standard across the McCreary ecosystem because it is intuitive and provides natural weighting granularity</li> <li>Sub-categories make quality diagnostic: a score of 73/100 with \"Distinctiveness: 8/25\" is more actionable than just \"73/100\"</li> <li>Standard thresholds are 85 (excellent/deliver), 70 (good/deliver with notes), 55 (adequate/ask), below 55 (needs work/halt)</li> <li>Some criteria function as veto conditions: if circular dependencies exist, the learning graph score is irrelevant</li> <li>Use quality scoring for spectrum outputs; use checklists for binary conditions; combine them in the order checklist gate \u2192 quality score</li> <li>Calibrate thresholds to the stakes of your domain \u2014 raise them for high-stakes content, lower them for first-draft work</li> <li>Anchor every criterion level to observable, measurable properties \u2014 not subjective impressions</li> </ul> <p>This chapter completes the Skill Anatomy section of this guide. You now have everything you need to write complete, production-quality SKILL.md files with proper frontmatter, structured workflows, and self-evaluating quality scoring systems. In Chapter 8, we move into Advanced Patterns \u2014 starting with meta-skill routers that consolidate multiple skills under a single invocation point.</p>"},{"location":"chapters/08/","title":"Chapter 8: Meta-Skill Routers","text":"<p>A single skill handles one job well. But what happens when a domain naturally splits into multiple related workflows \u2014 each sharing a common entry point, the same quality standards, and many of the same concepts, but diverging sharply in execution?</p> <p>That is the problem meta-skill routers solve.</p> <p>This chapter explains the routing pattern in depth: how it works, when to use it, and how to build a production-quality meta-skill from scratch. By the end you will have a complete worked example \u2014 a <code>report-generator</code> meta-skill with four distinct sub-skill variants \u2014 including every file in the directory structure.</p>"},{"location":"chapters/08/#what-is-a-meta-skill","title":"What Is a Meta-Skill?","text":"<p>A meta-skill is a skill whose primary job is routing. When invoked, it does not immediately begin producing output. Instead, it analyzes the user's request, identifies which sub-skill variant is appropriate, loads the relevant reference guide, and then executes within that variant's context.</p> <p>From the user's perspective, they invoke one skill. Under the hood, the meta-skill transparently selects and runs the right workflow for their specific request.</p> <p>This pattern solves three real problems:</p> <p>1. The proliferation problem. If every sub-variant were its own skill, users would need to know which variant to pick before they even understood the domain. A <code>report-generator</code> entry point is more intuitive than having to choose between <code>executive-summary-generator</code>, <code>technical-spec-generator</code>, <code>sales-report-generator</code>, and <code>ops-dashboard-generator</code>.</p> <p>2. The token budget problem. A SKILL.md file that tries to embed full instructions for fourteen sub-variants would balloon to 15,000+ tokens. The system prompt budget is fixed. Lazy loading reference guides means you only pay the token cost for the variant you actually need. (This is covered in depth in Chapter 9.)</p> <p>3. The maintenance problem. Shared scaffolding \u2014 quality scoring, log format, file naming conventions \u2014 lives in the meta-skill SKILL.md. Variant-specific detail lives in the reference guide. You update each in exactly one place.</p>"},{"location":"chapters/08/#when-to-build-a-meta-skill","title":"When to Build a Meta-Skill","text":"<p>Use a meta-skill when all of the following are true:</p> <ul> <li>You have 3 or more sub-variants that share a common user intent but diverge in workflow steps</li> <li>The variants share quality criteria, output format conventions, or file naming patterns</li> <li>The correct variant is usually determinable from the user's request without interactive disambiguation</li> <li>Each variant's detail is substantial enough (500+ tokens) to justify lazy loading</li> </ul> <p>Do not use a meta-skill for two variants with minor differences. A conditional in a single SKILL.md handles that cleanly. Reserve the routing pattern for genuine divergence.</p>"},{"location":"chapters/08/#the-routing-table-pattern","title":"The Routing Table Pattern","text":"<p>The core mechanism is a routing table \u2014 a structured mapping from keywords or request patterns to reference file paths.</p> <pre><code>routing:\n  keywords:\n    executive:     references/executive-summary-guide.md\n    summary:       references/executive-summary-guide.md\n    leadership:    references/executive-summary-guide.md\n    technical:     references/technical-spec-guide.md\n    spec:          references/technical-spec-guide.md\n    architecture:  references/technical-spec-guide.md\n    sales:         references/sales-report-guide.md\n    pipeline:      references/sales-report-guide.md\n    revenue:       references/sales-report-guide.md\n    ops:           references/ops-dashboard-guide.md\n    operations:    references/ops-dashboard-guide.md\n    metrics:       references/ops-dashboard-guide.md\n    dashboard:     references/ops-dashboard-guide.md\n</code></pre> <p>Claude reads this table as part of the SKILL.md. When the user's request contains a matching keyword, Claude loads the corresponding reference file and proceeds with that variant's workflow.</p> <p>Multiple keywords mapping to the same file is intentional and important. Users rarely use the exact vocabulary you expect. \"Revenue breakdown\" should route the same as \"sales report.\" Covering synonyms and related terms dramatically reduces routing failures.</p>"},{"location":"chapters/08/#keyword-matching-rules","title":"Keyword Matching Rules","text":"<p>The routing logic Claude applies when reading this table follows a priority order:</p> <ol> <li>Exact keyword match in the user's request \u2014 highest confidence</li> <li>Semantic match \u2014 the concept is clearly present even if the word differs (\"Q3 numbers\" implies sales/revenue context)</li> <li>Context from prior messages \u2014 if the conversation already established report type, use it</li> <li>Decision tree \u2014 when ambiguity remains after the above</li> </ol> <p>This is not regex matching. Claude uses natural language understanding, which means the routing table functions as a signal vocabulary rather than a rigid parser. Write your keyword list to seed Claude's semantic understanding, not to enumerate every possible phrasing.</p>"},{"location":"chapters/08/#the-decision-tree-for-ambiguous-requests","title":"The Decision Tree for Ambiguous Requests","text":"<p>Not every request resolves cleanly to one route. \"Generate a report on Q3 performance\" is ambiguous \u2014 it could be executive, sales, or ops depending on context.</p> <p>A decision tree handles this. In the meta-skill SKILL.md, after the routing table, include an explicit disambiguation section:</p> <pre><code>## Disambiguation Protocol\n\nIf routing is ambiguous after keyword matching, apply this decision tree:\n\n1. Is the primary audience C-suite or board-level?\n   YES \u2192 executive-summary-guide.md\n   NO  \u2192 continue\n\n2. Does the request emphasize technical architecture, APIs, or system design?\n   YES \u2192 technical-spec-guide.md\n   NO  \u2192 continue\n\n3. Does the request emphasize revenue, deals, pipeline, or quota?\n   YES \u2192 sales-report-guide.md\n   NO  \u2192 continue\n\n4. Default to ops-dashboard-guide.md for operational metrics, KPIs, or\n   process performance.\n\n5. If still unclear after applying this tree, ask the user ONE clarifying\n   question: \"Is this report primarily for leadership, technical review,\n   sales tracking, or operational monitoring?\"\n</code></pre> <p>The decision tree has two properties that make it effective:</p> <ul> <li>It is deterministic. Working through the tree from top to bottom always produces an answer.</li> <li>It has an escape hatch. If the tree still cannot resolve ambiguity, Claude asks one targeted question rather than guessing or refusing.</li> </ul> <p>Keep the escape hatch tight. \"Ask ONE clarifying question\" is a constraint, not a suggestion. Users who invoke a skill want immediate output, not an interview.</p>"},{"location":"chapters/08/#the-references-directory","title":"The References Directory","text":"<p>The <code>references/</code> directory lives inside the skill folder:</p> <pre><code>~/.claude/skills/report-generator/\n\u251c\u2500\u2500 SKILL.md\n\u2514\u2500\u2500 references/\n    \u251c\u2500\u2500 executive-summary-guide.md\n    \u251c\u2500\u2500 technical-spec-guide.md\n    \u251c\u2500\u2500 sales-report-guide.md\n    \u2514\u2500\u2500 ops-dashboard-guide.md\n</code></pre> <p>Each reference guide contains the full, detailed workflow for that variant. It is written as a standalone document that Claude can execute once loaded.</p> <p>Reference guides are never loaded speculatively. The meta-skill loads exactly one guide \u2014 the one that matched the routing decision. All other variant guides consume zero tokens in that session.</p>"},{"location":"chapters/08/#what-goes-in-a-reference-guide","title":"What Goes in a Reference Guide","text":"<p>A reference guide is not a summary or a pointer. It is the complete execution specification for its variant. It should contain:</p> <ul> <li>Workflow steps \u2014 numbered, sequential, specific</li> <li>Output format \u2014 exact structure, sections, headings expected in the output document</li> <li>Quality criteria \u2014 what \"good\" looks like for this variant</li> <li>Examples \u2014 at least one condensed example of correct output structure</li> <li>Variant-specific warnings \u2014 common mistakes unique to this type</li> </ul> <p>Think of the reference guide as a sub-skill SKILL.md that happened to be stored separately for token efficiency reasons. Write it with the same completeness you would give a standalone skill.</p>"},{"location":"chapters/08/#worked-example-report-generator","title":"Worked Example: report-generator","text":"<p>Here is the complete file structure and content for a production <code>report-generator</code> meta-skill.</p>"},{"location":"chapters/08/#file-structure","title":"File Structure","text":"<pre><code>~/.claude/skills/report-generator/\n\u251c\u2500\u2500 SKILL.md\n\u2514\u2500\u2500 references/\n    \u251c\u2500\u2500 executive-summary-guide.md\n    \u251c\u2500\u2500 technical-spec-guide.md\n    \u251c\u2500\u2500 sales-report-guide.md\n    \u2514\u2500\u2500 ops-dashboard-guide.md\n</code></pre>"},{"location":"chapters/08/#skillmd","title":"SKILL.md","text":"<p><pre><code>---\nname: report-generator\nversion: 1.2.0\ndescription: &gt;\n  Generates structured business reports. Routes to one of four variants:\n  executive summary, technical specification, sales performance, or\n  operational dashboard. Provide a topic, time period, and any available\n  data \u2014 the skill selects the appropriate format automatically.\nauthor: your-name\ntags: [reports, business, meta-skill, router]\n---\n\n# Report Generator\n\n## Purpose\n\nGenerate professional business reports in the format appropriate for your\naudience and purpose. This skill routes automatically between four variants\nbased on the keywords and context in your request.\n\n## Invocation\n</code></pre> /report-generator [describe your report topic and audience] <pre><code>Examples:\n- `/report-generator Q3 executive summary for the board`\n- `/report-generator technical spec for the authentication service redesign`\n- `/report-generator sales pipeline report, week ending Nov 15`\n- `/report-generator ops dashboard for customer support team, October`\n\n## Routing Table\n\n| Keywords | \u2192 Variant | Reference |\n|----------|-----------|-----------|\n| executive, summary, board, leadership, C-suite | Executive Summary | references/executive-summary-guide.md |\n| technical, spec, architecture, system, API, design | Technical Spec | references/technical-spec-guide.md |\n| sales, pipeline, revenue, quota, deals, ARR, MRR | Sales Report | references/sales-report-guide.md |\n| ops, operations, metrics, dashboard, KPIs, SLA, support | Ops Dashboard | references/ops-dashboard-guide.md |\n\n## Disambiguation Protocol\n\nIf routing is ambiguous after keyword matching:\n\n1. Is the primary audience C-suite or board-level? \u2192 executive-summary-guide.md\n2. Does the request center on technical architecture or system design? \u2192 technical-spec-guide.md\n3. Does the request center on revenue, deals, or quota attainment? \u2192 sales-report-guide.md\n4. Default: ops-dashboard-guide.md\n\nIf still unclear: ask \"Is this report for leadership review, technical\ndocumentation, sales tracking, or operational monitoring?\"\n\n## Execution Protocol\n\n1. Parse the user's request for routing keywords\n2. Apply the routing table (highest-confidence match wins)\n3. If ambiguous, apply the disambiguation protocol\n4. Read the matched reference guide\n5. Execute the variant workflow from the reference guide\n6. Apply the shared quality checklist below\n7. Write the session log\n\n## Shared Quality Checklist\n\nBefore delivering any output, verify:\n- [ ] Report has a clear title with date/period\n- [ ] Executive summary / TL;DR section present (even technical reports)\n- [ ] All data claims are attributed or flagged as estimates\n- [ ] Recommendations section present if report type supports it\n- [ ] Output written in the correct register for the target audience\n\n## Output Location\n\nReports are saved to: `./reports/[variant]-[YYYY-MM-DD].md`\n\n## Session Log\n\nAfter completion, write a log entry to:\n`./logs/report-generator-v1.2.0-[DATE].md`\n\nInclude: variant selected, routing confidence (high/medium/low), time to\ncomplete, word count, quality score (0-10).\n</code></pre></p>"},{"location":"chapters/08/#referencesexecutive-summary-guidemd","title":"references/executive-summary-guide.md","text":"<pre><code># Executive Summary Variant \u2014 Workflow Guide\n\n## Audience\n\nC-suite leaders, board members, investors. Readers who have 3 minutes, not 30.\nEvery sentence must earn its place. Cut ruthlessly.\n\n## Output Structure\n\n1. **Title Block** \u2014 Report title, date, author, period covered\n2. **One-Sentence Summary** \u2014 The single most important thing\n3. **Key Findings** (3-5 bullets, each under 20 words)\n4. **Business Impact** \u2014 Dollar amounts, percentages, customer counts\n5. **Recommended Actions** (2-4 items, each with an owner and deadline)\n6. **Supporting Context** \u2014 Optional, max 200 words, for readers who want more\n\nTotal target length: 400-600 words. Never exceed 800.\n\n## Workflow Steps\n\n1. Identify the primary business question the report must answer\n2. Extract the 3-5 findings that most directly answer that question\n3. Quantify each finding (no qualitative-only bullets in an exec summary)\n4. Write the one-sentence summary last, after findings are clear\n5. Write recommendations as action verbs: \"Hire,\" \"Approve,\" \"Pause,\" \"Redirect\"\n6. Remove all jargon. If a term needs defining, the term is wrong for this audience.\n\n## Quality Criteria\n\n- Findings are quantified (not \"significantly improved\" but \"improved 23%\")\n- Recommendations have explicit owners or owner roles\n- No sentence exceeds 25 words\n- A reader who skips to page 2 of supporting context still has everything they need from pages 0-1\n\n## Common Mistakes\n\n- Including methodology in the body (it belongs in an appendix if anywhere)\n- Writing \"As you can see...\" \u2014 the reader should see without narration\n- Using passive voice for recommendations (\"Action should be taken\" \u2192 \"CMO to approve budget by Nov 30\")\n</code></pre>"},{"location":"chapters/08/#referencestechnical-spec-guidemd","title":"references/technical-spec-guide.md","text":"<pre><code># Technical Specification Variant \u2014 Workflow Guide\n\n## Audience\n\nEngineers, architects, technical leads. Readers who will implement or review\nthe system being specified. Precision over brevity. Ambiguity is a defect.\n\n## Output Structure\n\n1. **Title Block** \u2014 Document title, version, date, authors, status (Draft/Review/Approved)\n2. **Overview** \u2014 What the system does and why it exists (2-3 paragraphs)\n3. **Scope** \u2014 What is in scope, what is explicitly out of scope\n4. **Architecture Diagram** \u2014 Mermaid block required for any system with &gt;2 components\n5. **Component Specifications** \u2014 One subsection per component\n6. **Data Model** \u2014 Entity relationships, key fields, types\n7. **API Contracts** (if applicable) \u2014 Endpoint, method, request/response schema\n8. **Error Handling** \u2014 What can go wrong and how the system responds\n9. **Open Questions** \u2014 Unresolved decisions, flagged explicitly\n10. **Revision History** \u2014 Version table at bottom\n\n## Workflow Steps\n\n1. Identify all system components from the user's description\n2. Determine boundaries (what this system owns vs. what it calls)\n3. Draft the architecture diagram before writing prose\n4. Write component specs in dependency order (upstream components first)\n5. Flag every assumption explicitly as [ASSUMPTION: ...]\n6. Flag every open decision explicitly as [OPEN: ...]\n\n## Quality Criteria\n\n- No ambiguous pronoun references (\"it,\" \"they,\" \"this\") without clear antecedents\n- Every interface is specified bidirectionally (what caller sends, what callee returns)\n- Error states are enumerated, not handwaved (\"handles errors appropriately\" is not acceptable)\n- Open questions are distinguished from decisions already made\n\n## Common Mistakes\n\n- Writing prose for things that should be tables or diagrams\n- Assuming the reader shares context the author has\n- Omitting the \"out of scope\" section (creates scope creep later)\n</code></pre>"},{"location":"chapters/08/#referencessales-report-guidemd","title":"references/sales-report-guide.md","text":"<pre><code># Sales Performance Report Variant \u2014 Workflow Guide\n\n## Audience\n\nSales leadership, RevOps, finance. Readers tracking quota attainment,\npipeline health, and revenue trends against targets.\n\n## Output Structure\n\n1. **Title Block** \u2014 Report title, period, author, date generated\n2. **Period Summary** (4 key metrics in a 2x2 grid):\n   - Closed revenue vs. target\n   - Pipeline coverage ratio\n   - New opportunities created\n   - Average deal cycle (days)\n3. **Closed Deals Table** \u2014 Deal name, ACV, close date, rep, stage\n4. **Pipeline Snapshot** \u2014 By stage, with aging flags (&gt;90 days at stage = flag)\n5. **Rep Performance** \u2014 Quota attainment % per rep (no names if requested)\n6. **Trend Line** \u2014 Last 4 periods, same metrics as Period Summary\n7. **Risk Flags** \u2014 Deals at risk, slippage, churn signals\n8. **Forecast** \u2014 Current quarter projection with confidence range\n\n## Workflow Steps\n\n1. Extract all deal/revenue data from user input\n2. Compute period-over-period changes for all key metrics\n3. Flag any deal &gt;90 days at current stage\n4. Calculate pipeline coverage (pipeline value / remaining quota target)\n5. Generate forecast as a range, not a point estimate\n6. Write risk flags before recommendations \u2014 risks inform recommendations\n\n## Quality Criteria\n\n- All percentages reference a clear baseline\n- Forecast includes confidence range, not just a number\n- Aging flags use consistent threshold (default: 90 days)\n- Rep performance section is present even if data is aggregate-only\n\n## Common Mistakes\n\n- Reporting revenue without comparing to target\n- Presenting a point forecast without confidence range\n- Omitting pipeline coverage ratio (most critical leading indicator)\n</code></pre>"},{"location":"chapters/08/#referencesops-dashboard-guidemd","title":"references/ops-dashboard-guide.md","text":"<pre><code># Operational Dashboard Variant \u2014 Workflow Guide\n\n## Audience\n\nOperations managers, team leads, department heads. Readers monitoring\nprocess performance against SLAs, capacity, and efficiency targets.\n\n## Output Structure\n\n1. **Title Block** \u2014 Dashboard title, period, team/system, date generated\n2. **Health Summary** \u2014 RAG (Red/Amber/Green) status for each key area\n3. **Volume Metrics** \u2014 Throughput, load, transaction counts for the period\n4. **SLA Performance** \u2014 Each SLA with target, actual, variance\n5. **Incident Summary** \u2014 Count, severity breakdown, MTTR, top repeat issues\n6. **Capacity Utilization** \u2014 Current utilization vs. designed capacity\n7. **Trend Comparison** \u2014 Same period last month / last quarter\n8. **Action Items** \u2014 Prioritized list, each with owner and due date\n\n## Workflow Steps\n\n1. Identify the team or system being monitored\n2. List all SLAs or KPIs the user has provided\n3. Assign RAG status to each: Red = missed target, Amber = within 10% of threshold, Green = on target\n4. Calculate MTTR for incidents if incident data is available\n5. Flag capacity risks if utilization &gt;80%\n6. Write action items last, after all data sections are populated\n\n## Quality Criteria\n\n- Every metric has a target to compare against (no contextless numbers)\n- RAG status is applied consistently using the defined thresholds\n- Capacity section flags utilization &gt;80% as amber, &gt;95% as red\n- Action items are specific and assigned (not \"review this area\")\n\n## Common Mistakes\n\n- Reporting metrics without targets\n- Mixing rolling averages with point-in-time snapshots without labeling\n- Burying critical incidents in narrative rather than surfacing in the summary\n</code></pre>"},{"location":"chapters/08/#building-your-own-meta-skill-the-process","title":"Building Your Own Meta-Skill: The Process","text":""},{"location":"chapters/08/#step-1-inventory-your-variants","title":"Step 1: Inventory Your Variants","text":"<p>List all the sub-variants your domain needs. For each one, write a one-sentence description of who uses it and what they need. If you cannot write that sentence clearly, the variant is not well-defined yet \u2014 clarify it before building.</p>"},{"location":"chapters/08/#step-2-build-the-routing-table","title":"Step 2: Build the Routing Table","text":"<p>Extract the vocabulary each variant's users naturally use. Include: - The canonical term (what you'd call it) - Synonyms users might reach for - Related concepts that correlate with this variant</p> <p>Aim for 4-6 keywords per variant. Fewer than 3 creates routing gaps. More than 8 starts to introduce false matches across variants.</p>"},{"location":"chapters/08/#step-3-design-the-decision-tree","title":"Step 3: Design the Decision Tree","text":"<p>Walk through three deliberately ambiguous request examples. If your routing table resolves all three correctly, the decision tree is a safety net for edge cases. If the table fails on any of the three, refine it before writing the decision tree.</p> <p>The decision tree should have at most 4-5 nodes. If it requires more, your variants are not well enough differentiated and you should reconsider the decomposition.</p>"},{"location":"chapters/08/#step-4-write-the-reference-guides","title":"Step 4: Write the Reference Guides","text":"<p>Write each reference guide as if it were a standalone SKILL.md. Include the full workflow, complete output structure, quality criteria, and common mistakes. Do not abbreviate or summarize \u2014 the whole point of the reference file is to carry the full detail that you offloaded from the main SKILL.md.</p> <p>A reference guide that is too thin defeats the purpose. If a guide is under 300 words, it probably should not be a separate file \u2014 fold it back into the main SKILL.md as a conditional section.</p>"},{"location":"chapters/08/#step-5-test-the-routing","title":"Step 5: Test the Routing","text":"<p>Before deploying, test with a set of requests that includes: - One clear example of each variant - Two ambiguous requests that require the decision tree - One request that should trigger the clarifying question</p> <p>Document the routing result for each test case. If any case routes incorrectly, adjust the keyword table or decision tree before releasing.</p>"},{"location":"chapters/08/#routing-failures-and-how-to-handle-them","title":"Routing Failures and How to Handle Them","text":"<p>Even well-designed routing tables fail. These are the common failure modes and their fixes:</p> Failure Mode Symptom Fix Vocabulary mismatch User says \"briefing\" but no variant covers that Add synonym to routing table Cross-variant pollution Technical terms appear in sales requests Add negative signals or decision tree guard Over-specification Decision tree asks 3 questions before routing Simplify tree; add more keywords Ambiguity explosion &gt;30% of requests hit the escape hatch Variants are not well-differentiated; redesign Reference guide not found Claude cannot locate the reference file Verify path in routing table exactly matches filesystem <p>The most important diagnostic is the escape hatch rate. If more than 20-30% of invocations end up at the clarifying question, the routing design needs work, not just the keyword list.</p>"},{"location":"chapters/08/#anti-patterns-to-avoid","title":"Anti-Patterns to Avoid","text":"<p>Do not embed full variant workflows in the SKILL.md. The entire purpose of the meta-skill pattern is to keep the main file lean. If you find yourself writing 2,000 words of variant-specific workflow in SKILL.md, move it to a reference guide.</p> <p>Do not build a meta-skill for two variants. Two variants is a conditional, not a router. Use an <code>if/else</code> block in a single SKILL.md instead.</p> <p>Do not let reference guides diverge in quality. If three guides are excellent and one is thin, routing to that thin guide will produce noticeably worse output. Maintain all guides to the same standard.</p> <p>Do not silently default. If routing fails and the skill silently picks a default variant without informing the user, errors are invisible. Always log the routing decision and, if using the escape hatch, always tell the user which variant you selected and why.</p>"},{"location":"chapters/08/#summary","title":"Summary","text":"<p>The meta-skill router pattern solves the consolidation-vs-detail tension by separating routing logic from execution detail. The SKILL.md stays lean and handles routing. The reference guides carry full execution specifications, loaded only when needed.</p> <p>The four components are: 1. A routing table mapping keywords to reference file paths 2. A decision tree for ambiguous cases with a single escape-hatch question 3. A <code>references/</code> directory containing complete variant guides 4. Shared scaffolding (quality checklist, log format, output naming) in the main SKILL.md</p> <p>In Chapter 9, we examine why this architecture also solves a token budget problem \u2014 and how to measure the token cost of every design decision you make in a skill.</p>"},{"location":"chapters/09/","title":"Chapter 9: Token Efficiency Design","text":"<p>Every skill you write operates within a fixed budget: the context window. That budget is shared among the system prompt, the skill definition, the conversation history, and all content Claude generates. There is no escaping this constraint, only designing around it intelligently.</p> <p>Token efficiency is not about writing short skills. It is about ensuring that every token in your skill definition is doing necessary work \u2014 and that tokens for detail you do not currently need are deferred until you do.</p> <p>This chapter covers the mechanics of token consumption in skills, the tiered retrieval pattern that keeps individual sessions lean, lazy loading, skip-if-complete detection, and how to measure the actual token impact of your design decisions.</p>"},{"location":"chapters/09/#why-token-efficiency-matters","title":"Why Token Efficiency Matters","text":""},{"location":"chapters/09/#the-system-prompt-budget","title":"The System Prompt Budget","text":"<p>When Claude Code loads a skill, the SKILL.md content is injected into the context. This happens before the user has typed a single word. Every token in your SKILL.md is spent unconditionally \u2014 whether or not that content is relevant to the specific invocation.</p> <p>In a typical Claude Code session with a complex project, the context might look like this:</p> Component Approximate Tokens Base system prompt 2,000 - 4,000 Active skill definition (SKILL.md) 500 - 8,000 Project files read during session 2,000 - 40,000+ Conversation history Grows linearly Claude's output so far Grows linearly <p>The context window for Claude Sonnet is 200,000 tokens (context windows vary by model and platform \u2014 Claude Sonnet supports 200K tokens, but other models or deployment environments may differ). That sounds large until you realize a multi-hour deep work session with file reads, iterations, and conversation history can consume 80,000-150,000 tokens. A bloated skill definition is dead weight from the moment the session starts.</p>"},{"location":"chapters/09/#the-cascade-effect","title":"The Cascade Effect","text":"<p>Token waste compounds. A SKILL.md with 8,000 tokens instead of 2,000 does not just cost 6,000 tokens up front. Every time that skill is invoked in a session where Claude maintains context, those tokens remain loaded. In a pipeline session that invokes 8 skills sequentially, an extra 6,000 tokens per skill adds 48,000 tokens of overhead \u2014 enough to noticeably shorten how long the session can run before context fills and the user must compact.</p>"},{"location":"chapters/09/#the-real-cost-in-concrete-numbers","title":"The Real Cost in Concrete Numbers","text":"<p>Here is what different skill design choices actually cost:</p> Design Decision Token Cost Notes Reading skill name from MCP server registry ~0 tokens MCP metadata is external Shell script returning skill description ~67 tokens A brief description string Loading full SKILL.md (lean) ~800 - 1,500 tokens Well-designed, single-variant skill Loading full SKILL.md (average) ~2,000 - 4,000 tokens Typical production skill Loading full SKILL.md (bloated) ~6,000 - 12,000 tokens Skills with embedded examples and verbose prose Loading one reference guide ~500 - 1,200 tokens Via lazy load on demand Loading all reference guides unconditionally ~3,000 - 8,000 tokens The anti-pattern to avoid Full project file (2,000 lines of code) ~6,000 - 15,000 tokens Context of the read, not the skill <p>The difference between a lean skill and a bloated one is roughly equivalent to reading a 500-line source file \u2014 every invocation, whether or not that detail was needed.</p>"},{"location":"chapters/09/#the-tiered-information-retrieval-pattern","title":"The Tiered Information Retrieval Pattern","text":"<p>Tiered retrieval is the organizing principle of token-efficient skill design. Information is classified by when it is needed and retrieved at the appropriate tier \u2014 never earlier.</p>"},{"location":"chapters/09/#tier-0-mcp-server-metadata-0-tokens","title":"Tier 0: MCP Server Metadata (0 tokens)","text":"<p>Note: This tier describes how skill discovery works in Claude Code specifically. Other platforms and environments may discover or surface skills differently \u2014 for example, through tool registries, API manifests, or explicit user configuration. The zero-token-at-discovery principle is a Claude Code implementation detail, not part of the agentskills.io skill spec itself.</p> <p>The MCP (Model Context Protocol) server maintains a registry of installed skills. Claude can query this registry to get a skill's name, description, and invocation syntax without reading the skill file itself.</p> <p>A skill's MCP metadata \u2014 its name, one-line description, and invocation signature \u2014 is retrieved from the server at essentially zero token cost to the context window. This is how skill discovery works in Claude Code: Claude can list available skills and describe them without loading any of them.</p> <p>Design implication: your one-line description in the YAML frontmatter needs to be precise enough that a user can select the right skill from a list. This description lives at Tier 0. It needs to work without any supporting context.</p> <pre><code>description: &gt;\n  Generates four variants of business reports (executive summary,\n  technical spec, sales performance, ops dashboard) with routing\n  and quality scoring.\n</code></pre> <p>This description tells a user everything they need to know to decide whether to invoke this skill. It is also all the information available at Tier 0.</p>"},{"location":"chapters/09/#tier-1-the-skillmd-800-4000-tokens","title":"Tier 1: The SKILL.md (~800 - 4,000 tokens)","text":"<p>When a user invokes the skill, Claude reads the SKILL.md. This is the first real token expenditure. Everything in SKILL.md is loaded unconditionally for every invocation.</p> <p>Principle: SKILL.md should contain only what is needed for every invocation.</p> <p>What belongs at Tier 1: - Purpose and audience - Invocation examples - The routing table (for meta-skills) - Shared workflow steps that run regardless of variant - Shared quality checklist - Output naming conventions - Log format specification - Disambiguation protocol - Pointers to reference guides (not the guides themselves)</p> <p>What does not belong at Tier 1: - Full variant workflows (unless only one variant exists) - Detailed examples - Lengthy background explanations - Reference material that is only relevant sometimes - Anything that is a variant-specific detail</p>"},{"location":"chapters/09/#tier-2-reference-guides-500-1200-tokens-each","title":"Tier 2: Reference Guides (~500 - 1,200 tokens each)","text":"<p>Reference guides are loaded lazily \u2014 only when routing selects them. A meta-skill with four variants loads exactly one reference guide per invocation. The other three guides cost zero tokens.</p> <p>This is the payload of the meta-skill pattern. Without lazy loading, a four-variant skill would need to carry all four workflows in SKILL.md. With lazy loading, you only pay for the workflow you are actually using.</p> <p>Principle: reference guides should be self-contained execution specifications.</p> <p>A reference guide cannot assume context from other reference guides. Once loaded, it must have everything Claude needs to execute the variant. Do not use \"see the executive-summary-guide.md for an example\" in a technical-spec guide \u2014 that would require loading a second guide.</p>"},{"location":"chapters/09/#tier-3-external-files-variable-loaded-on-demand","title":"Tier 3: External Files (variable, loaded on demand)","text":"<p>Some skills need to read actual project files \u2014 source code, data, configuration, prior outputs. These are loaded at the explicit direction of the workflow steps, not automatically.</p> <p>Token-efficient skills defer these reads as long as possible and read only what they need. \"Read the entire codebase\" is an anti-pattern. \"Read <code>src/auth/login.py</code> to check the current error handling pattern\" is a targeted read.</p>"},{"location":"chapters/09/#lazy-loading-in-practice","title":"Lazy Loading in Practice","text":"<p>Lazy loading means: do not read the file until you need what is in it.</p> <p>In skill design, this applies at two levels.</p>"},{"location":"chapters/09/#level-1-reference-guide-lazy-loading","title":"Level 1: Reference Guide Lazy Loading","text":"<p>The meta-skill pattern is the primary example. The SKILL.md contains the routing table but not the reference content. The reference guide is loaded only after routing resolves.</p> <p>In the workflow section of your SKILL.md, the lazy load is expressed as an explicit step:</p> <pre><code>## Execution Protocol\n\n1. Parse the user's request for routing keywords\n2. Resolve routing using the table above\n3. **Read the matched reference guide** (and only that guide)\n4. Execute the variant workflow from the reference guide\n</code></pre> <p>Step 3 is where the lazy load occurs. Until routing resolves, no reference guide is read. After routing resolves, exactly one guide is read. Claude executes this naturally when the workflow is written this way.</p>"},{"location":"chapters/09/#level-2-project-file-lazy-loading","title":"Level 2: Project File Lazy Loading","text":"<p>Within a skill's workflow, reads of external files should be deferred to the step that requires them:</p> <p>Inefficient (eager loading): <pre><code>## Workflow\n\n1. Read all files in the `src/` directory\n2. Read the existing test suite\n3. Read the configuration files\n4. Identify which component needs updating\n...\n</code></pre></p> <p>Efficient (lazy loading): <pre><code>## Workflow\n\n1. Identify which component needs updating from the user's description\n2. Read ONLY the source file for that component: `src/[component-name].py`\n3. Read the existing tests for that component: `tests/test_[component-name].py`\n4. Generate the update\n5. If the update affects configuration, read the relevant config section\n</code></pre></p> <p>The lazy version reads substantially fewer tokens in the common case where only one component is involved. The eager version reads everything regardless.</p>"},{"location":"chapters/09/#skip-if-complete-detection","title":"Skip-If-Complete Detection","text":"<p>Skip-if-complete is a pattern where a skill checks whether its output already exists before running the full workflow. This is especially valuable in pipeline contexts where a skill might be re-invoked after a failure partway through.</p>"},{"location":"chapters/09/#the-pattern","title":"The Pattern","text":"<p>At the beginning of the workflow (before any heavy work), add a detection step:</p> <pre><code>## Pre-Flight Check\n\nBefore executing, check:\n\n1. Does the expected output file already exist?\n   - Check: `./reports/[variant]-[YYYY-MM-DD].md`\n   - If YES and file is &gt;500 words: report completion to user, offer to\n     regenerate or skip\n   - If NO: proceed with full workflow\n\n2. Does a session log from today already exist for this skill?\n   - Check: `./logs/report-generator-v1.2.0-[TODAY'S DATE].md`\n   - If YES: read the log summary only (not the full report) to understand\n     what was completed\n</code></pre> <p>The critical detail: when checking for prior output, read as little as possible. A word count check or header check often tells you everything you need to know without reading the full file. \"Does a file exist at this path and is it substantial?\" requires far fewer tokens than reading the file to determine its completeness.</p>"},{"location":"chapters/09/#when-skip-if-complete-matters-most","title":"When Skip-If-Complete Matters Most","text":"<p>Skip-if-complete pays the largest dividend in:</p> <ol> <li> <p>Batch operations \u2014 when the same skill runs against 12 items and a crash occurred on item 7, you resume from item 8, not item 1</p> </li> <li> <p>Pipeline resumption \u2014 when a 12-step pipeline is re-started after a checkpoint, skills that already completed their step can skip</p> </li> <li> <p>Idempotent workflows \u2014 when a skill might be invoked twice with the same parameters (user reruns without realizing), duplicate work is avoided</p> </li> </ol>"},{"location":"chapters/09/#state-tracking-with-json","title":"State Tracking with JSON","text":"<p>For batch operations and pipeline resumption, a simple JSON state file tracks what has and has not been completed. This is the <code>sim-status.json</code> pattern (covered in depth in Chapter 10), but the token efficiency angle is worth noting here:</p> <p>A small JSON file (200-500 tokens when read) encodes the state of a batch operation that might otherwise require reading 20+ output files to reconstruct. Reading the state file is dramatically cheaper than reconstructing state from the outputs.</p> <pre><code>{\n  \"batch_id\": \"report-batch-2024-11-15\",\n  \"total\": 12,\n  \"completed\": [\"item-01\", \"item-02\", \"item-03\", \"item-04\", \"item-05\"],\n  \"in_progress\": [\"item-06\"],\n  \"failed\": [],\n  \"pending\": [\"item-07\", \"item-08\", \"item-09\", \"item-10\", \"item-11\", \"item-12\"],\n  \"last_updated\": \"2024-11-15T14:23:11Z\"\n}\n</code></pre> <p>Reading this file tells Claude exactly where to resume without reading any of the five completed outputs.</p>"},{"location":"chapters/09/#minimal-context-strategies","title":"Minimal Context Strategies","text":"<p>Beyond tiered retrieval and lazy loading, several structural strategies reduce token consumption.</p>"},{"location":"chapters/09/#strategy-1-tables-over-prose","title":"Strategy 1: Tables Over Prose","text":"<p>Tables convey structured information at roughly half the token cost of equivalent prose. A routing table is not just visually cleaner than a paragraph describing routing rules \u2014 it is substantially cheaper to load.</p> <p>Prose version (~60 tokens):</p> <p>If the user mentions the word \"executive\" or \"summary\" or \"leadership\" in their request, route to the executive summary variant. If they mention \"technical\" or \"spec\" or \"architecture\" or \"system,\" route to the technical specification variant.</p> <p>Table version (~25 tokens): <pre><code>| executive, summary, leadership | \u2192 executive-summary-guide.md |\n| technical, spec, architecture  | \u2192 technical-spec-guide.md    |\n</code></pre></p> <p>Apply this consistently across your SKILL.md. Checklists, routing rules, output structures, quality criteria \u2014 all of these are more token-efficient as structured lists or tables than as narrative prose.</p>"},{"location":"chapters/09/#strategy-2-code-blocks-for-structured-output","title":"Strategy 2: Code Blocks for Structured Output","text":"<p>When specifying output structure, a code block template costs fewer tokens than describing the same structure in prose, and is less ambiguous:</p> <p>Prose (~80 tokens):</p> <p>The output should begin with a title block containing the report title, the date it was generated, the author name, and the period the report covers. Below that should be an executive summary section...</p> <p>Template (~40 tokens): <pre><code># [Report Title]\n**Generated:** [DATE] | **Author:** [NAME] | **Period:** [PERIOD]\n\n## Executive Summary\n[2-3 sentence summary]\n</code></pre></p>"},{"location":"chapters/09/#strategy-3-pointer-not-payload","title":"Strategy 3: Pointer, Not Payload","text":"<p>In SKILL.md, reference information that lives elsewhere with a path, not the content itself:</p> <p>Payload (high tokens, always loaded): <pre><code>## ChartJS Configuration Reference\n\nTo create a bar chart, use the following configuration:\n[200 lines of configuration examples]\n</code></pre></p> <p>Pointer (low tokens, loads on demand): <pre><code>## Chart Reference\nFor chart configuration details, read: references/chartjs-guide.md\n</code></pre></p> <p>The pointer loads with SKILL.md (minimal cost). The chartjs-guide.md only loads when the workflow actually reaches a charting step.</p>"},{"location":"chapters/09/#strategy-4-compress-examples","title":"Strategy 4: Compress Examples","text":"<p>Examples are among the highest-value content in a skill definition and also among the most token-expensive. The balance point is compressed examples \u2014 enough structure to be useful, stripped of all non-essential words.</p> <p>Full example (~150 tokens):</p> <p>Here is an example of a well-formed executive summary. Notice how the first line immediately states the business outcome. The key findings section uses specific numbers rather than qualitative descriptors. The recommended actions section uses strong action verbs and includes explicit owners and deadlines.</p> <p>Q3 Revenue Performance \u2014 Executive Summary Q3 closed at $4.2M, 8% above plan, driven by enterprise segment outperformance...</p> <p>Compressed example (~60 tokens): <pre><code># Q3 Revenue Performance \u2014 Executive Summary\nClosed $4.2M, 8% above plan (enterprise outperformed, SMB -12%).\n\n**Key Findings:** [3 bulleted numbers] | **Actions:** [2 owner+deadline pairs]\n</code></pre></p> <p>The compressed version demonstrates the pattern without wasting tokens on narration.</p>"},{"location":"chapters/09/#measuring-token-impact","title":"Measuring Token Impact","text":"<p>Before shipping a skill, audit it. Here is a practical measurement process.</p>"},{"location":"chapters/09/#step-1-estimate-skillmd-token-cost","title":"Step 1: Estimate SKILL.md Token Cost","text":"<p>Copy the raw text of your SKILL.md and paste it into a token counting tool (many are available online for the tokenizers used by major models). Alternatively, estimate at approximately 750 words per 1,000 tokens for English prose, or 450-600 words per 1,000 tokens for structured markdown with code blocks (which tokenize less efficiently).</p> <p>Target benchmarks: - Single-variant skill: 600 - 1,500 tokens - Meta-skill SKILL.md (routing only): 1,000 - 2,500 tokens - Each reference guide: 400 - 1,000 tokens</p> <p>If your SKILL.md exceeds 4,000 tokens, audit it aggressively. Something belongs in a reference guide or can be compressed.</p>"},{"location":"chapters/09/#step-2-identify-the-most-expensive-sections","title":"Step 2: Identify the Most Expensive Sections","text":"<p>Within the SKILL.md, identify the three heaviest sections by word count. For each:</p> <ul> <li>Is this section needed for every invocation? If no \u2192 move to a reference guide</li> <li>Is it in prose when it could be a table? If yes \u2192 convert</li> <li>Does it include examples? If yes \u2192 compress the examples</li> </ul>"},{"location":"chapters/09/#step-3-calculate-worst-case-session-budget","title":"Step 3: Calculate Worst-Case Session Budget","text":"<p>For any skill that is part of a larger pipeline, calculate the worst-case token consumption for the full pipeline session:</p> <pre><code>Base system prompt:          3,000 tokens (estimate)\nAll skill SKILL.md files:    N \u00d7 avg_skill_tokens\nReference guides loaded:     M \u00d7 avg_reference_tokens\nProject files read:          Sum of files read\nConversation history:        grows_linearly\nClaude output:               grows_linearly\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTotal at completion:         [sum]\n</code></pre> <p>If the estimated total approaches 150,000 tokens, the session will likely hit context limits before completing. Either compact the skill definitions or add compaction checkpoints to the pipeline (see Chapter 11).</p>"},{"location":"chapters/09/#step-4-measure-reference-guide-savings","title":"Step 4: Measure Reference Guide Savings","text":"<p>For each meta-skill, calculate the savings from lazy loading:</p> <pre><code>Total reference content:    4 guides \u00d7 800 tokens = 3,200 tokens\nLazy-load cost per invoke:  1 guide \u00d7 800 tokens = 800 tokens\nSavings per invocation:     2,400 tokens (75%)\n</code></pre> <p>Over 10 invocations in a session, that 2,400-token saving per invocation represents 24,000 tokens of freed context \u2014 enough to read several more project files or extend the conversation substantially.</p>"},{"location":"chapters/09/#the-cost-of-loading-the-full-skillmd","title":"The Cost of Loading the Full SKILL.md","text":"<p>There is one more token efficiency consideration that is easy to overlook: the difference between what the user sees when discovering a skill versus what Claude loads when executing it.</p> <p>At the discovery stage (Tier 0), Claude sees only the description metadata \u2014 about 30-50 tokens. At the invocation stage (Tier 1), Claude reads the full SKILL.md \u2014 800 to 4,000 tokens. This is a 20-80x token multiplication that happens the moment the user types the <code>/skill-name</code> command.</p> <p>This means every token in your SKILL.md must justify itself against the question: \"Is this worth paying for on every single invocation of this skill, including invocations where this content turns out to be irrelevant?\"</p> <p>If the answer is no \u2014 if there is a realistic invocation scenario where this content is not needed \u2014 it belongs in a reference guide, not in SKILL.md.</p>"},{"location":"chapters/09/#a-practical-audit-checklist","title":"A Practical Audit Checklist","text":"<p>Run this checklist on every skill before deploying it:</p> <ul> <li> SKILL.md is under 3,000 tokens (use a token counter to verify)</li> <li> All variant-specific detail is in reference guides, not in SKILL.md</li> <li> Routing tables use tables, not prose</li> <li> Quality criteria use bullet lists, not narrative</li> <li> Examples are compressed to their minimum useful form</li> <li> Output structure is shown as a template, not described in prose</li> <li> Pre-flight check is present and reads as little as possible</li> <li> Reference guide paths in the routing table are exact (no typos)</li> <li> Each reference guide is self-contained (requires no cross-guide reads)</li> </ul>"},{"location":"chapters/09/#summary","title":"Summary","text":"<p>Token efficiency is an engineering discipline, not an afterthought. The key principles:</p> <p>Tiered retrieval classifies information into tiers by when it is needed: discovery metadata (0 tokens, surfaced via MCP in Claude Code or equivalent mechanisms on other platforms), SKILL.md (loaded on invoke), reference guides (loaded on demand), project files (loaded when needed by the workflow). Note that MCP is a separate protocol that complements the skill system \u2014 it handles discovery and invocation routing in Claude Code, but is not part of the agentskills.io skill spec itself.</p> <p>Lazy loading defers reference guide reads until routing resolves, and defers project file reads until the specific workflow step that requires them.</p> <p>Skip-if-complete checks for prior output before running the full workflow, enabling safe resumption in batch and pipeline contexts.</p> <p>Structural compression uses tables, templates, and code blocks instead of prose wherever the content is inherently structured.</p> <p>Measurement involves actually counting tokens, auditing the heaviest sections, and calculating worst-case session budgets for pipeline skills.</p> <p>The payoff compounds. A well-optimized skill that saves 2,000 tokens per invocation enables longer sessions, larger batch operations, and more complex pipelines \u2014 without requiring any changes to the user's workflow.</p> <p>Chapter 10 extends this efficiency conversation into the temporal domain: how session logs enable complex work to survive context window limits and resume cleanly across multiple sessions.</p>"},{"location":"chapters/10/","title":"Chapter 10: Session Logging","text":"<p>Complex skills do complex work. A skill that generates a 12-chapter textbook, processes a batch of 20 reports, or orchestrates multiple downstream tools can run for an hour before completing. During that time, the context window fills incrementally with conversation history, file reads, and generated output. Eventually, the session must be compacted or a new session started \u2014 and the work must continue.</p> <p>Session logging is the infrastructure that makes continuation possible. It is also the infrastructure that makes debugging possible, quality measurement possible, and batch resumption possible.</p> <p>This chapter covers why session logs exist, what they must contain, the conventions used across the skill ecosystem, state tracking for batch operations, and how to build skills that detect and resume from prior session state.</p>"},{"location":"chapters/10/#why-session-logging-exists","title":"Why Session Logging Exists","text":""},{"location":"chapters/10/#the-context-window-fills","title":"The Context Window Fills","text":"<p>Every token generated \u2014 by Claude and by the user \u2014 accumulates in the context window. In a long session involving deep file reads, iterative revisions, and substantial output generation, the window fills faster than you might expect.</p> <p>When context fills, there are two options:</p> <ol> <li>Compact the session \u2014 Claude Code summarizes the conversation, freeing space but losing detail</li> <li>Start a new session \u2014 the fresh session has no memory of what was accomplished</li> </ol> <p>Without session logs, both options destroy continuity. After compaction, Claude may not remember that chapters 1-7 are complete and chapter 8 is in progress. After a new session, Claude has no information at all.</p> <p>With a session log, the first step in any continuation session is a single, cheap read: load the log, understand the state, resume from exactly where work stopped. The log is the external memory that survives context resets.</p>"},{"location":"chapters/10/#batch-operations-fail-partway-through","title":"Batch Operations Fail Partway Through","text":"<p>When a skill processes 20 items in a batch, statistical reality guarantees occasional failures \u2014 a malformed input, an unexpected file format, a tool call that times out. Without state tracking, the only recovery option is to re-run the entire batch and deduplicate the results.</p> <p>With a session log and a state file, recovery is precise: identify the failed item, fix the input, and resume from that item. Items 1-19 that completed successfully are not touched.</p>"},{"location":"chapters/10/#quality-is-invisible-without-measurement","title":"Quality is Invisible Without Measurement","text":"<p>If a skill produces output without logging anything, you cannot answer the question \"are our outputs getting better or worse over time?\" You cannot identify which variants have lower quality scores. You cannot correlate output quality with session duration or input complexity.</p> <p>Session logs create the data that makes quality measurement possible. Every logged session contributes to a corpus that answers questions like: \"The technical-spec variant has an average quality score of 7.2, while the executive-summary variant scores 8.8 \u2014 what accounts for the gap?\"</p>"},{"location":"chapters/10/#the-logs-directory-convention","title":"The Logs Directory Convention","text":"<p>Every skill that produces substantial output should write logs to a <code>logs/</code> directory relative to the working directory. The convention used across the skill ecosystem is:</p> <pre><code>./logs/[skill-name]-v[VERSION]-[YYYY-MM-DD].md\n</code></pre> <p>Examples: <pre><code>./logs/report-generator-v1.2.0-2024-11-15.md\n./logs/microsim-generator-v2.0.1-2024-11-15.md\n./logs/chapter-writer-v1.0.0-2024-11-14.md\n</code></pre></p>"},{"location":"chapters/10/#why-markdown-not-json","title":"Why Markdown, Not JSON?","text":"<p>Logs use <code>.md</code> format (not <code>.json</code>) for two reasons:</p> <ol> <li>Human readability \u2014 a developer can open a log and understand what happened without a parser</li> <li>Claude readability \u2014 when Claude reads a session log to resume work, markdown is parsed naturally without any schema overhead</li> </ol> <p>State files (covered later) use <code>.json</code> because they are read programmatically and precision matters more than human readability.</p>"},{"location":"chapters/10/#one-log-per-skill-per-day","title":"One Log Per Skill Per Day","text":"<p>The convention is one log file per skill per calendar day. If a skill is invoked three times in a day, all three invocations append to the same log file under separate <code>## Session [N]</code> headers. This keeps the log corpus from proliferating into dozens of tiny files.</p> <p>The exception is batch operations, where each batch run gets its own log file identified by batch ID.</p>"},{"location":"chapters/10/#log-format-reference","title":"Log Format Reference","text":"<p>A complete session log entry has seven sections. Not every section applies to every skill \u2014 a simple single-purpose skill might omit timing details \u2014 but more complex skills should populate all seven.</p> <pre><code># [Skill Name] Session Log\n**Version:** [x.y.z] | **Date:** [YYYY-MM-DD] | **Session:** [N]\n\n## Invocation\n\n**Trigger:** [How the skill was invoked \u2014 full command or description]\n**Variant:** [Which variant was selected, if meta-skill]\n**Routing Confidence:** [High / Medium / Low]\n\n## Timing\n\n| Phase | Start | End | Duration |\n|-------|-------|-----|----------|\n| Routing resolution | 14:03:22 | 14:03:24 | 2s |\n| Reference guide load | 14:03:24 | 14:03:25 | 1s |\n| Draft generation | 14:03:25 | 14:11:40 | 8m 15s |\n| Quality scoring | 14:11:40 | 14:11:55 | 15s |\n| Log write | 14:11:55 | 14:12:00 | 5s |\n| **Total** | | | **8m 38s** |\n\n## Files Created\n\n| File | Size | Status |\n|------|------|--------|\n| `./reports/executive-summary-2024-11-15.md` | 612 words | Complete |\n| `./reports/executive-summary-2024-11-15-appendix.md` | 203 words | Complete |\n\n## Token Usage Estimate\n\n| Phase | Estimated Tokens |\n|-------|------------------|\n| Skill load (SKILL.md) | 1,840 |\n| Reference guide load | 720 |\n| User input | 145 |\n| Files read | 0 |\n| Claude output | 3,200 |\n| **Session total** | **5,905** |\n\n## Quality Score\n\n**Overall Score:** 8.2 / 10\n\n| Criterion | Score | Note |\n|-----------|-------|------|\n| Findings are quantified | 9/10 | All 4 findings have numbers |\n| Recommendations have owners | 7/10 | 2/3 have explicit owners |\n| Register appropriate for audience | 9/10 | Clean executive language |\n| Length within target | 8/10 | 612 words, target 400-600 |\n\n## Decisions Made\n\n- Routed to executive-summary variant (confidence: high \u2014 \"board\" keyword present)\n- Excluded raw financials from body; referenced as appendix per audience guidance\n- Used Q3 vs Q2 comparison frame rather than Q3 vs plan (data provided supported both)\n\n## Resume State\n\n**Status:** Complete\n**Next Step:** N/A \u2014 standalone invocation\n**Outstanding:** None\n</code></pre> <p>Each section serves a purpose:</p> <ul> <li>Invocation \u2014 lets future sessions understand exactly what was requested</li> <li>Timing \u2014 identifies bottlenecks and tracks performance over time</li> <li>Files Created \u2014 tells a resume session exactly what exists and is ready</li> <li>Token Usage Estimate \u2014 informs optimization decisions</li> <li>Quality Score \u2014 creates the measurement corpus</li> <li>Decisions Made \u2014 documents judgment calls that might need to be reversed or referenced</li> <li>Resume State \u2014 the most critical section for continuation sessions</li> </ul>"},{"location":"chapters/10/#what-to-log-a-decision-framework","title":"What to Log: A Decision Framework","text":"<p>Not every detail belongs in a log. Over-logging wastes tokens when Claude reads the log to resume. Under-logging defeats the purpose. Apply this decision framework:</p> <p>Always log: - Which variant was selected and at what confidence - What files were created (name, size, completion status) - Overall quality score - Resume state (complete / in-progress / blocked) - Any decision that involved a judgment call rather than a deterministic rule</p> <p>Log if the skill runs for more than 5 minutes: - Timing data for each phase - Token usage estimates</p> <p>Log if the skill is part of a batch or pipeline: - Item ID or step number - Whether this item/step is unblocked for the next stage - Dependencies satisfied</p> <p>Do not log: - The full text of generated outputs (they are in the output files) - Intermediate draft revisions - Standard workflow steps that completed normally (only log deviations) - Anything that is recoverable from the output files themselves</p> <p>The goal of a log is to reconstruct session state, not to duplicate output. If Claude can derive the information by reading the output file, it should not be duplicated in the log.</p>"},{"location":"chapters/10/#state-tracking-with-json-files","title":"State Tracking with JSON Files","text":"<p>For batch operations and pipeline orchestration, a small JSON state file provides precise status tracking at minimal token cost.</p>"},{"location":"chapters/10/#the-sim-statusjson-pattern","title":"The sim-status.json Pattern","text":"<p>The name \"sim-status.json\" comes from the MicroSim Generator skill ecosystem, but the pattern applies to any batch skill. The file tracks the lifecycle state of each item in a batch:</p> <pre><code>{\n  \"batch_id\": \"textbook-ch-batch-2024-11-15\",\n  \"skill\": \"chapter-writer\",\n  \"version\": \"1.0.0\",\n  \"created\": \"2024-11-15T10:00:00Z\",\n  \"last_updated\": \"2024-11-15T14:23:11Z\",\n  \"total_items\": 12,\n  \"summary\": {\n    \"complete\": 7,\n    \"in_progress\": 1,\n    \"pending\": 4,\n    \"failed\": 0\n  },\n  \"items\": {\n    \"ch-01\": {\n      \"status\": \"complete\",\n      \"output_file\": \"./docs/chapters/01/index.md\",\n      \"quality_score\": 8.4,\n      \"word_count\": 3241,\n      \"completed_at\": \"2024-11-15T10:47:33Z\"\n    },\n    \"ch-02\": {\n      \"status\": \"complete\",\n      \"output_file\": \"./docs/chapters/02/index.md\",\n      \"quality_score\": 7.9,\n      \"word_count\": 2987,\n      \"completed_at\": \"2024-11-15T11:31:05Z\"\n    },\n    \"ch-08\": {\n      \"status\": \"in_progress\",\n      \"output_file\": \"./docs/chapters/08/index.md\",\n      \"started_at\": \"2024-11-15T14:18:00Z\",\n      \"progress_note\": \"Draft complete, quality scoring in progress\"\n    },\n    \"ch-09\": {\n      \"status\": \"pending\",\n      \"output_file\": null,\n      \"depends_on\": []\n    },\n    \"ch-10\": {\n      \"status\": \"failed\",\n      \"output_file\": null,\n      \"error\": \"Input data missing: no section outline provided\",\n      \"failed_at\": \"2024-11-15T13:45:22Z\",\n      \"retry_ready\": false\n    }\n  }\n}\n</code></pre>"},{"location":"chapters/10/#lifecycle-states","title":"Lifecycle States","text":"<p>Items in a batch state file move through defined lifecycle states:</p> <pre><code>pending \u2192 in_progress \u2192 complete\n                     \u2198 failed \u2192 (retry) \u2192 in_progress \u2192 complete\n</code></pre> State Meaning Resume Action <code>pending</code> Not yet started Start next available <code>in_progress</code> Started but not confirmed complete Verify output file exists; mark complete or restart <code>complete</code> Output file exists and passed quality check Skip <code>failed</code> Encountered an error Check <code>retry_ready</code> flag before re-queueing <p>The <code>in_progress</code> state requires special handling on resume. When a session ends unexpectedly (crash, timeout, context limit), items marked <code>in_progress</code> may or may not have produced usable output. The resume protocol for <code>in_progress</code> items:</p> <ol> <li>Check if the output file exists</li> <li>If yes: read the first 200 words and the final 200 words to assess completeness</li> <li>If the file appears complete: run quality scoring and mark as <code>complete</code></li> <li>If the file appears truncated: mark as <code>failed</code> and add to the retry queue</li> <li>If no file: mark as <code>pending</code> and add to the next run queue</li> </ol>"},{"location":"chapters/10/#writing-skills-that-resume-from-prior-state","title":"Writing Skills That Resume from Prior State","text":"<p>A skill that supports resumption has a different first step than one that does not. Instead of immediately beginning work, it begins by understanding what has already been done.</p>"},{"location":"chapters/10/#the-resume-detection-block","title":"The Resume Detection Block","text":"<p>Add this block at the start of your workflow section, before any execution steps:</p> <pre><code>## Pre-Flight: Resume Detection\n\nBefore any work begins:\n\n1. Check for a state file at `./[skill-name]-status.json`\n   - If found: read it (it is small, ~500 tokens)\n   - If not found: this is a fresh run; initialize the state file\n\n2. If state file found:\n   - Report current batch status to user: N complete, M in-progress, K pending\n   - Ask: \"Resume from in-progress items, or start a fresh run?\"\n   - If resume: skip all `complete` items, re-validate `in_progress` items,\n     queue `pending` items\n   - If fresh: rename old state file with timestamp, initialize new state file\n\n3. For each `in_progress` item found on resume:\n   - Check if output file exists at the logged path\n   - If yes and &gt;500 words: quality-score and mark complete\n   - If yes and &lt;500 words: mark failed, add to queue\n   - If no: mark pending, add to queue\n</code></pre> <p>This block costs the user nothing \u2014 it is fast, requires reading only one small file, and almost always produces the right result automatically. The only user prompt is the fresh-vs-resume decision, which cannot be automated because it depends on user intent.</p>"},{"location":"chapters/10/#writing-the-state-file-incrementally","title":"Writing the State File Incrementally","text":"<p>The state file must be updated after each item completes, not at the end of the batch. If the session ends after completing item 7 of 12 but before the final write, and the state file only reflects items 1-6, item 7's work is lost.</p> <p>The incremental update pattern in a skill's workflow:</p> <pre><code>## Batch Execution Protocol\n\nFor each item in the queue:\n\n1. Mark item as `in_progress` in the state file (write immediately)\n2. Execute the workflow for this item\n3. Write the output file\n4. Run quality scoring\n5. Mark item as `complete` in the state file with quality score (write immediately)\n6. Proceed to next item\n\nIf any step fails:\n- Mark item as `failed` in the state file with error note (write immediately)\n- Log the failure\n- Continue to next item (do not halt the batch)\n</code></pre> <p>The two write-immediately instructions are critical. They ensure the state file accurately reflects current reality at every point in execution. A state file that is updated in batches is unreliable during resumption.</p>"},{"location":"chapters/10/#log-continuity-across-sessions","title":"Log Continuity Across Sessions","text":"<p>When a new session reads a log to resume work, it should be able to reconstruct everything it needs from three sources:</p> <ol> <li>The state file \u2014 what is complete, in-progress, pending, failed</li> <li>The log file \u2014 what decisions were made, what quality scores were assigned, what is outstanding</li> <li>The output files \u2014 the actual work product</li> </ol> <p>The resume session should never need to re-read files that are already marked complete in the state file. It should never need to re-derive decisions that are documented in the log. It should never need to re-run quality scoring on outputs that already have scores in the log.</p> <p>This is the efficiency principle of session logging: logs exist to eliminate redundant work on resume, not to provide a narrative history.</p>"},{"location":"chapters/10/#the-resume-session-opening-protocol","title":"The Resume Session Opening Protocol","text":"<p>Build this into skills that support multi-session work:</p> <pre><code>## Session Opening (Resume Mode)\n\n1. Read state file (one read, ~500 tokens)\n2. Read the most recent log entry (one read, ~1,000 tokens)\n3. Report to user:\n   - Items complete: [N]\n   - Items in-progress: [M] (will be re-validated)\n   - Items pending: [K]\n   - Estimated session work remaining: [time estimate]\n   - Quality scores so far: [average and range]\n4. Re-validate any in-progress items\n5. Begin next pending item\n</code></pre> <p>Total token cost of resuming: ~1,500 tokens for the state + log read, plus the re-validation work on in-progress items. This is dramatically cheaper than re-reading all output files to reconstruct state, which might cost 10,000-50,000 tokens for a large batch.</p>"},{"location":"chapters/10/#quality-scoring-in-logs","title":"Quality Scoring in Logs","text":"<p>Quality scores are a first-class element of session logs. Every skill that produces substantial output should include a quality score in its log, derived from the skill's quality checklist.</p>"},{"location":"chapters/10/#scoring-mechanics","title":"Scoring Mechanics","text":"<p>Quality scores are calculated from a rubric defined in the SKILL.md or reference guide. Each criterion is scored on a 0-10 scale, then averaged (or weighted if some criteria are more important than others).</p> <p>Example rubric from the executive summary variant:</p> <pre><code>## Quality Scoring\n\nScore each criterion 0-10:\n\n| Criterion | Weight | How to Score |\n|-----------|--------|--------------|\n| Findings are quantified | 1.0x | 10 = all findings have numbers; 5 = half do; 0 = no numbers |\n| Recommendations have owners | 0.8x | 10 = all have owners; 5 = some have owners; 0 = none do |\n| Register is appropriate | 1.0x | 10 = clean executive language throughout; 5 = some jargon or passive voice |\n| Length within target | 0.6x | 10 = within target; 7 = 10% over; 3 = 50% over; 0 = 2x over |\n| One-sentence summary present | 1.0x | 10 = present and strong; 5 = present but weak; 0 = missing |\n\nOverall score = weighted average of all criteria \u00d7 10\n</code></pre> <p>The log records both the overall score and the individual criterion scores. This enables diagnosis: if a skill consistently underperforms on one criterion, that is a signal that the workflow step addressing that criterion needs improvement.</p>"},{"location":"chapters/10/#using-scores-for-skill-improvement","title":"Using Scores for Skill Improvement","text":"<p>Aggregate quality scores across log files to identify systemic issues:</p> <ul> <li>If the average score for a variant is below 7.0, the reference guide workflow needs revision</li> <li>If one criterion is consistently low, the instruction for that criterion is unclear</li> <li>If scores dropped after a version update, the update regressed something</li> </ul> <p>This feedback loop \u2014 log quality scores, aggregate across sessions, identify patterns, revise the skill \u2014 is how skills improve over time. Without logs, this loop does not exist.</p>"},{"location":"chapters/10/#implementing-logging-in-a-new-skill","title":"Implementing Logging in a New Skill","text":"<p>When you build a new skill, add logging from the start. Retrofitting logging into an existing skill is harder than building it in.</p>"},{"location":"chapters/10/#minimum-viable-log-entry","title":"Minimum Viable Log Entry","text":"<p>For a simple single-variant skill, the minimum useful log entry is:</p> <pre><code>## [Skill Name] \u2014 [YYYY-MM-DD]\n\n**Invocation:** [command / description]\n**Output:** [file path] ([word count] words)\n**Quality Score:** [X.X / 10]\n**Status:** Complete\n</code></pre> <p>That is 4 lines and approximately 60 tokens to write. A resume session can read it in under 50 tokens. This minimum-viable format is enough to answer the question \"has this skill run on this input before?\" at trivial cost.</p> <p>Add more structure as the skill grows in complexity. Do not start with a fully instrumented log if the skill is simple \u2014 let the log format evolve with the skill's complexity.</p>"},{"location":"chapters/10/#log-directory-setup-step","title":"Log Directory Setup Step","text":"<p>Add a log setup step as the first step in every skill's execution protocol:</p> <pre><code>## Execution Protocol\n\n**Step 0: Initialize logging**\n- Ensure `./logs/` directory exists\n- Open log file: `./logs/[skill-name]-v[VERSION]-[TODAY].md`\n- Write the invocation header immediately (before any other work)\n</code></pre> <p>Writing the invocation header before any work begins ensures that even if the skill crashes in the first step, there is evidence in the log that an attempt was made. This makes debugging significantly easier.</p>"},{"location":"chapters/10/#anti-patterns","title":"Anti-Patterns","text":"<p>Do not log full output text. Logs should reference output files, not duplicate them. A log entry that includes the full generated report is wasteful \u2014 reading the log to resume work would cost as much as reading the report directly.</p> <p>Do not write state files only at the end of a batch. If the session ends before the final write, the state file is wrong. Write after every item.</p> <p>Do not use logs as the primary recovery mechanism. Logs support recovery; output files are the primary artifact. A log that says \"complete\" for an item whose output file is missing is in an inconsistent state. The resume protocol must check both.</p> <p>Do not skip logging for \"simple\" skills. Simple skills become complex over time. A skill that starts as a single-variant, single-output tool often grows into a multi-variant pipeline component. Starting with no logging makes that transition harder. Even a minimum-viable log entry is better than none.</p>"},{"location":"chapters/10/#summary","title":"Summary","text":"<p>Session logging exists because complex skills do non-trivial work that cannot be reliably resumed without external memory. The context window fills, sessions end, batches fail partway \u2014 and the infrastructure for continuing is the log.</p> <p>The key components of a robust logging implementation:</p> <p>The logs/ directory holds human- and Claude-readable <code>.md</code> files, one per skill per day, with multiple sessions appending to the same file under separate headers.</p> <p>The log format captures invocation, timing, files created, token estimates, quality scores, decisions made, and resume state.</p> <p>The state file (JSON) tracks batch item lifecycle states with precision, updated after every item, readable in ~500 tokens.</p> <p>Resume detection is the first step in any multi-session skill \u2014 read the state file, report current status, re-validate in-progress items, queue next pending items.</p> <p>Quality scores in logs create the measurement corpus that enables systematic skill improvement over time.</p> <p>Chapter 11 extends these concepts into the pipeline context \u2014 how multiple skills chain together in dependency order, how checkpoints work, and how a 12-step orchestration manages the continuity of an entire textbook production pipeline.</p>"},{"location":"chapters/11/","title":"Chapter 11: Pipeline Orchestration","text":"<p>A single skill solves a single problem well. But some problems are not single problems. They are sequences of interdependent problems where the solution to each step becomes the raw material for the next.</p> <p>Building an intelligent textbook \u2014 a production-grade, multi-chapter, multi-MicroSim learning resource \u2014 is not something a single skill can handle. It requires a concept map, a chapter outline, chapter content, MicroSimulations, quality validation, a navigation structure, and a deployment configuration. Each of these depends on earlier outputs. None of them can be parallelized arbitrarily.</p> <p>This is the problem pipeline orchestration solves.</p> <p>A pipeline is an ordered sequence of skill invocations where each skill's output feeds the next skill's input, dependencies are enforced, checkpoints allow user review at critical junctures, and the whole sequence can be paused and resumed without loss.</p> <p>This chapter explains how pipelines work, covers the 12-step intelligent textbook pipeline as the canonical example, explains checkpoint design and prerequisite detection, and shows you how to design your own multi-skill pipeline.</p>"},{"location":"chapters/11/#the-core-concept-skills-as-pipeline-stages","title":"The Core Concept: Skills as Pipeline Stages","text":"<p>In a pipeline, each skill is a stage. A stage takes inputs, produces outputs, and signals completion so the next stage can begin.</p> <p>The properties that make a skill pipeline-ready:</p> <ol> <li>Defined inputs \u2014 the skill specifies exactly what it needs (files, parameters, prior outputs)</li> <li>Defined outputs \u2014 the skill produces artifacts with predictable names and locations</li> <li>Prerequisite detection \u2014 the skill checks that its required inputs exist before starting</li> <li>Completion signaling \u2014 the skill writes a clear completion marker (log entry, state file update, or output file) that the next stage can check</li> <li>Idempotency \u2014 running the skill twice with the same inputs produces the same outputs (enabling safe retries)</li> </ol> <p>Skills that lack any of these properties can still be used in pipelines, but they introduce fragility. A skill that does not check its prerequisites may run and fail silently. A skill that does not signal completion may cause the next stage to start on incomplete data.</p>"},{"location":"chapters/11/#dependency-order-and-the-dependency-graph","title":"Dependency Order and the Dependency Graph","text":"<p>Pipelines are defined by their dependency graph \u2014 a directed acyclic graph (DAG) where each node is a stage and each edge means \"this stage must complete before that stage can begin.\"</p> <p>A simple three-stage pipeline has a linear graph:</p> <pre><code>Stage A \u2192 Stage B \u2192 Stage C\n</code></pre> <p>Real pipelines have branching dependencies. Some stages can run in parallel once their shared predecessor completes. Some stages have multiple predecessors that must all complete before they can begin.</p> <pre><code>Stage A \u2192 Stage B \u2192 Stage D \u2192 Stage F\n       \u2198           \u2197\n        Stage C \u2192\n</code></pre> <p>In this graph, Stage B and Stage C can run in parallel once Stage A completes. Stage D cannot begin until both Stage B and Stage C are complete. Stage F runs last.</p> <p>When designing a pipeline, drawing the dependency graph before writing any skill definitions is the most valuable step you can take. It reveals:</p> <ul> <li>Which stages can be parallelized (accelerating the pipeline)</li> <li>Which stages are critical path (where delays cascade)</li> <li>Which stages have multiple dependencies (where coordination is needed)</li> <li>Whether any circular dependencies exist (which would make the pipeline impossible to execute)</li> </ul>"},{"location":"chapters/11/#the-12-step-intelligent-textbook-pipeline","title":"The 12-Step Intelligent Textbook Pipeline","text":"<p>The intelligent textbook pipeline is the most fully-developed pipeline in the Claude Code skill ecosystem. It takes a topic and produces a complete, deployed MkDocs Material textbook \u2014 chapter content, MicroSimulations, navigation, quality validation, and GitHub Pages deployment.</p> <p>Here is the complete pipeline as a Mermaid diagram:</p> <pre><code>flowchart TD\n    S1[\"Step 1\\nTopic Analysis\\nconcept-map-generator\"] --&gt; S2[\"Step 2\\nChapter Outline\\nchapter-planner\"]\n    S2 --&gt; CHKPT1{{\"Checkpoint 1\\nUser reviews\\noutline\"}}\n    CHKPT1 --&gt; S3[\"Step 3\\nMkDocs Scaffold\\nmkdocs-scaffolder\"]\n    S3 --&gt; S4[\"Step 4\\nConcept Definitions\\nconcept-definer\"]\n    S4 --&gt; CHKPT2{{\"Checkpoint 2\\nUser reviews\\nconcepts\"}}\n    CHKPT2 --&gt; S5[\"Step 5\\nChapter Content\\nchapter-writer\"]\n    S5 --&gt; CHKPT3{{\"Checkpoint 3\\nUser reviews\\nch. 1 draft\"}}\n    CHKPT3 --&gt; S6[\"Step 6\\nMicroSim Specs\\nmicrosim-planner\"]\n    S6 --&gt; S7[\"Step 7\\nMicroSim Generation\\nmicrosim-generator\"]\n    S7 --&gt; S8[\"Step 8\\nGlossary Build\\nglossary-builder\"]\n    S5 --&gt; S8\n    S8 --&gt; S9[\"Step 9\\nNavigation Config\\nnav-configurator\"]\n    S7 --&gt; S9\n    S9 --&gt; S10[\"Step 10\\nQuality Validation\\nquality-validator\"]\n    S10 --&gt; CHKPT4{{\"Checkpoint 4\\nUser reviews\\nquality report\"}}\n    CHKPT4 --&gt; S11[\"Step 11\\nCross-Reference Check\\ncross-ref-checker\"]\n    S11 --&gt; CHKPT5{{\"Checkpoint 5\\nFinal approval\\nbefore deploy\"}}\n    CHKPT5 --&gt; S12[\"Step 12\\nDeploy to GitHub Pages\\ngh-deployer\"]\n\n    style CHKPT1 fill:#f59e0b,color:#000\n    style CHKPT2 fill:#f59e0b,color:#000\n    style CHKPT3 fill:#f59e0b,color:#000\n    style CHKPT4 fill:#f59e0b,color:#000\n    style CHKPT5 fill:#f59e0b,color:#000\n    style S12 fill:#10b981,color:#fff</code></pre>"},{"location":"chapters/11/#pipeline-summary","title":"Pipeline Summary","text":"Step Skill Input Output Checkpoint After? 1 <code>concept-map-generator</code> Topic string <code>concept-map.md</code>, <code>concepts.json</code> No 2 <code>chapter-planner</code> <code>concept-map.md</code> <code>chapter-outline.md</code> Yes \u2014 outline review 3 <code>mkdocs-scaffolder</code> <code>chapter-outline.md</code> Full directory tree, <code>mkdocs.yml</code> No 4 <code>concept-definer</code> <code>concepts.json</code> <code>definitions.md</code>, <code>glossary-draft.md</code> Yes \u2014 concept review 5 <code>chapter-writer</code> <code>chapter-outline.md</code>, <code>definitions.md</code> <code>docs/chapters/*/index.md</code> (all) Yes \u2014 ch.1 sample review 6 <code>microsim-planner</code> <code>chapter-outline.md</code>, chapters <code>microsim-plan.json</code> No 7 <code>microsim-generator</code> <code>microsim-plan.json</code> <code>docs/sims/*.html</code> No 8 <code>glossary-builder</code> All chapters, <code>glossary-draft.md</code> <code>docs/glossary.md</code> No 9 <code>nav-configurator</code> All outputs <code>mkdocs.yml</code> (updated nav) No 10 <code>quality-validator</code> All chapters, all sims <code>quality-report.md</code> Yes \u2014 quality review 11 <code>cross-ref-checker</code> All chapters, glossary <code>cross-ref-report.md</code> Yes \u2014 final approval 12 <code>gh-deployer</code> Entire <code>docs/</code> tree Live GitHub Pages site No (is the end)"},{"location":"chapters/11/#stage-by-stage-walkthrough","title":"Stage-by-Stage Walkthrough","text":""},{"location":"chapters/11/#step-1-concept-map-generator","title":"Step 1: Concept Map Generator","text":"<p>Input: A topic string (e.g., \"intelligent textbook design\") provided by the user.</p> <p>What it does: Analyzes the topic to extract 40-80 core concepts, their relationships, and the domain structure. Outputs a visual concept map in Mermaid format and a structured <code>concepts.json</code> for downstream use.</p> <p>Key output: <code>concept-map.md</code> (human-readable), <code>concepts.json</code> (machine-readable, consumed by Step 4).</p> <p>Why it is first: Everything downstream \u2014 chapter structure, MicroSim selection, glossary \u2014 derives from the concept inventory. No step can run meaningfully without it.</p>"},{"location":"chapters/11/#step-2-chapter-planner","title":"Step 2: Chapter Planner","text":"<p>Input: <code>concept-map.md</code></p> <p>What it does: Groups concepts into pedagogically sound chapters. Sequences chapters from foundational to advanced. Proposes 10-14 chapter titles with 6-8 section headings per chapter.</p> <p>Key output: <code>chapter-outline.md</code> \u2014 the master document consumed by Steps 3, 5, and 6.</p> <p>Checkpoint 1: The user reviews the chapter outline here. This is the highest-leverage review in the pipeline \u2014 changing the outline after chapters are written requires rewriting chapters. The checkpoint is mandatory, not optional.</p> <p>Checkpoint design: Claude presents the outline and explicitly pauses: \"Review the chapter outline above. Reply with any changes, or type 'approve' to continue to Step 3.\" No step 3 skill is invoked until the user responds.</p>"},{"location":"chapters/11/#step-3-mkdocs-scaffolder","title":"Step 3: MkDocs Scaffolder","text":"<p>Input: <code>chapter-outline.md</code></p> <p>What it does: Creates the full MkDocs Material directory structure \u2014 all chapter directories, placeholder <code>index.md</code> files, <code>mkdocs.yml</code> with full navigation, <code>docs/</code> structure including the <code>sims/</code> directory for MicroSims.</p> <p>Key output: A fully functional (but empty) MkDocs site that can be served locally immediately.</p> <p>Prerequisite check: Verifies that <code>chapter-outline.md</code> exists and is non-empty before starting. If missing, warns the user and halts.</p>"},{"location":"chapters/11/#step-4-concept-definer","title":"Step 4: Concept Definer","text":"<p>Input: <code>concepts.json</code></p> <p>What it does: Generates precise, jargon-free definitions for every concept in the inventory. Produces a <code>definitions.md</code> file consumed by Step 5 to ensure consistent terminology across all chapters, and a <code>glossary-draft.md</code> that Step 8 will expand.</p> <p>Checkpoint 2: The user reviews the concept definitions. This is particularly important for domain-specific terminology where the auto-generated definition might be subtly wrong. A wrong definition propagated across 12 chapters creates a consistency problem that is expensive to fix later.</p>"},{"location":"chapters/11/#step-5-chapter-writer","title":"Step 5: Chapter Writer","text":"<p>Input: <code>chapter-outline.md</code>, <code>definitions.md</code></p> <p>What it does: Writes all chapters in sequence, using the outline for structure and the definitions for consistent terminology. Each chapter is written to <code>docs/chapters/[N]/index.md</code>.</p> <p>Checkpoint 3: After chapter 1 is complete but before proceeding to chapters 2-12, the user reviews chapter 1. This gives the user the opportunity to adjust tone, depth, length, and style before 11 more chapters are written in the same pattern. This checkpoint can save enormous rework.</p> <p>Batch resumption: This step writes a <code>chapter-status.json</code> file tracking each chapter's completion state. If the session ends mid-batch, the next session reads this file and resumes from the next pending chapter.</p>"},{"location":"chapters/11/#step-6-microsim-planner","title":"Step 6: MicroSim Planner","text":"<p>Input: <code>chapter-outline.md</code>, completed chapter files</p> <p>What it does: Identifies the 5-8 highest-value locations for interactive MicroSimulations within the textbook. For each location, produces a detailed MicroSim specification: concept being illustrated, interaction type, data range, visual style, and expected learning outcome.</p> <p>Key output: <code>microsim-plan.json</code> \u2014 consumed by Step 7.</p> <p>Why after Step 5: MicroSim placement decisions are informed by the actual chapter content. The planner reads chapters to find concepts that are difficult to convey in prose and would benefit most from an interactive visualization.</p>"},{"location":"chapters/11/#step-7-microsim-generator","title":"Step 7: MicroSim Generator","text":"<p>Input: <code>microsim-plan.json</code></p> <p>What it does: For each MicroSim specification, generates a standalone HTML file with embedded JavaScript and Chart.js (or D3.js for complex visualizations). Each file is saved to <code>docs/sims/[sim-name].html</code>.</p> <p>This is a meta-skill: The MicroSim Generator routes internally to different reference guides (chartjs-guide.md, d3-guide.md, canvas-guide.md) based on the visualization type in each specification \u2014 the meta-skill router pattern from Chapter 8 in practice.</p> <p>State tracking: <code>sim-status.json</code> tracks each MicroSim's lifecycle state, enabling batch resumption.</p>"},{"location":"chapters/11/#step-8-glossary-builder","title":"Step 8: Glossary Builder","text":"<p>Input: All chapter files, <code>glossary-draft.md</code></p> <p>What it does: Scans all chapter content for term usage frequency and consistency. Expands the draft glossary with usage context, cross-references, and related terms. Produces the final <code>docs/glossary.md</code>.</p> <p>Convergent inputs: Step 8 accepts inputs from both Step 5 (chapters) and Step 4 (glossary draft). It is a convergence point in the dependency graph \u2014 it must wait for both upstream paths to complete.</p>"},{"location":"chapters/11/#step-9-navigation-configurator","title":"Step 9: Navigation Configurator","text":"<p>Input: All completed <code>docs/</code> files</p> <p>What it does: Reads the actual files produced (not the planned files) and builds the correct <code>mkdocs.yml</code> navigation block. Handles chapters, MicroSims, glossary, and any auxiliary pages. Replaces the scaffolded nav with the production nav.</p> <p>Why after Steps 7 and 8: Navigation must reflect what actually exists, not what was planned. MicroSim names may differ from specifications. Additional pages may have been created.</p>"},{"location":"chapters/11/#step-10-quality-validator","title":"Step 10: Quality Validator","text":"<p>Input: All chapter files, all MicroSim files</p> <p>What it does: Applies a comprehensive quality rubric across all content: - Chapter word count targets met - All chapter sections from the outline are present - No broken internal links - MicroSims load correctly (JavaScript syntax check) - Glossary terms cross-referenced in at least one chapter - Reading level appropriate for target audience</p> <p>Key output: <code>quality-report.md</code> \u2014 a scored assessment of every chapter and MicroSim with specific remediation notes for anything below threshold.</p> <p>Checkpoint 4: The user reviews the quality report. Issues flagged here can be fixed before the cross-reference check and before deployment. The checkpoint exists because quality issues are cheaper to fix at this stage than after deployment.</p>"},{"location":"chapters/11/#step-11-cross-reference-checker","title":"Step 11: Cross-Reference Checker","text":"<p>Input: All chapters, glossary</p> <p>What it does: Verifies that terms defined in the glossary are referenced appropriately in chapters. Checks that forward references (\"see Chapter 8\") point to chapters that exist. Verifies that MicroSim references in chapter prose match actual MicroSim filenames.</p> <p>Key output: <code>cross-ref-report.md</code></p> <p>Checkpoint 5: The final approval before deployment. After this checkpoint, the user confirms everything is ready and the deploy runs. This is the last chance to pull back without any public-facing consequence.</p>"},{"location":"chapters/11/#step-12-github-pages-deployer","title":"Step 12: GitHub Pages Deployer","text":"<p>Input: Entire <code>docs/</code> tree, <code>mkdocs.yml</code></p> <p>What it does: Runs <code>mkdocs gh-deploy</code>, configures GitHub Pages, and verifies the site is accessible. Logs the deployment URL.</p> <p>This step is irreversible in the public sense \u2014 once deployed, the site is live. The checkpoint before it exists for this reason.</p>"},{"location":"chapters/11/#checkpoint-design","title":"Checkpoint Design","text":"<p>Checkpoints are deliberate pauses in the pipeline where Claude stops, presents its work, and waits for explicit user approval before continuing. They are not optional review suggestions \u2014 they are hard stops.</p>"},{"location":"chapters/11/#where-to-place-checkpoints","title":"Where to Place Checkpoints","text":"<p>Place checkpoints at:</p> <ol> <li>High-leverage decision points \u2014 where the user's approval gates a large amount of downstream work (outline review, chapter 1 review)</li> <li>Irreversibility boundaries \u2014 just before steps that are difficult or impossible to undo (deployment, bulk file writes that overwrite prior outputs)</li> <li>Quality gates \u2014 after validation steps where issues must be resolved before proceeding</li> <li>Expensive downstream steps \u2014 before steps that will consume significant time and tokens, giving the user a chance to redirect</li> </ol> <p>The textbook pipeline has 5 checkpoints because it has 5 distinct decision points that meet these criteria. Not every pipeline needs 5. A 4-step pipeline might have 1 or 2.</p>"},{"location":"chapters/11/#checkpoint-implementation","title":"Checkpoint Implementation","text":"<p>In a pipeline orchestrator skill, checkpoints are implemented as explicit pause instructions:</p> <pre><code>## Checkpoint 1: Outline Review\n\nThe chapter outline above represents the complete structure of the textbook.\nEvery chapter will be written to match this outline.\n\n**Review carefully:**\n- Chapter titles and sequence\n- Section headings within each chapter\n- Balance of coverage across topics\n\nTo proceed: reply \"approve\" or provide specific changes.\nTo modify: describe the changes needed and the pipeline will revise the outline\nbefore continuing.\n\n**The pipeline will not advance to Step 3 until you respond.**\n</code></pre> <p>The final sentence is the critical instruction. Claude will not invoke the next skill until the checkpoint is resolved.</p>"},{"location":"chapters/11/#checkpoint-state-in-the-session-log","title":"Checkpoint State in the Session Log","text":"<p>Every checkpoint event is logged:</p> <pre><code>## Checkpoint 1 \u2014 Outline Review\n**Status:** Approved\n**User response:** \"approve \u2014 add a chapter on deployment before the reference chapter\"\n**Action taken:** Chapter outline revised to add Chapter 15 on deployment\n**Time elapsed waiting for approval:** 4m 12s\n**Proceeding to:** Step 3\n</code></pre> <p>This allows a resume session to know which checkpoints were approved and with what instructions, without re-asking the user.</p>"},{"location":"chapters/11/#prerequisite-detection","title":"Prerequisite Detection","text":"<p>Every skill in a pipeline should check its prerequisites before beginning work. This prevents a common failure mode: a skill runs, produces output based on incomplete inputs, and the pipeline continues on a flawed foundation.</p>"},{"location":"chapters/11/#implementing-prerequisite-detection","title":"Implementing Prerequisite Detection","text":"<p>In the skill's pre-flight section:</p> <pre><code>## Pre-Flight: Prerequisite Check\n\nBefore starting, verify all required inputs exist:\n\nRequired files:\n- [ ] `chapter-outline.md` \u2014 read first 100 words to verify structure\n- [ ] `definitions.md` \u2014 verify file exists and is non-empty\n\nIf any required file is missing:\nHALT. Report to user: \"Step 5 (chapter-writer) cannot start: [filename] is\nmissing. Complete Step [N] first or provide the file manually.\"\n\nDo NOT proceed with missing prerequisites. Do NOT attempt to generate\nsubstitute content.\n</code></pre> <p>The final two sentences are important. A skill should not attempt to be helpful by generating a substitute for a missing prerequisite \u2014 that produces subtly wrong output that is harder to debug than an explicit error. Fail loudly and specifically.</p>"},{"location":"chapters/11/#prerequisite-check-format","title":"Prerequisite Check Format","text":"<p>The check should be as lightweight as possible \u2014 just enough to verify existence and basic validity, not enough to consume significant tokens:</p> <ul> <li>For required files: check existence + read first 100 words (verifies it is not empty or corrupted)</li> <li>For JSON state files: check existence + read top-level keys (verifies the schema matches expectations)</li> <li>For directories: check existence + count files (verifies scaffolding completed)</li> </ul> <p>Do not read entire prerequisite files as part of the check. Read only enough to confirm validity.</p>"},{"location":"chapters/11/#how-one-skills-output-becomes-the-next-skills-input","title":"How One Skill's Output Becomes the Next Skill's Input","text":"<p>The handoff between pipeline stages is defined by file contracts \u2014 agreements about what a skill produces and where it puts it.</p>"},{"location":"chapters/11/#defining-file-contracts","title":"Defining File Contracts","text":"<p>Every pipeline skill defines two things in its SKILL.md:</p> <p>Input contract \u2014 what files it expects, at what paths, in what format: <pre><code>## Inputs\n\n| File | Path | Format | Required |\n|------|------|--------|----------|\n| Chapter outline | `./chapter-outline.md` | Markdown with H2 chapter headers | Yes |\n| Concept definitions | `./definitions.md` | Markdown, one H3 per concept | Yes |\n</code></pre></p> <p>Output contract \u2014 what files it produces, at what paths, in what format: <pre><code>## Outputs\n\n| File | Path | Format | Consumed By |\n|------|------|--------|-------------|\n| Chapter N | `./docs/chapters/[N]/index.md` | MkDocs Material markdown | Steps 8, 9, 10, 11 |\n| Status file | `./chapter-status.json` | JSON, lifecycle states | Resume detection |\n| Session log | `./logs/chapter-writer-v1.0.0-[DATE].md` | Markdown log format | Resume detection |\n</code></pre></p> <p>When the output contract of Step N matches the input contract of Step N+1, the handoff is automatic. The orchestrating pipeline skill (if you build one) or the user simply invokes the next skill after the previous one signals completion.</p>"},{"location":"chapters/11/#the-orchestrator-pattern","title":"The Orchestrator Pattern","text":"<p>For complex pipelines, an orchestrator skill manages the sequencing. Instead of the user manually invoking each skill, they invoke the orchestrator once:</p> <pre><code>/textbook-pipeline \"AI and machine learning fundamentals\" --audience \"graduate students\"\n</code></pre> <p>The orchestrator: 1. Reads the pipeline definition (which steps exist, in what order, with what dependencies) 2. Invokes Step 1 3. Detects Step 1 completion from its output file 4. Invokes Step 2 5. Reaches Checkpoint 1, pauses, presents work, waits for approval 6. On approval, invokes Step 3 7. Continues until pipeline completes or a step fails</p> <p>The orchestrator's SKILL.md contains the pipeline definition \u2014 the ordered list of steps, their dependencies, and checkpoint positions \u2014 not the execution logic for any individual step. Each step's logic lives in its own skill's SKILL.md.</p>"},{"location":"chapters/11/#designing-your-own-multi-skill-pipeline","title":"Designing Your Own Multi-Skill Pipeline","text":""},{"location":"chapters/11/#step-1-define-the-end-state","title":"Step 1: Define the End State","text":"<p>Start from what you want to have when the pipeline completes. Be specific. \"A deployed textbook with 12 chapters and 5 MicroSims at [URL]\" is a definition. \"A good textbook\" is not.</p>"},{"location":"chapters/11/#step-2-identify-the-stages","title":"Step 2: Identify the Stages","text":"<p>Work backward from the end state. What must exist for the last step to run? What must exist for the second-to-last step to run? Continue until you reach a stage whose only input is the user's initial request.</p> <p>Each stage you identify becomes a potential skill. Some stages may be combinable (if they are small and have the same inputs/outputs). Some stages may need to split (if a stage is too complex to handle in one skill).</p>"},{"location":"chapters/11/#step-3-draw-the-dependency-graph","title":"Step 3: Draw the Dependency Graph","text":"<p>Draw the DAG before writing any skill definitions. Identify: - Which stages can run in parallel (accelerate these paths) - Which stages are on the critical path (optimize these first) - Where convergent inputs occur (coordinate these carefully) - Whether any circular dependencies exist (redesign if they do)</p>"},{"location":"chapters/11/#step-4-define-file-contracts","title":"Step 4: Define File Contracts","text":"<p>For each stage, define the input and output contracts. Verify that every output contract has a matching input contract downstream. If an output file is never consumed, the stage may be unnecessary.</p>"},{"location":"chapters/11/#step-5-place-checkpoints","title":"Step 5: Place Checkpoints","text":"<p>Apply the checkpoint criteria: high-leverage decision points, irreversibility boundaries, quality gates, expensive downstream steps. Place checkpoints at those locations. Err on the side of more checkpoints during initial pipeline development \u2014 you can remove them later if they feel unnecessary.</p>"},{"location":"chapters/11/#step-6-define-prerequisite-checks","title":"Step 6: Define Prerequisite Checks","text":"<p>For each stage with a prerequisite (every stage except the first), write the pre-flight check. Specify exactly what is checked and what the error message says if the check fails.</p>"},{"location":"chapters/11/#step-7-build-and-test-incrementally","title":"Step 7: Build and Test Incrementally","text":"<p>Build the first two stages and test the handoff before building stages 3-12. The most common pipeline failures are handoff failures \u2014 output contracts that do not match input contracts, file paths that differ by one character, JSON schemas that evolved between writing the producer and the consumer.</p> <p>Testing stage-by-stage catches these failures at their source.</p>"},{"location":"chapters/11/#session-management-in-long-pipelines","title":"Session Management in Long Pipelines","text":"<p>A 12-step pipeline that produces 120,000 words of content across dozens of files will run across multiple sessions. The session management patterns from Chapter 10 apply here in a pipeline-specific way.</p>"},{"location":"chapters/11/#the-pipeline-state-file","title":"The Pipeline State File","text":"<p>The orchestrator skill maintains a <code>pipeline-status.json</code> that tracks the state of each pipeline stage:</p> <pre><code>{\n  \"pipeline_id\": \"textbook-ai-fundamentals-2024-11-15\",\n  \"topic\": \"AI and machine learning fundamentals\",\n  \"version\": \"1.0.0\",\n  \"created\": \"2024-11-15T09:00:00Z\",\n  \"last_updated\": \"2024-11-15T16:44:00Z\",\n  \"stages\": {\n    \"step-01-concept-map\": { \"status\": \"complete\", \"completed_at\": \"2024-11-15T09:47:00Z\" },\n    \"step-02-chapter-planner\": { \"status\": \"complete\", \"checkpoint_approved\": true, \"completed_at\": \"2024-11-15T10:15:00Z\" },\n    \"step-03-scaffolder\": { \"status\": \"complete\", \"completed_at\": \"2024-11-15T10:22:00Z\" },\n    \"step-04-concept-definer\": { \"status\": \"complete\", \"checkpoint_approved\": true, \"completed_at\": \"2024-11-15T11:05:00Z\" },\n    \"step-05-chapter-writer\": { \"status\": \"in_progress\", \"sub_status\": { \"complete\": 7, \"pending\": 5 } },\n    \"step-06-microsim-planner\": { \"status\": \"pending\" },\n    \"step-07-microsim-generator\": { \"status\": \"pending\" },\n    \"step-08-glossary-builder\": { \"status\": \"pending\" },\n    \"step-09-nav-configurator\": { \"status\": \"pending\" },\n    \"step-10-quality-validator\": { \"status\": \"pending\" },\n    \"step-11-cross-ref-checker\": { \"status\": \"pending\" },\n    \"step-12-deployer\": { \"status\": \"pending\" }\n  }\n}\n</code></pre> <p>A resume session reads this file first (~600 tokens) and immediately knows: steps 1-4 are complete, step 5 is in progress with 7 of 12 chapters done, steps 6-12 are pending. Without this file, reconstructing this state would require reading all produced outputs \u2014 potentially 50,000+ tokens.</p>"},{"location":"chapters/11/#checkpoint-approval-persistence","title":"Checkpoint Approval Persistence","text":"<p>Checkpoint approvals are persisted in the pipeline state file. When a resume session reaches a step that follows a checkpoint, it checks whether the checkpoint is already approved before displaying it again. This prevents the user from being shown the same checkpoint review twice after a session resume.</p>"},{"location":"chapters/11/#anti-patterns-in-pipeline-design","title":"Anti-Patterns in Pipeline Design","text":"<p>Do not build a monolithic orchestrator. An orchestrator skill that contains the full logic for all 12 steps is not a pipeline \u2014 it is a single skill with 12 workflows embedded. The correct design is an orchestrator that invokes individual skills, each of which contains its own workflow logic.</p> <p>Do not skip prerequisite checks to save tokens. A 100-token prerequisite check prevents the 10,000-token cost of a skill that runs to completion on incomplete inputs and produces unusable output. The math is always in favor of the check.</p> <p>Do not place checkpoints after irreversible actions. A checkpoint before deployment is useful. A checkpoint after deployment is decorative. Checkpoints must occur before the action that requires approval, not after it.</p> <p>Do not hardcode file paths in skill definitions. Skills that hardcode absolute paths work for exactly one user in exactly one directory structure. Use relative paths from the working directory, or accept the output directory as a parameter.</p> <p>Do not let a stage fail silently. When a stage fails, it should write its failure to the pipeline state file, log the error with specifics, and surface a clear message to the user. Silent failures produce pipelines where stages 5-12 appear to complete successfully on broken inputs from stage 4.</p>"},{"location":"chapters/11/#summary","title":"Summary","text":"<p>Pipeline orchestration transforms a collection of individual skills into a coordinated production system. The 12-step intelligent textbook pipeline demonstrates every pattern in practice: dependency order, checkpoint design, prerequisite detection, file contracts, batch resumption, and session state management.</p> <p>The key principles:</p> <p>Dependency graphs first. Draw the DAG before writing any skill. The graph reveals parallelism opportunities, critical path, and convergent inputs.</p> <p>File contracts define handoffs. Every stage's output contract must match the next stage's input contract. Mismatches are the primary source of pipeline failures.</p> <p>Checkpoints are hard stops. At high-leverage decision points and irreversibility boundaries, the pipeline waits for explicit user approval. This is not optional politeness \u2014 it is a design requirement.</p> <p>Prerequisite checks fail loudly. A missing prerequisite is reported immediately and specifically. No stage attempts to substitute for a missing input.</p> <p>State files enable resumption. The pipeline state file allows any new session to understand exactly where the pipeline stands at the cost of one small file read.</p> <p>The pipeline pattern is the culmination of everything in this guide: meta-skill routing (Chapter 8) for variant selection within stages, token efficiency design (Chapter 9) to keep individual stages lean, and session logging (Chapter 10) to provide the state persistence that makes multi-session pipelines possible.</p> <p>With these four patterns mastered \u2014 routing, efficiency, logging, and orchestration \u2014 you have the full toolkit for building production-grade Claude Code skill systems at any scale.</p>"},{"location":"chapters/12/","title":"Chapter 12: Data Format Skills","text":"<p>Data format skills occupy a specific and important role in the Claude Code skill ecosystem. They accept structured input \u2014 CSV files, JSON payloads, learning graphs, metadata schemas \u2014 and return transformed output that downstream tools, MicroSims, or human reviewers can use directly. Unlike analysis skills (which evaluate content) or code generation skills (which produce executable programs), data format skills are fundamentally about transformation: changing the shape, encoding, or organization of information without changing its meaning.</p> <p>This chapter covers the conceptual framework for data format skills, the practical file patterns they rely on, how Python helper scripts extend their capabilities, and a complete worked example: a skill that converts survey data into a format ready for Chart.js visualization.</p>"},{"location":"chapters/12/#what-makes-a-skill-a-data-format-skill","title":"What Makes a Skill a \"Data Format\" Skill","text":"<p>The defining characteristic is that the primary deliverable is a data artifact rather than narrative text or executable code. The output might be:</p> <ul> <li>A JSON file structured for a specific consumer (a MicroSim, a React component, an MkDocs plugin)</li> <li>A CSV file with normalized, cleaned column names</li> <li>A metadata schema with validation rules</li> <li>A learning graph represented as nodes and edges in a standardized format</li> </ul> <p>The content going in and coming out is largely the same information \u2014 the skill's value is in the reshaping.</p> <p>This distinction matters for skill design because:</p> <ol> <li>Output validation is deterministic. You can write a schema and check the output against it. A narrative skill's quality is subjective; a data format skill either produces valid JSON or it does not.</li> <li>Python helpers become first-class tools. Parsing CSVs, validating schemas, and normalizing strings are all tasks where a five-line Python script is more reliable than prompting Claude to do it in its head.</li> <li>The <code>data.json</code> pattern applies. Data format skills typically produce or consume a canonical <code>data.json</code> or similar file that serves as the handoff point between the skill and its consumers.</li> </ol>"},{"location":"chapters/12/#the-datajson-pattern","title":"The <code>data.json</code> Pattern","text":"<p>Many Claude Code MicroSims and textbook visualizations follow a pattern where the interactive component reads its configuration and data from a <code>data.json</code> file in the same directory. This separation of data from rendering logic is intentional: it allows a data format skill to regenerate the data without touching the visualization code.</p> <p>A typical <code>data.json</code> for a bar chart MicroSim looks like this:</p> <pre><code>{\n  \"title\": \"Survey Results: AI Tool Adoption\",\n  \"description\": \"Responses from 47 participants across 3 cohorts\",\n  \"chart_type\": \"bar\",\n  \"x_axis_label\": \"Tool Category\",\n  \"y_axis_label\": \"Percentage of Respondents\",\n  \"colors\": [\"#6200ea\", \"#03dac6\", \"#ff6d00\"],\n  \"series\": [\n    {\n      \"label\": \"Cohort A\",\n      \"values\": [72, 45, 88, 31, 67]\n    },\n    {\n      \"label\": \"Cohort B\",\n      \"values\": [58, 61, 79, 44, 55]\n    }\n  ],\n  \"categories\": [\"LLM Chat\", \"Image Gen\", \"Code Assist\", \"Voice AI\", \"Search AI\"]\n}\n</code></pre> <p>The MicroSim's JavaScript reads this file at initialization:</p> <pre><code>fetch('data.json')\n  .then(res =&gt; res.json())\n  .then(data =&gt; initChart(data));\n</code></pre> <p>When you build a data format skill, your goal is often to produce exactly this kind of file \u2014 a structured, validated artifact that another system will consume without further transformation.</p>"},{"location":"chapters/12/#python-helper-scripts-in-data-format-skills","title":"Python Helper Scripts in Data Format Skills","text":"<p>Claude Code can execute Python scripts during skill execution using the <code>Bash</code> tool. This capability transforms data format skills from \"prompt Claude to reshape this data\" into \"run a deterministic transformation, then let Claude handle the parts that require judgment.\"</p> <p>A skill directory that includes a Python helper looks like this:</p> <pre><code>skills/\n  survey-to-chart-data/\n    SKILL.md\n    transform.py\n    schema.json\n    example-input.csv\n    example-output.json\n</code></pre> <p>The <code>SKILL.md</code> references the Python script explicitly in its workflow:</p> <pre><code>---\nname: survey-to-chart-data\ndescription: Converts a survey CSV export to Chart.js-ready data.json\ntriggers:\n  - /survey-to-chart\n  - /chart-data\nallowed-tools:\n  - Read\n  - Write\n  - Bash\n---\n\n## Workflow\n\n1. Read the input CSV file specified by the user\n2. Run `python3 transform.py &lt;input_file&gt; &lt;output_file&gt;` using the Bash tool\n3. Read the output JSON and validate it against schema.json\n4. Report any validation errors and corrections made\n5. Confirm the output file path to the user\n</code></pre> <p>The <code>allowed-tools</code> frontmatter field is important here. Without <code>Bash</code> in the list, Claude Code will not execute shell commands during this skill's execution. Always declare tool permissions explicitly for data format skills that use Python helpers.</p>"},{"location":"chapters/12/#python-helper-design-principles","title":"Python Helper Design Principles","text":"<p>When writing <code>transform.py</code> for a data format skill, follow these conventions:</p> <p>Accept file paths as arguments, not stdin. Claude Code's Bash tool works most reliably with explicit file paths. Use <code>sys.argv</code> rather than piping data.</p> <p>Write to a file, not stdout. The skill workflow reads the output file using the <code>Read</code> tool. Writing to stdout requires capturing it, which adds complexity.</p> <p>Exit with a non-zero code on failure. Claude Code checks exit codes. If your Python script fails silently and exits 0, the skill will proceed with bad or missing output.</p> <p>Include a schema validation step in the script. Validate the output against the expected schema before writing it. Return a descriptive error message if validation fails.</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\ntransform.py - Convert survey CSV to Chart.js-ready data.json\n\nUsage: python3 transform.py &lt;input_csv&gt; &lt;output_json&gt;\n\"\"\"\n\nimport sys\nimport json\nimport csv\nimport re\nfrom pathlib import Path\n\n\ndef normalize_column(name: str) -&gt; str:\n    \"\"\"Convert a human-readable column header to a valid key.\"\"\"\n    return re.sub(r'[^a-z0-9_]', '_', name.lower().strip()).strip('_')\n\n\ndef load_survey_csv(path: str) -&gt; dict:\n    rows = []\n    with open(path, newline='', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            rows.append({normalize_column(k): v for k, v in row.items()})\n    return rows\n\n\ndef build_chart_data(rows: list, group_col: str, value_col: str) -&gt; dict:\n    groups = {}\n    for row in rows:\n        group = row.get(group_col, 'Unknown')\n        try:\n            value = float(row.get(value_col, 0))\n        except ValueError:\n            value = 0\n        if group not in groups:\n            groups[group] = []\n        groups[group].append(value)\n\n    categories = sorted(groups.keys())\n    averages = [round(sum(groups[c]) / len(groups[c]), 1) for c in categories]\n\n    return {\n        \"chart_type\": \"bar\",\n        \"categories\": categories,\n        \"series\": [{\"label\": value_col.replace('_', ' ').title(), \"values\": averages}],\n        \"x_axis_label\": group_col.replace('_', ' ').title(),\n        \"y_axis_label\": value_col.replace('_', ' ').title()\n    }\n\n\ndef validate_output(data: dict) -&gt; list:\n    errors = []\n    required = [\"chart_type\", \"categories\", \"series\", \"x_axis_label\", \"y_axis_label\"]\n    for field in required:\n        if field not in data:\n            errors.append(f\"Missing required field: {field}\")\n    if \"series\" in data:\n        for s in data[\"series\"]:\n            if \"label\" not in s or \"values\" not in s:\n                errors.append(\"Each series must have 'label' and 'values'\")\n    return errors\n\n\ndef main():\n    if len(sys.argv) &lt; 3:\n        print(\"Usage: python3 transform.py &lt;input_csv&gt; &lt;output_json&gt;\", file=sys.stderr)\n        sys.exit(1)\n\n    input_path = sys.argv[1]\n    output_path = sys.argv[2]\n\n    # Allow optional column overrides\n    group_col = sys.argv[3] if len(sys.argv) &gt; 3 else \"cohort\"\n    value_col = sys.argv[4] if len(sys.argv) &gt; 4 else \"score\"\n\n    rows = load_survey_csv(input_path)\n    data = build_chart_data(rows, group_col, value_col)\n\n    errors = validate_output(data)\n    if errors:\n        for e in errors:\n            print(f\"Validation error: {e}\", file=sys.stderr)\n        sys.exit(2)\n\n    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(data, f, indent=2)\n\n    print(f\"Output written to {output_path}\")\n    print(f\"Categories: {len(data['categories'])}, Series: {len(data['series'])}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"chapters/12/#learning-graph-data-flows","title":"Learning Graph Data Flows","text":"<p>Learning graphs \u2014 the data structures that power concept relationship visualizations in intelligent textbooks \u2014 have their own canonical format. A data format skill for learning graphs accepts a list of concepts and their relationships and produces a JSON structure that a D3.js or Cytoscape.js visualization can render.</p> <p>The standard learning graph format used in the claude-skills ecosystem is:</p> <pre><code>{\n  \"concepts\": [\n    {\n      \"id\": \"skill-anatomy\",\n      \"label\": \"Skill Anatomy\",\n      \"type\": \"foundation\",\n      \"level\": 1,\n      \"description\": \"The structural components of a SKILL.md file\"\n    },\n    {\n      \"id\": \"yaml-frontmatter\",\n      \"label\": \"YAML Frontmatter\",\n      \"type\": \"component\",\n      \"level\": 2,\n      \"description\": \"Metadata block at the top of a SKILL.md\"\n    }\n  ],\n  \"relationships\": [\n    {\n      \"source\": \"skill-anatomy\",\n      \"target\": \"yaml-frontmatter\",\n      \"type\": \"contains\",\n      \"weight\": 1.0\n    }\n  ],\n  \"metadata\": {\n    \"domain\": \"claude-code-skills\",\n    \"total_concepts\": 2,\n    \"total_relationships\": 1,\n    \"generated_at\": \"2026-02-20\"\n  }\n}\n</code></pre> <p>A skill that generates this format from a plain-text concept list is straightforward to build. The workflow:</p> <ol> <li>Accept a Markdown or plain-text file listing concept names with optional descriptions</li> <li>Use Claude to infer relationships between concepts (this is where the LLM adds genuine value \u2014 relationship inference from a concept list is a judgment task)</li> <li>Assign level numbers based on prerequisite depth (foundational concepts = level 1, advanced = higher levels)</li> <li>Run a Python validator to confirm the output conforms to the schema</li> <li>Write the result to <code>learning-graph.json</code></li> </ol> <pre><code>flowchart LR\n    A[Input: concepts.txt] --&gt; B[Claude infers relationships]\n    B --&gt; C[Python assigns levels]\n    C --&gt; D[Validator checks schema]\n    D --&gt; E{Valid?}\n    E --&gt;|Yes| F[Write learning-graph.json]\n    E --&gt;|No| G[Claude corrects errors]\n    G --&gt; D</code></pre>"},{"location":"chapters/12/#metadata-schema-validation","title":"Metadata Schema Validation","text":"<p>Data format skills that produce schemas \u2014 rather than instance data \u2014 serve a different purpose: they define the shape that other data must conform to. A metadata schema skill might accept a directory of Markdown files and produce a JSON Schema document that describes the frontmatter fields those files use.</p> <p>This is a two-pass process:</p> <p>Pass 1: Discovery. The skill reads all files in the directory and catalogs every frontmatter field it finds, along with its inferred type and whether it appears consistently.</p> <p>Pass 2: Synthesis. Claude synthesizes the catalog into a JSON Schema with descriptions, required fields, and enum constraints where the values are consistent.</p> <pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"MicroSim Metadata Schema\",\n  \"type\": \"object\",\n  \"required\": [\"title\", \"description\", \"difficulty\", \"concepts\"],\n  \"properties\": {\n    \"title\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable title for the MicroSim\"\n    },\n    \"description\": {\n      \"type\": \"string\",\n      \"description\": \"One or two sentence description of what the sim teaches\"\n    },\n    \"difficulty\": {\n      \"type\": \"string\",\n      \"enum\": [\"beginner\", \"intermediate\", \"advanced\"],\n      \"description\": \"Difficulty level for the target learner\"\n    },\n    \"concepts\": {\n      \"type\": \"array\",\n      \"items\": { \"type\": \"string\" },\n      \"description\": \"List of concept IDs this sim teaches\"\n    }\n  }\n}\n</code></pre>"},{"location":"chapters/12/#worked-example-survey-data-to-visualization-ready-format","title":"Worked Example: Survey Data to Visualization-Ready Format","text":"<p>Here is the complete, end-to-end design of a data format skill that converts survey export data to a Chart.js-ready format. This example brings together all the patterns discussed in this chapter.</p>"},{"location":"chapters/12/#skill-directory-structure","title":"Skill Directory Structure","text":"<pre><code>skills/\n  survey-to-chart-data/\n    SKILL.md\n    transform.py\n    validate_schema.py\n    examples/\n      sample-input.csv\n      expected-output.json\n    README.md\n</code></pre>"},{"location":"chapters/12/#skillmd","title":"SKILL.md","text":"<p><pre><code>---\nname: survey-to-chart-data\ndescription: |\n  Converts a survey CSV export to a Chart.js-ready data.json file.\n  Supports grouped bar charts, line charts, and pie charts.\n  Validates output against the MicroSim data schema before writing.\ntriggers:\n  - /survey-to-chart\n  - /chart-data\n  - /survey-convert\nallowed-tools:\n  - Read\n  - Bash\n  - Write\nversion: 1.0.0\n---\n\n## Purpose\n\nTransform survey data exports (CSV) into structured JSON files that\nChart.js MicroSims can consume directly. Handle column name normalization,\ntype coercion, missing value imputation, and output schema validation.\n\n## Parameters\n\nThe user must provide:\n- `input`: Path to the CSV file\n- `output`: Path for the data.json output (default: same directory as input)\n- `group_col`: Column name to use for grouping (default: first string column)\n- `value_col`: Column name to use for values (default: first numeric column)\n- `chart_type`: One of bar, line, pie (default: bar)\n\n## Workflow\n\n### Step 1: Parse the user's request\nExtract input path, output path, and column preferences from the user message.\nIf paths are ambiguous, ask for clarification before proceeding.\n\n### Step 2: Inspect the CSV headers\nUse the Read tool to read the first 5 lines of the CSV.\nReport the column names to the user and confirm which columns to use.\n\n### Step 3: Run the transformation script\nExecute:\n</code></pre> python3 ~/.claude/skills/survey-to-chart-data/transform.py \\    <pre><code>### Step 4: Validate the output\nRun:\n</code></pre> python3 ~/.claude/skills/survey-to-chart-data/validate_schema.py  <pre><code>If validation fails, read the error output and attempt to correct the issue\nby re-running the transformation with adjusted parameters.\n\n### Step 5: Report to the user\nConfirm the output file path, the number of categories and series,\nand any data quality issues detected during transformation.\n\n## Error Handling\n\n- If the CSV is empty or has no parseable rows, report this immediately and stop.\n- If the specified column does not exist, list available columns and ask\n  the user to choose.\n- If the output directory does not exist, create it before writing.\n- Never overwrite an existing data.json without confirming with the user.\n</code></pre>"},{"location":"chapters/12/#the-validation-script","title":"The Validation Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nvalidate_schema.py - Validate a data.json against the MicroSim chart schema\n\nUsage: python3 validate_schema.py &lt;data_json_path&gt;\nExit 0: valid | Exit 1: invalid\n\"\"\"\n\nimport sys\nimport json\n\nREQUIRED_FIELDS = [\"chart_type\", \"categories\", \"series\"]\nVALID_CHART_TYPES = {\"bar\", \"line\", \"pie\"}\n\n\ndef validate(data: dict) -&gt; list:\n    errors = []\n\n    for field in REQUIRED_FIELDS:\n        if field not in data:\n            errors.append(f\"Missing required field: '{field}'\")\n\n    if \"chart_type\" in data and data[\"chart_type\"] not in VALID_CHART_TYPES:\n        errors.append(\n            f\"Invalid chart_type '{data['chart_type']}'. \"\n            f\"Must be one of: {', '.join(VALID_CHART_TYPES)}\"\n        )\n\n    if \"categories\" in data:\n        if not isinstance(data[\"categories\"], list) or len(data[\"categories\"]) == 0:\n            errors.append(\"'categories' must be a non-empty list\")\n\n    if \"series\" in data:\n        if not isinstance(data[\"series\"], list) or len(data[\"series\"]) == 0:\n            errors.append(\"'series' must be a non-empty list\")\n        else:\n            for i, s in enumerate(data[\"series\"]):\n                if \"label\" not in s:\n                    errors.append(f\"Series[{i}] missing 'label'\")\n                if \"values\" not in s:\n                    errors.append(f\"Series[{i}] missing 'values'\")\n                elif \"categories\" in data:\n                    if len(s[\"values\"]) != len(data[\"categories\"]):\n                        errors.append(\n                            f\"Series[{i}] has {len(s['values'])} values \"\n                            f\"but {len(data['categories'])} categories\"\n                        )\n\n    return errors\n\n\ndef main():\n    if len(sys.argv) &lt; 2:\n        print(\"Usage: python3 validate_schema.py &lt;data_json_path&gt;\", file=sys.stderr)\n        sys.exit(1)\n\n    path = sys.argv[1]\n    try:\n        with open(path) as f:\n            data = json.load(f)\n    except json.JSONDecodeError as e:\n        print(f\"Invalid JSON: {e}\", file=sys.stderr)\n        sys.exit(1)\n    except FileNotFoundError:\n        print(f\"File not found: {path}\", file=sys.stderr)\n        sys.exit(1)\n\n    errors = validate(data)\n    if errors:\n        print(f\"Validation failed ({len(errors)} errors):\")\n        for e in errors:\n            print(f\"  - {e}\")\n        sys.exit(1)\n    else:\n        print(\"Validation passed.\")\n        print(f\"  chart_type: {data['chart_type']}\")\n        print(f\"  categories: {len(data['categories'])}\")\n        print(f\"  series: {len(data['series'])}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"chapters/12/#handling-schema-evolution","title":"Handling Schema Evolution","text":"<p>Data format skills often outlive the schemas they produce. When a MicroSim's <code>data.json</code> format changes \u2014 a field is renamed, a new required field is added, an array becomes a nested object \u2014 existing skills that produce the old format break silently. The generated files look valid, but the visualization fails at runtime.</p> <p>Three practices prevent schema evolution from becoming a problem:</p> <p>Embed the schema version in the output. Every <code>data.json</code> should include a <code>schema_version</code> field:</p> <pre><code>{\n  \"schema_version\": \"2.0\",\n  \"chart_type\": \"bar\",\n  ...\n}\n</code></pre> <p>The consumer (the MicroSim JavaScript) reads <code>schema_version</code> and either handles migration in-place or raises a clear error explaining which version it expects.</p> <p>Keep old schemas in the skill repository. When you update the schema, do not delete the old one. Store schemas in a <code>schemas/</code> directory:</p> <pre><code>skills/\n  survey-to-chart-data/\n    schemas/\n      v1.0.json\n      v2.0.json\n    SKILL.md\n    transform.py\n    validate_schema.py   # reads version from output, validates against correct schema\n</code></pre> <p>The validation script reads the <code>schema_version</code> field from the output and selects the appropriate schema file to validate against. This allows a single skill to produce and validate multiple schema versions during a transition period.</p> <p>Document breaking changes in the SKILL.md changelog. When the output schema changes in a way that breaks existing consumers, make it explicit in the SKILL.md frontmatter with a <code>breaking_changes</code> field and document migration steps in the README.</p>"},{"location":"chapters/12/#when-to-use-a-python-helper-vs-pure-prompt","title":"When to Use a Python Helper vs. Pure Prompt","text":"<p>The decision between using a Python helper script and relying on Claude to perform the transformation in a single prompt turn is not always obvious. The rule of thumb:</p> <p>Use Python when the transformation is deterministic. Column name normalization, type coercion, numeric aggregation, schema validation \u2014 these have exactly one correct answer for any given input. A Python function is faster, cheaper, and more reliable than asking Claude to reason through each row of a CSV.</p> <p>Use Claude when the transformation requires judgment. Inferring relationships between concepts, choosing which columns are semantically related, deciding whether a value is an outlier or a legitimate data point \u2014 these require the kind of contextual reasoning that Claude does well and Python cannot.</p> <p>The best data format skills use both: Python for the mechanical parts, Claude for the judgment calls. The Python helper does the heavy lifting (parsing, normalizing, aggregating) and returns a partially complete structure. Claude fills in the fields that require semantic understanding (descriptions, labels, relationship types) and runs the final validation.</p> <p>This division of labor also improves debugging. When a data format skill produces wrong output, you can isolate whether the problem is in the Python transformation (test the script directly) or in Claude's judgment layer (look at what structure the Python returned and how Claude modified it).</p>"},{"location":"chapters/12/#design-checklist-for-data-format-skills","title":"Design Checklist for Data Format Skills","text":"<p>Before shipping a data format skill, verify these properties:</p> Property Check Output schema is documented Does <code>README.md</code> describe the output format? Python helper exits non-zero on failure Tested with bad input? <code>allowed-tools</code> includes <code>Bash</code> if script runs Frontmatter has <code>Bash</code>? Output file is never silently overwritten Skill checks for existing file? Column/field names are normalized No spaces or special chars in keys? Example input and expected output included <code>examples/</code> directory exists? Validation script is separate from transform script Two scripts, clear separation?"},{"location":"chapters/12/#summary","title":"Summary","text":"<p>Data format skills are transformation tools: they change the shape of data without changing its meaning. The patterns that make them reliable are consistent output schemas, deterministic Python helpers for the mechanical parts, explicit tool permissions in frontmatter, and separate validation scripts that confirm output quality before Claude reports success. The <code>data.json</code> pattern connects data format skills to the broader MicroSim ecosystem, making them a foundational layer for any textbook or course that uses interactive visualizations.</p>"},{"location":"chapters/13/","title":"Chapter 13: Code Generation Skills","text":"<p>Code generation skills occupy a privileged position in the Claude Code skill ecosystem. They do not just transform existing artifacts or score existing content \u2014 they produce new, executable programs from a description. A well-designed code generation skill is, in effect, a highly constrained specialist that knows exactly what kind of code to produce, what quality it must meet, and how to structure the output for immediate use.</p> <p>This chapter covers the patterns that make code generation skills reliable: template-driven output, multi-file generation with consistent naming, metadata files that describe the generated artifact, and output validation strategies. The concept-classifier skill serves as a sustained case study throughout.</p>"},{"location":"chapters/13/#what-separates-a-code-generation-skill-from-a-prompt","title":"What Separates a Code Generation Skill from a Prompt","text":"<p>When you ask Claude to \"write a Chart.js bar chart,\" you get something that works \u2014 sometimes. The output quality varies with phrasing, context, and session state. It might use a deprecated API, omit the responsive wrapper, or forget to include the CDN link.</p> <p>A code generation skill eliminates this variability by encoding all of those requirements explicitly in the skill's workflow and quality criteria. Every invocation of the skill produces code that meets the same specification, regardless of how the user phrased the request.</p> <p>The difference is the difference between improvisation and execution. The skill author does the design work once. Every builder who uses the skill benefits from that design work.</p>"},{"location":"chapters/13/#the-microsim-generation-pattern","title":"The MicroSim Generation Pattern","text":"<p>MicroSims are self-contained, single-file interactive simulations used in intelligent textbooks. A MicroSim generation skill produces a complete, deployable artifact every time it runs. The canonical output is:</p> <pre><code>output/\n  index.html          # The complete simulation (HTML + CSS + JS in one file)\n  metadata.json       # Structured description of the sim\n  screenshot.png      # Optional: captured via headless browser\n</code></pre> <p>The <code>index.html</code> follows a strict template structure:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;&lt;!-- TITLE --&gt;&lt;/title&gt;\n    &lt;style&gt;\n        /* Base layout \u2014 do not modify */\n        body {\n            font-family: system-ui, -apple-system, sans-serif;\n            margin: 0;\n            padding: 1rem;\n            background: #fafafa;\n        }\n        #canvas-container {\n            max-width: 800px;\n            margin: 0 auto;\n        }\n        h1 { font-size: 1.4rem; margin-bottom: 0.25rem; }\n        p.description { color: #555; font-size: 0.9rem; margin-bottom: 1rem; }\n        .controls { margin: 1rem 0; display: flex; gap: 0.5rem; flex-wrap: wrap; }\n        canvas { border: 1px solid #ddd; border-radius: 4px; width: 100%; }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div id=\"canvas-container\"&gt;\n        &lt;h1&gt;&lt;!-- TITLE --&gt;&lt;/h1&gt;\n        &lt;p class=\"description\"&gt;&lt;!-- DESCRIPTION --&gt;&lt;/p&gt;\n        &lt;div class=\"controls\" id=\"controls\"&gt;&lt;/div&gt;\n        &lt;canvas id=\"sim\" width=\"760\" height=\"400\"&gt;&lt;/canvas&gt;\n        &lt;p id=\"status\" style=\"color:#666;font-size:0.8rem\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n\n    &lt;!-- Library loaded from CDN \u2014 version pinned --&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/p5@1.9.0/lib/p5.min.js\"&gt;&lt;/script&gt;\n    &lt;script&gt;\n    // ============================================================\n    // MicroSim: &lt;!-- TITLE --&gt;\n    // Concepts: &lt;!-- CONCEPTS --&gt;\n    // Generated: &lt;!-- DATE --&gt;\n    // ============================================================\n\n    // --- State variables ---\n    // &lt;!-- STATE_VARS --&gt;\n\n    // --- p5.js setup ---\n    function setup() {\n        let canvas = createCanvas(760, 400);\n        canvas.parent('sim');\n        // &lt;!-- SETUP_BODY --&gt;\n    }\n\n    // --- p5.js draw loop ---\n    function draw() {\n        background(250);\n        // &lt;!-- DRAW_BODY --&gt;\n    }\n\n    // --- Controls ---\n    // &lt;!-- CONTROLS_CODE --&gt;\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>A code generation skill that uses this template fills in the comment placeholders. The template guarantees consistent layout, pinned library versions, and required metadata comments on every generation.</p>"},{"location":"chapters/13/#the-metadatajson-file","title":"The <code>metadata.json</code> File","text":"<p>Every generated MicroSim includes a <code>metadata.json</code> that describes the artifact independently of the HTML. This file serves multiple purposes:</p> <ul> <li>It powers search and discovery in a textbook's concept index</li> <li>It provides the data a quality audit skill needs to score the sim without reading the HTML</li> <li>It enables automated learning graph generation (concepts are the nodes)</li> </ul> <pre><code>{\n  \"title\": \"Concept Classifier Simulation\",\n  \"description\": \"Interactive demonstration of how a classifier routes concepts to skill categories based on keyword matching and semantic similarity.\",\n  \"sim_type\": \"interactive\",\n  \"library\": \"p5.js\",\n  \"library_version\": \"1.9.0\",\n  \"concepts\": [\n    \"concept-classification\",\n    \"skill-routing\",\n    \"keyword-matching\",\n    \"semantic-distance\"\n  ],\n  \"difficulty\": \"intermediate\",\n  \"estimated_time_minutes\": 8,\n  \"controls\": [\n    {\n      \"type\": \"text-input\",\n      \"label\": \"Concept name\",\n      \"description\": \"Enter a concept to classify\"\n    },\n    {\n      \"type\": \"button\",\n      \"label\": \"Classify\",\n      \"description\": \"Trigger classification\"\n    }\n  ],\n  \"learning_outcomes\": [\n    \"Explain how keyword matching differs from semantic classification\",\n    \"Predict which skill category a concept will route to\",\n    \"Identify edge cases where classification confidence is low\"\n  ],\n  \"generated_at\": \"2026-02-20\",\n  \"generator_skill\": \"microsim-generator\",\n  \"generator_version\": \"2.1.0\",\n  \"quality_score\": 87\n}\n</code></pre>"},{"location":"chapters/13/#template-patterns-for-consistent-output","title":"Template Patterns for Consistent Output","text":"<p>The core technique in code generation skills is the use of fill-in-the-blank templates rather than open-ended generation. There are three approaches, each with distinct tradeoffs:</p>"},{"location":"chapters/13/#approach-1-comment-placeholders-shown-above","title":"Approach 1: Comment Placeholders (shown above)","text":"<p>Claude fills in comment-delimited sections. The surrounding structure is fixed. This is the most reliable approach for HTML/JS output because the structural scaffolding cannot be accidentally omitted.</p> <p>Best for: Single-file outputs, when the structure is fixed and only the logic varies.</p> <p>Risk: If Claude misinterprets a placeholder boundary, it can corrupt the surrounding structure.</p>"},{"location":"chapters/13/#approach-2-json-template-filling","title":"Approach 2: JSON Template Filling","text":"<p>The skill produces a JSON object with well-defined fields, then a second step renders that JSON into final code. This separates the \"what should this sim do\" decision (Claude's role) from the \"how should it be structured\" (the template's role).</p> <pre><code>## Step 1: Generate the sim specification as JSON\n\nProduce a JSON object with these fields:\n- title: string\n- description: string\n- state_vars: array of {name, type, initial_value, description}\n- setup_logic: string (pseudocode or JavaScript)\n- draw_logic: string (JavaScript for the draw loop)\n- controls: array of {type, label, variable, min, max, step}\n\n## Step 2: Render the JSON into the HTML template\n\nUse the template at ~/.claude/skills/microsim-generator/template.html.\nReplace each placeholder with the corresponding JSON field value.\n</code></pre> <p>Best for: Complex sims where decomposing the specification makes quality easier to verify.</p>"},{"location":"chapters/13/#approach-3-multi-pass-generation","title":"Approach 3: Multi-Pass Generation","text":"<p>Generate the code, run it through a validator (headless browser, linter, or unit test), then regenerate any sections that fail. This is the most expensive approach but produces the highest quality output.</p> <pre><code>flowchart TD\n    A[User describes sim] --&gt; B[Generate specification JSON]\n    B --&gt; C[Render to HTML template]\n    C --&gt; D[Run JS linter via Bash]\n    D --&gt; E{Lint errors?}\n    E --&gt;|Yes| F[Read error messages]\n    F --&gt; G[Correct specific code sections]\n    G --&gt; D\n    E --&gt;|No| H[Validate metadata.json]\n    H --&gt; I{Valid schema?}\n    I --&gt;|No| J[Fix metadata fields]\n    J --&gt; H\n    I --&gt;|Yes| K[Write output files]\n    K --&gt; L[Report to user]</code></pre>"},{"location":"chapters/13/#the-concept-classifier-skill-a-case-study","title":"The Concept-Classifier Skill: A Case Study","text":"<p>The concept-classifier skill routes a concept name to one of several skill categories: foundational, applied, procedural, evaluative, or meta. It produces a classification report as a JSON file. This skill is useful for automatically tagging concepts in a learning graph with their cognitive type.</p>"},{"location":"chapters/13/#directory-structure","title":"Directory Structure","text":"<pre><code>skills/\n  concept-classifier/\n    SKILL.md\n    classify.py\n    categories.json\n    test-cases.txt\n</code></pre>"},{"location":"chapters/13/#categoriesjson-the-classification-schema","title":"<code>categories.json</code> \u2014 The Classification Schema","text":"<pre><code>{\n  \"categories\": {\n    \"foundational\": {\n      \"description\": \"Definitions, concepts, principles \u2014 what something IS\",\n      \"keywords\": [\"what is\", \"definition\", \"principle\", \"concept\", \"theory\", \"model\"],\n      \"bloom_level\": 1\n    },\n    \"applied\": {\n      \"description\": \"Using a concept in a specific context \u2014 what something DOES\",\n      \"keywords\": [\"using\", \"applying\", \"implementing\", \"building\", \"creating\"],\n      \"bloom_level\": 3\n    },\n    \"procedural\": {\n      \"description\": \"Step-by-step processes \u2014 HOW to do something\",\n      \"keywords\": [\"how to\", \"process\", \"workflow\", \"steps\", \"procedure\", \"install\"],\n      \"bloom_level\": 3\n    },\n    \"evaluative\": {\n      \"description\": \"Judgment, scoring, auditing \u2014 assessing quality or correctness\",\n      \"keywords\": [\"evaluate\", \"assess\", \"score\", \"audit\", \"validate\", \"review\"],\n      \"bloom_level\": 5\n    },\n    \"meta\": {\n      \"description\": \"Concepts about the system itself \u2014 strategy, architecture, design\",\n      \"keywords\": [\"strategy\", \"architecture\", \"design\", \"system\", \"orchestrate\", \"router\"],\n      \"bloom_level\": 6\n    }\n  }\n}\n</code></pre>"},{"location":"chapters/13/#skillmd","title":"<code>SKILL.md</code>","text":"<p><pre><code>---\nname: concept-classifier\ndescription: |\n  Classifies a concept into one of five cognitive categories based on\n  keyword matching and Claude's semantic judgment. Produces a JSON\n  classification report.\ntriggers:\n  - /classify-concept\n  - /concept-type\nallowed-tools:\n  - Read\n  - Bash\n  - Write\nversion: 1.2.0\n---\n\n## Purpose\n\nGiven a concept name and optional description, determine which cognitive\ncategory it belongs to. Return a structured JSON report with the\nclassification, confidence score, and rationale.\n\n## Workflow\n\n### Step 1: Read the categories schema\nRead ~/.claude/skills/concept-classifier/categories.json\n\n### Step 2: Run keyword-based pre-classification\nExecute:\n</code></pre> python3 ~/.claude/skills/concept-classifier/classify.py \"\" <pre><code>This returns a JSON object with keyword-match scores for each category.\n\n### Step 3: Apply semantic judgment\nReview the keyword scores alongside your semantic understanding of the\nconcept. Consider:\n- What cognitive operation does mastering this concept require?\n- At what Bloom's Taxonomy level would a learner demonstrate understanding?\n- Is the concept primarily definitional, procedural, or evaluative?\n\n### Step 4: Produce the classification report\n\nOutput a JSON object:\n{\n  \"concept\": \"&lt;name&gt;\",\n  \"category\": \"&lt;category&gt;\",\n  \"confidence\": &lt;0.0 to 1.0&gt;,\n  \"bloom_level\": &lt;1 to 6&gt;,\n  \"rationale\": \"&lt;one sentence&gt;\",\n  \"keyword_scores\": { ... }\n}\n\n### Step 5: Write the report\nIf the user specified an output path, write the JSON to that path.\nOtherwise, display it inline.\n\n## Quality Standards\n\n- Confidence below 0.6 should be flagged as uncertain\n- Rationale must reference the specific characteristic that drove the classification\n- Never classify as \"meta\" without explicit architectural or strategic framing\n</code></pre>"},{"location":"chapters/13/#the-python-pre-classifier","title":"The Python Pre-Classifier","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nclassify.py - Keyword-based pre-classification for concept-classifier skill\n\nUsage: python3 classify.py \"&lt;concept_name&gt;\"\nOutput: JSON with keyword match scores per category\n\"\"\"\n\nimport sys\nimport json\nimport re\nfrom pathlib import Path\n\nCATEGORIES_FILE = Path(__file__).parent / \"categories.json\"\n\n\ndef load_categories():\n    with open(CATEGORIES_FILE) as f:\n        return json.load(f)[\"categories\"]\n\n\ndef score_concept(concept: str, categories: dict) -&gt; dict:\n    concept_lower = concept.lower()\n    words = set(re.findall(r'\\w+', concept_lower))\n    scores = {}\n\n    for cat_name, cat_data in categories.items():\n        score = 0\n        matched_keywords = []\n        for keyword in cat_data[\"keywords\"]:\n            kw_words = keyword.split()\n            if all(w in concept_lower for w in kw_words):\n                score += 1\n                matched_keywords.append(keyword)\n        scores[cat_name] = {\n            \"score\": score,\n            \"matched_keywords\": matched_keywords,\n            \"bloom_level\": cat_data[\"bloom_level\"]\n        }\n\n    return scores\n\n\ndef main():\n    if len(sys.argv) &lt; 2:\n        print(\"Usage: python3 classify.py \\\"&lt;concept_name&gt;\\\"\", file=sys.stderr)\n        sys.exit(1)\n\n    concept = sys.argv[1]\n    categories = load_categories()\n    scores = score_concept(concept, categories)\n\n    result = {\n        \"concept\": concept,\n        \"keyword_scores\": scores,\n        \"top_category\": max(scores, key=lambda k: scores[k][\"score\"])\n    }\n\n    print(json.dumps(result, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"chapters/13/#output-validation-strategies","title":"Output Validation Strategies","text":"<p>Code generation skills must answer a fundamental question: how do you know the generated code works? There are four strategies, each appropriate for different contexts:</p>"},{"location":"chapters/13/#1-structural-validation-always-applicable","title":"1. Structural Validation (always applicable)","text":"<p>Check that the output file exists, is non-empty, and matches the expected structure. For HTML, verify the file parses as valid HTML. For JSON, verify it parses as valid JSON. This catches catastrophic failures \u2014 blank files, truncated output, syntax errors.</p> <pre><code># Check HTML is well-formed (requires html5lib or similar)\npython3 -c \"\nimport sys\nfrom html.parser import HTMLParser\nclass Validator(HTMLParser): pass\nwith open(sys.argv[1]) as f: Validator().feed(f.read())\nprint('HTML structure valid')\n\" index.html\n</code></pre>"},{"location":"chapters/13/#2-schema-validation-for-json-outputs","title":"2. Schema Validation (for JSON outputs)","text":"<p>Run the output against a JSON Schema. This is the most reliable automated check for metadata files and data format outputs.</p>"},{"location":"chapters/13/#3-linting-for-javascript","title":"3. Linting (for JavaScript)","text":"<p>Run ESLint or a basic syntax check against the generated JavaScript. This catches undefined variables, missing semicolons, and common logic errors before the user opens the file.</p> <pre><code># Quick syntax check using Node.js\nnode --check generated-sim.js 2&gt;&amp;1\n</code></pre>"},{"location":"chapters/13/#4-smoke-testing-for-interactive-sims","title":"4. Smoke Testing (for interactive sims)","text":"<p>The most thorough check is running the sim in a headless browser and verifying that no console errors appear. This requires Playwright or Puppeteer and is worth setting up for skills that generate complex interactive code.</p> <pre><code>// smoke-test.js \u2014 run with: node smoke-test.js index.html\nconst { chromium } = require('playwright');\n\n(async () =&gt; {\n  const browser = await chromium.launch();\n  const page = await browser.newPage();\n  const errors = [];\n  page.on('console', msg =&gt; {\n    if (msg.type() === 'error') errors.push(msg.text());\n  });\n  await page.goto(`file://${process.argv[2]}`);\n  await page.waitForTimeout(2000);\n  await browser.close();\n\n  if (errors.length &gt; 0) {\n    console.error('Console errors detected:');\n    errors.forEach(e =&gt; console.error('  ', e));\n    process.exit(1);\n  } else {\n    console.log('Smoke test passed \u2014 no console errors');\n  }\n})();\n</code></pre>"},{"location":"chapters/13/#multi-file-code-generation","title":"Multi-File Code Generation","text":"<p>Some code generation skills produce multiple files that must work together. A React component skill might produce:</p> <pre><code>output/\n  ConceptCard.jsx        # The component\n  ConceptCard.test.js    # Unit tests\n  ConceptCard.stories.js # Storybook story\n  index.js               # Re-export\n</code></pre> <p>The critical design principle for multi-file generation: generate a manifest first, then generate each file in sequence. This prevents the common failure mode where Claude generates file A, then in generating file B, \"forgets\" what it decided in file A, producing inconsistent code.</p> <pre><code>## Multi-File Generation Workflow\n\n### Step 1: Generate the manifest\n\nBefore writing any code, produce a JSON manifest that specifies:\n- All files to be generated with their purposes\n- All shared variables, types, and interfaces\n- The import/export graph between files\n\nWrite this manifest to manifest.json before proceeding.\n\n### Step 2: Generate each file\n\nFor each file in the manifest:\n1. Read manifest.json to recall shared decisions\n2. Generate the file\n3. Verify it imports only what the manifest specifies\n4. Write the file\n\n### Step 3: Validate cross-file consistency\n\nAfter all files are written, read each one and verify:\n- Import paths match the actual file names\n- Exported symbols match what other files import\n- No undefined references across file boundaries\n</code></pre>"},{"location":"chapters/13/#structuring-skill-workflows-for-multi-step-generation","title":"Structuring Skill Workflows for Multi-Step Generation","text":"<p>Code generation skills that produce complex output benefit from an explicit sequencing model in SKILL.md. The workflow section should not simply say \"generate a MicroSim\" \u2014 it should enumerate each step, specify what is written to disk at each step, and define what constitutes success at each step before proceeding.</p> <p>This structure serves two purposes. First, it makes the skill's behavior predictable and testable. Second, it gives Claude clear stopping points where it can verify partial work before continuing \u2014 which prevents the common failure mode of generating a large block of code, discovering a structural problem at the end, and needing to regenerate everything.</p>"},{"location":"chapters/13/#the-checkpoint-pattern","title":"The Checkpoint Pattern","text":"<pre><code>## Workflow\n\n### Step 1: Generate the sim specification\nProduce a JSON object matching the spec schema.\nWrite it to spec.json in the output directory.\nCHECKPOINT: Validate spec.json against the spec schema before continuing.\n\n### Step 2: Generate the HTML template output\nUsing spec.json, fill in all template placeholders.\nWrite to index.html.\nCHECKPOINT: Verify the HTML file size is &gt; 1KB and contains the title\nfrom spec.json before continuing.\n\n### Step 3: Generate metadata.json\nUsing spec.json, produce the metadata object.\nWrite to metadata.json.\nCHECKPOINT: Validate metadata.json against the MicroSim metadata schema.\n\n### Step 4: Report completion\nList all generated files with sizes.\nReport the overall quality score from metadata.json.\n</code></pre> <p>Checkpoints transform a single large generation task into a series of smaller, verifiable sub-tasks. If step 2 fails, the skill can retry step 2 without discarding the work from step 1. This is analogous to database transactions: commit the result of each step before starting the next.</p>"},{"location":"chapters/13/#when-not-to-use-a-code-generation-skill","title":"When Not to Use a Code Generation Skill","text":"<p>Code generation skills are powerful but not always the right tool. Three situations where a skill is the wrong approach:</p> <p>When the code is highly one-off. If a user needs a Chart.js bar chart for a specific dataset with unique styling that will never be reused, a general prompt works fine. The overhead of invoking a skill (which loads a template, runs a specification generator, validates output) is not worth it for a single use.</p> <p>When the target technology changes frequently. A skill that generates React code will become outdated as React evolves. If you cannot commit to maintaining the skill, a prompt with explicit version requirements is lower maintenance.</p> <p>When the user needs to understand the code deeply. A skill produces correct code efficiently. If the user's goal is to learn how the code works by seeing it constructed step by step, a conversational approach with explanations is more educational than a skill that delivers a finished artifact.</p> <p>Code generation skills pay off when: (1) the same type of code is generated repeatedly, (2) the structure and quality bar are consistent across all instances, and (3) speed and consistency matter more than the generative process.</p>"},{"location":"chapters/13/#summary","title":"Summary","text":"<p>Code generation skills are reliable because they constrain Claude's generative behavior to a well-defined template and quality specification. The key patterns are: use comment-placeholder or JSON-fill templates to guarantee structural consistency; generate a <code>metadata.json</code> alongside every code artifact; validate output against a schema or linter before reporting success; use checkpoints between generation steps for complex multi-file output; and for multi-file generation, produce a manifest first to anchor shared decisions. The concept-classifier skill demonstrates how a lightweight Python pre-processor, combined with Claude's semantic judgment, produces higher-quality classification than either approach alone. Code generation skills are the right investment when the same type of artifact is produced repeatedly and consistency matters more than flexibility.</p>"},{"location":"chapters/14/","title":"Chapter 14: Analysis and Validation Skills","text":"<p>Analysis skills are the quality layer of the Claude Code skill ecosystem. Where data format skills transform and code generation skills produce, analysis skills evaluate. They take an existing artifact \u2014 a Markdown file, a learning graph, a course description, a MicroSim \u2014 and return a structured assessment: scores, flags, specific recommendations, and sometimes an automated pass/fail verdict that gates further work.</p> <p>This chapter covers the design patterns for analysis skills, with deep dives into DAG validation for learning graphs, the book-metrics-generator pattern, course description assessment scoring, and the general-purpose quality gate pattern that enables skills to enforce standards before proceeding.</p>"},{"location":"chapters/14/#the-anatomy-of-an-analysis-skill","title":"The Anatomy of an Analysis Skill","text":"<p>Every analysis skill shares the same fundamental structure:</p> <ol> <li>Input acquisition \u2014 read the artifact to be evaluated</li> <li>Criteria application \u2014 compare the artifact against defined standards</li> <li>Scoring \u2014 assign numeric or categorical scores to each criterion</li> <li>Reporting \u2014 produce a structured report with scores, findings, and recommendations</li> <li>Gate decision \u2014 optionally block downstream work if scores fall below a threshold</li> </ol> <p>The critical design insight is that criteria must be explicit and measurable before the skill is written. Vague criteria like \"good quality\" or \"well-structured\" produce inconsistent scores. Concrete criteria like \"has 8 to 12 concepts per learning unit\" or \"every concept node has at least one inbound and one outbound edge\" produce consistent, actionable scores.</p> <pre><code>flowchart LR\n    A[Artifact] --&gt; B[Analysis Skill]\n    B --&gt; C[Criteria Engine]\n    C --&gt; D[Score Calculator]\n    D --&gt; E[Report Generator]\n    E --&gt; F[Score Report]\n    F --&gt; G{Gate Check}\n    G --&gt;|Pass| H[Downstream Work]\n    G --&gt;|Fail| I[Block + Recommendations]</code></pre>"},{"location":"chapters/14/#dag-validation-for-learning-graphs","title":"DAG Validation for Learning Graphs","text":"<p>A learning graph is a directed acyclic graph (DAG): concepts are nodes, prerequisites are edges, and the graph must be acyclic (no circular prerequisite chains). A DAG validation skill verifies that a learning graph file:</p> <ol> <li>Has no cycles (the graph is a true DAG)</li> <li>Has no orphan nodes (every concept is reachable from at least one entry point)</li> <li>Has no dangling references (every edge references concepts that exist as nodes)</li> <li>Has appropriate depth distribution (not all concepts at the same level)</li> <li>Has appropriate connectivity (no concepts with zero edges)</li> </ol>"},{"location":"chapters/14/#the-validation-algorithm","title":"The Validation Algorithm","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nvalidate_dag.py - Validate a learning graph JSON as a proper DAG\n\nUsage: python3 validate_dag.py &lt;learning_graph.json&gt;\n\"\"\"\n\nimport sys\nimport json\nfrom collections import defaultdict, deque\n\n\ndef load_graph(path: str) -&gt; dict:\n    with open(path) as f:\n        return json.load(f)\n\n\ndef extract_graph(data: dict):\n    \"\"\"Extract nodes and adjacency list from the learning graph format.\"\"\"\n    concepts = {c[\"id\"]: c for c in data.get(\"concepts\", [])}\n    edges = data.get(\"relationships\", [])\n    adj = defaultdict(list)\n    rev_adj = defaultdict(list)\n    for edge in edges:\n        adj[edge[\"source\"]].append(edge[\"target\"])\n        rev_adj[edge[\"target\"]].append(edge[\"source\"])\n    return concepts, edges, adj, rev_adj\n\n\ndef check_dangling_refs(concepts: dict, edges: list) -&gt; list:\n    errors = []\n    ids = set(concepts.keys())\n    for edge in edges:\n        if edge[\"source\"] not in ids:\n            errors.append(f\"Edge source '{edge['source']}' does not exist as a concept\")\n        if edge[\"target\"] not in ids:\n            errors.append(f\"Edge target '{edge['target']}' does not exist as a concept\")\n    return errors\n\n\ndef check_cycles(concepts: dict, adj: dict) -&gt; list:\n    \"\"\"Kahn's algorithm for topological sort \u2014 if we can't sort all nodes, there's a cycle.\"\"\"\n    in_degree = defaultdict(int)\n    for node in concepts:\n        in_degree[node] = in_degree.get(node, 0)\n    for node, neighbors in adj.items():\n        for n in neighbors:\n            in_degree[n] += 1\n\n    queue = deque([n for n in concepts if in_degree[n] == 0])\n    visited = 0\n    while queue:\n        node = queue.popleft()\n        visited += 1\n        for neighbor in adj[node]:\n            in_degree[neighbor] -= 1\n            if in_degree[neighbor] == 0:\n                queue.append(neighbor)\n\n    if visited &lt; len(concepts):\n        return [f\"Cycle detected: {len(concepts) - visited} concept(s) in a cycle\"]\n    return []\n\n\ndef check_orphans(concepts: dict, adj: dict, rev_adj: dict) -&gt; list:\n    warnings = []\n    for cid in concepts:\n        has_incoming = len(rev_adj.get(cid, [])) &gt; 0\n        has_outgoing = len(adj.get(cid, [])) &gt; 0\n        if not has_incoming and not has_outgoing:\n            warnings.append(f\"Isolated concept (no edges): '{cid}'\")\n    return warnings\n\n\ndef check_depth_distribution(concepts: dict, adj: dict) -&gt; dict:\n    \"\"\"Calculate depth of each node using BFS from root nodes.\"\"\"\n    roots = [n for n in concepts if not any(n in adj[src] for src in adj)]\n    depth = {n: 0 for n in roots}\n    queue = deque(roots)\n    while queue:\n        node = queue.popleft()\n        for neighbor in adj.get(node, []):\n            if neighbor not in depth:\n                depth[neighbor] = depth[node] + 1\n                queue.append(neighbor)\n\n    if not depth:\n        return {\"max_depth\": 0, \"avg_depth\": 0.0, \"distribution\": {}}\n\n    distribution = defaultdict(int)\n    for d in depth.values():\n        distribution[d] += 1\n\n    return {\n        \"max_depth\": max(depth.values()),\n        \"avg_depth\": round(sum(depth.values()) / len(depth), 2),\n        \"distribution\": dict(distribution),\n        \"uncovered_concepts\": [c for c in concepts if c not in depth]\n    }\n\n\ndef score_graph(concepts, edges, errors, warnings, depth_stats) -&gt; int:\n    \"\"\"Score 0-100 based on graph health.\"\"\"\n    score = 100\n    score -= len(errors) * 15           # Hard errors are costly\n    score -= len(warnings) * 5          # Warnings are less costly\n    if depth_stats[\"max_depth\"] &lt; 2:\n        score -= 10                     # Graph is too flat\n    if depth_stats[\"avg_depth\"] &lt; 1.5:\n        score -= 10\n    if len(depth_stats.get(\"uncovered_concepts\", [])) &gt; 0:\n        score -= 10                     # Disconnected subgraphs\n    return max(0, score)\n\n\ndef main():\n    if len(sys.argv) &lt; 2:\n        print(\"Usage: python3 validate_dag.py &lt;learning_graph.json&gt;\", file=sys.stderr)\n        sys.exit(1)\n\n    data = load_graph(sys.argv[1])\n    concepts, edges, adj, rev_adj = extract_graph(data)\n\n    errors = check_dangling_refs(concepts, edges)\n    errors += check_cycles(concepts, adj)\n    warnings = check_orphans(concepts, adj, rev_adj)\n    depth_stats = check_depth_distribution(concepts, adj)\n    score = score_graph(concepts, edges, errors, warnings, depth_stats)\n\n    report = {\n        \"valid\": len(errors) == 0,\n        \"score\": score,\n        \"concept_count\": len(concepts),\n        \"relationship_count\": len(edges),\n        \"errors\": errors,\n        \"warnings\": warnings,\n        \"depth_analysis\": depth_stats\n    }\n\n    print(json.dumps(report, indent=2))\n\n    if errors:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"chapters/14/#the-dag-validation-skill","title":"The DAG Validation Skill","text":"<pre><code>---\nname: dag-validator\ndescription: |\n  Validates a learning graph JSON file as a proper directed acyclic graph.\n  Checks for cycles, dangling references, orphan nodes, and depth distribution.\n  Produces a scored validation report.\ntriggers:\n  - /validate-dag\n  - /check-graph\n  - /dag-check\nallowed-tools:\n  - Read\n  - Bash\n  - Write\nversion: 1.1.0\n---\n\n## Workflow\n\n### Step 1: Locate the graph file\nIf the user specifies a path, use it. Otherwise, look for learning-graph.json\nin the current directory.\n\n### Step 2: Run the validator\npython3 ~/.claude/skills/dag-validator/validate_dag.py &lt;graph_path&gt;\n\n### Step 3: Interpret the results\n\nFor each error, explain what it means in plain language and suggest how to fix it.\nFor each warning, explain the impact and whether it is likely intentional.\n\n### Step 4: Report the score\n\nPresent a summary:\n- Score: X/100\n- Valid: Yes/No\n- Concepts: N, Relationships: N\n- Errors: list\n- Warnings: list\n- Depth analysis: max depth, average depth, distribution\n\n### Step 5: Recommendations\n\nIf score &lt; 70: block downstream work, list required fixes\nIf score 70-84: proceed with warnings, list suggested improvements\nIf score &gt;= 85: confirm the graph is ready for use\n\n## Scoring Thresholds\n\n| Score | Status | Action |\n|-------|--------|--------|\n| 85-100 | Excellent | Proceed |\n| 70-84 | Acceptable | Proceed with warnings |\n| 50-69 | Needs work | Fix before deploying |\n| &lt; 50 | Invalid | Do not use |\n</code></pre>"},{"location":"chapters/14/#the-book-metrics-generator-pattern","title":"The Book-Metrics-Generator Pattern","text":"<p>A book metrics generator is an analysis skill that evaluates an entire textbook or course \u2014 not just a single file. It scans all chapter files, extracts quantitative signals, and produces a metrics report that gives the author an at-a-glance view of content health.</p>"},{"location":"chapters/14/#key-metrics-to-track","title":"Key Metrics to Track","text":"Metric Description Target Range <code>avg_chapter_words</code> Average word count per chapter 2000-4000 <code>admonition_density</code> Admonitions per 1000 words 2-6 <code>code_block_density</code> Code blocks per chapter 3-8 <code>diagram_count</code> Mermaid diagrams across all chapters &gt;= 1 per 3 chapters <code>missing_chapters</code> Chapters in nav but no file exists 0 <code>orphan_files</code> Files that exist but are not in nav 0 <code>concept_coverage</code> % of glossary concepts mentioned in chapters &gt;= 80% <code>link_health</code> % of internal links that resolve 100%"},{"location":"chapters/14/#skillmd-for-book-metrics-generator","title":"SKILL.md for Book Metrics Generator","text":"<pre><code>---\nname: book-metrics-generator\ndescription: |\n  Analyzes an entire MkDocs textbook and produces a comprehensive metrics report.\n  Evaluates chapter completeness, content density, diagram usage, and concept coverage.\n  Writes a metrics.json report to the project root.\ntriggers:\n  - /book-metrics\n  - /textbook-audit\n  - /course-metrics\nallowed-tools:\n  - Read\n  - Bash\n  - Write\n  - Glob\nversion: 2.0.0\n---\n\n## Workflow\n\n### Step 1: Read mkdocs.yml\nExtract the nav structure and identify all chapters.\n\n### Step 2: Inventory files\nFor each chapter in the nav, check whether the file exists.\nList any files in docs/ that are NOT in the nav.\n\n### Step 3: Analyze each chapter file\nFor each existing chapter file:\n- Count words (approximately: split on whitespace)\n- Count admonitions (lines starting with `!!!` or `???`)\n- Count code blocks (triple-backtick blocks)\n- Count Mermaid diagrams (```mermaid blocks)\n- Extract headings structure\n- Note if the chapter has a summary section\n\n### Step 4: Run the metrics Python script\npython3 ~/.claude/skills/book-metrics-generator/metrics.py &lt;mkdocs_yml_path&gt;\n\n### Step 5: Cross-reference glossary\nRead glossary.md and extract all defined terms.\nFor each term, check whether it appears in any chapter file.\nCompute coverage percentage.\n\n### Step 6: Write the report\nWrite metrics.json to the project root.\nDisplay a summary table to the user.\n\n## Report Format\n\nThe metrics.json must include:\n- summary: overall scores per dimension\n- chapters: per-chapter statistics\n- issues: list of specific problems with file paths\n- recommendations: ordered list of highest-impact improvements\n</code></pre>"},{"location":"chapters/14/#sample-output-report","title":"Sample Output Report","text":"<pre><code>{\n  \"generated_at\": \"2026-02-20\",\n  \"project\": \"custom-skill-developer\",\n  \"summary\": {\n    \"total_chapters\": 17,\n    \"complete_chapters\": 12,\n    \"missing_chapters\": 5,\n    \"avg_word_count\": 2847,\n    \"total_diagrams\": 14,\n    \"overall_score\": 74\n  },\n  \"dimension_scores\": {\n    \"completeness\": 71,\n    \"content_density\": 82,\n    \"visual_richness\": 65,\n    \"concept_coverage\": 78\n  },\n  \"issues\": [\n    {\n      \"severity\": \"error\",\n      \"file\": \"docs/chapters/06/index.md\",\n      \"issue\": \"File does not exist\"\n    },\n    {\n      \"severity\": \"warning\",\n      \"file\": \"docs/chapters/03/index.md\",\n      \"issue\": \"Word count (847) below minimum (2000)\"\n    }\n  ],\n  \"recommendations\": [\n    \"Create missing chapter files: chapters 06, 09, 10, 11, 15\",\n    \"Add Mermaid diagrams to chapters 02, 05, 08 \u2014 currently zero diagrams\",\n    \"Expand chapter 03 \u2014 currently 847 words, target 2000+\"\n  ]\n}\n</code></pre>"},{"location":"chapters/14/#course-description-assessment-scoring","title":"Course Description Assessment Scoring","text":"<p>A course description assessment skill takes a single course description \u2014 typically a <code>course-description.md</code> file \u2014 and scores it against a rubric that reflects what makes a course description effective for its target audience.</p>"},{"location":"chapters/14/#the-scoring-rubric","title":"The Scoring Rubric","text":"<pre><code>## Course Description Scoring Rubric (100 points total)\n\n### Audience Clarity (20 points)\n- Names the target learner explicitly: 10 points\n- Specifies prerequisite knowledge: 10 points\n\n### Outcome Specificity (25 points)\n- Has at least 5 measurable learning outcomes: 15 points\n- Outcomes use action verbs (Bloom's): 10 points\n\n### Content Scope (20 points)\n- Lists major topics covered: 10 points\n- Indicates approximate time commitment: 5 points\n- Specifies delivery format (self-paced, cohort, etc.): 5 points\n\n### Differentiation (20 points)\n- States what makes this course unique: 10 points\n- References specific tools, frameworks, or methods: 10 points\n\n### Structural Quality (15 points)\n- Has a compelling opening sentence: 5 points\n- Length appropriate (300-600 words): 5 points\n- Well-organized with clear sections: 5 points\n</code></pre>"},{"location":"chapters/14/#skillmd-for-course-description-assessment","title":"SKILL.md for Course Description Assessment","text":"<pre><code>---\nname: course-description-assessor\ndescription: |\n  Scores a course description against a 100-point rubric.\n  Identifies specific gaps and provides rewrite suggestions for\n  each scoring criterion that is not fully met.\ntriggers:\n  - /assess-course-description\n  - /score-course-desc\n  - /course-audit\nallowed-tools:\n  - Read\n  - Write\nversion: 1.0.0\n---\n\n## Workflow\n\n### Step 1: Read the course description\nRead the specified file. If not specified, look for course-description.md.\n\n### Step 2: Apply the rubric\n\nFor each scoring criterion:\n1. State the criterion and its point value\n2. Assess whether the current description meets it\n3. Assign points (full, partial, or zero)\n4. Quote the specific text that earned (or failed to earn) points\n5. Provide a specific improvement if points were not fully awarded\n\n### Step 3: Calculate the total score\n\nSum all dimension scores. Compute the percentage.\n\n### Step 4: Produce the assessment report\n\nFormat:\n- Score: X/100\n- Dimension breakdown table\n- Per-criterion findings\n- Top 3 highest-impact improvements\n\n### Step 5: Offer to rewrite\n\nIf score &lt; 80, offer to rewrite the description.\nIf the user accepts, produce a revised version that would score &gt;= 88.\n\n## Scoring Standards\n\nNever award partial credit without quoting the specific text that earned it.\nNever deduct points without citing the specific missing element.\nThe rationale for every score must be reproducible \u2014 another reader of the\nsame document should arrive at the same score within \u00b15 points.\n</code></pre>"},{"location":"chapters/14/#quality-gate-patterns","title":"Quality Gate Patterns","text":"<p>Quality gates are the enforcement mechanism of analysis skills. A quality gate is a decision point where the skill either allows work to continue or blocks it, based on whether the artifact meets a minimum standard.</p>"},{"location":"chapters/14/#hard-gates-vs-soft-gates","title":"Hard Gates vs. Soft Gates","text":"<p>Hard gates block unconditionally. If the DAG has a cycle, the learning graph cannot be deployed \u2014 full stop. The skill reports the problem and stops.</p> <p>Soft gates warn and continue. If a chapter is 200 words below the minimum, the skill flags it but does not block publishing.</p> <p>The distinction in SKILL.md:</p> <pre><code>## Gate Thresholds\n\n### Hard Gates (block all downstream work)\n- Learning graph has cycles: BLOCK\n- Missing required metadata fields: BLOCK\n- Invalid JSON/YAML syntax: BLOCK\n\n### Soft Gates (warn, continue with user confirmation)\n- Chapter word count &lt; 2000: WARN\n- No diagrams in chapter: WARN\n- Score &lt; 70: WARN and request confirmation to proceed\n\n### Auto-Pass (no gate)\n- Score &gt;= 85: proceed without gate check\n</code></pre>"},{"location":"chapters/14/#building-gates-into-pipelines","title":"Building Gates into Pipelines","text":"<p>When skills are composed into pipelines (see Chapter 11), quality gates become the checkpoints that prevent bad output from propagating. A pipeline that generates MicroSims and then publishes them to a GitHub Pages site should include a quality gate between generation and publication:</p> <pre><code>flowchart LR\n    A[Generate MicroSim] --&gt; B[Quality Gate: validate-microsim]\n    B --&gt; C{Score &gt;= 75?}\n    C --&gt;|Yes| D[Add to textbook nav]\n    C --&gt;|No| E[Return to author with findings]\n    D --&gt; F[Deploy to GitHub Pages]</code></pre> <p>The gate skill reads <code>metadata.json</code> (produced by the generator skill) and verifies:</p> <ul> <li>All required metadata fields are present</li> <li><code>quality_score</code> is at or above threshold</li> <li><code>concepts</code> array is non-empty</li> <li><code>learning_outcomes</code> has at least 3 items</li> </ul>"},{"location":"chapters/14/#producing-actionable-reports","title":"Producing Actionable Reports","text":"<p>The output of an analysis skill is only as useful as the recommendations it produces. Three principles for actionable reporting:</p> <p>1. Reference specific locations. Do not write \"The chapter lacks diagrams.\" Write \"Chapter 03 (docs/chapters/03/index.md) has 0 Mermaid diagrams. Recommended: add at least 1 diagram illustrating the skill installation workflow.\"</p> <p>2. Order by impact, not by severity. Present the recommendation that will improve the score the most first, even if it is a soft warning rather than a hard error. Authors should know where to spend their effort.</p> <p>3. Include the \"fix\" alongside the \"finding.\" Every finding should be paired with a specific, executable action. Not \"improve concept coverage\" but \"Add definitions for these 4 glossary terms to the relevant chapters: token-efficiency (ch. 09), quality-gate (ch. 07), meta-skill (ch. 08), pipeline-orchestration (ch. 11).\"</p>"},{"location":"chapters/14/#summary","title":"Summary","text":"<p>Analysis and validation skills are quality infrastructure. They make implicit standards explicit, convert judgment calls into reproducible scores, and enforce quality gates that prevent bad output from propagating through a pipeline. The key patterns are: define criteria before writing the skill, produce numeric scores for every dimension, reference specific file locations in every finding, and distinguish hard gates (which block) from soft gates (which warn). The DAG validator, book metrics generator, and course description assessor are three patterns that demonstrate these principles at different scales \u2014 from a single file to an entire course.</p>"},{"location":"chapters/15/","title":"Chapter 15: Installation and Registry","text":"<p>Skills are discovered, not declared. There is no central JSON manifest, no database, no registry service that Claude Code queries to learn what skills are available. Instead, skills are discovered at session start by reading the filesystem: Claude Code scans the <code>~/.claude/skills/</code> directory, reads the <code>SKILL.md</code> file in each subdirectory, and builds a working understanding of available skills from those files.</p> <p>This architecture has important implications for how you install, manage, and think about skills. Installation is the act of making a skill visible to that filesystem scan. The filesystem is the registry.</p> <p>This chapter covers global versus project-local installation, the symlink pattern and install script, how Claude Code discovers skills at session start, the listing system, and how to manage the 30-skill limit strategically.</p>"},{"location":"chapters/15/#how-claude-code-discovers-skills","title":"How Claude Code Discovers Skills","text":"<p>At the start of every Claude Code session, the agent reads <code>~/.claude/skills/</code>. For each subdirectory it finds, it reads the <code>SKILL.md</code> file and extracts:</p> <ul> <li>The <code>name</code> field from the YAML frontmatter</li> <li>The <code>description</code> field</li> <li>The <code>triggers</code> list (slash commands and keyword phrases)</li> <li>The <code>allowed-tools</code> list</li> </ul> <p>These four pieces of information are sufficient to route a user's request to the correct skill. When a user types <code>/book-metrics</code> or says \"analyze my learning graph,\" Claude Code matches the trigger against its catalog of loaded skills and activates the matching one.</p> <p>The discovery process happens once at session start. If you install a new skill while a session is running, you need to start a new session (or reload the session context) for the new skill to be available.</p> <pre><code>sequenceDiagram\n    participant User\n    participant ClaudeCode\n    participant Filesystem\n\n    User-&gt;&gt;ClaudeCode: Start new session\n    ClaudeCode-&gt;&gt;Filesystem: Scan ~/.claude/skills/\n    Filesystem--&gt;&gt;ClaudeCode: List of subdirectories\n    loop For each subdirectory\n        ClaudeCode-&gt;&gt;Filesystem: Read SKILL.md\n        Filesystem--&gt;&gt;ClaudeCode: Name, triggers, description, tools\n    end\n    ClaudeCode-&gt;&gt;ClaudeCode: Build skill catalog\n    User-&gt;&gt;ClaudeCode: /book-metrics\n    ClaudeCode-&gt;&gt;ClaudeCode: Match trigger to catalog\n    ClaudeCode-&gt;&gt;ClaudeCode: Execute book-metrics-generator skill</code></pre>"},{"location":"chapters/15/#global-vs-project-local-installation","title":"Global vs. Project-Local Installation","text":"<p>Skills can be installed at two scopes:</p>"},{"location":"chapters/15/#global-installation-claudeskills","title":"Global Installation (<code>~/.claude/skills/</code>)","text":"<p>A globally installed skill is available in every Claude Code session, regardless of which project directory the session is started from. This is appropriate for:</p> <ul> <li>General-purpose utilities (microsim-generator, book-metrics, dag-validator)</li> <li>Skills you use across multiple projects</li> <li>Skills that belong to your personal workflow rather than a specific project</li> </ul> <p>Global installation path:</p> <pre><code>~/.claude/skills/\n  skill-name/\n    SKILL.md\n    (helper scripts, data files, etc.)\n</code></pre>"},{"location":"chapters/15/#project-local-installation-claudeskills","title":"Project-Local Installation (<code>.claude/skills/</code>)","text":"<p>A project-locally installed skill is only available when Claude Code is started from that project's directory (or a subdirectory). This is appropriate for:</p> <ul> <li>Skills that are specific to one project's conventions or data structures</li> <li>Skills that reference project-specific file paths</li> <li>Skills in active development that you do not want bleeding into other projects</li> <li>Skills that encode proprietary knowledge about a specific domain</li> </ul> <p>Project-local installation path:</p> <pre><code>/your-project/\n  .claude/\n    skills/\n      skill-name/\n        SKILL.md\n</code></pre>"},{"location":"chapters/15/#discovery-order","title":"Discovery Order","text":"<p>When both global and project-local skills exist, Claude Code loads project-local skills after global skills. If a project-local skill has the same trigger as a global skill, the project-local skill takes precedence. This allows you to override a global skill's behavior for a specific project without modifying the global installation.</p>"},{"location":"chapters/15/#the-symlink-pattern","title":"The Symlink Pattern","text":"<p>The most flexible installation architecture separates skill development from skill deployment. You develop skills in a version-controlled repository, then install them via symlinks that point from <code>~/.claude/skills/</code> to the development repository.</p>"},{"location":"chapters/15/#development-repository-structure","title":"Development Repository Structure","text":"<pre><code>~/projects/my-skills/\n  skills/\n    book-metrics-generator/\n      SKILL.md\n      metrics.py\n      README.md\n    dag-validator/\n      SKILL.md\n      validate_dag.py\n      README.md\n    microsim-generator/\n      SKILL.md\n      template.html\n      generate.py\n      README.md\n  install.sh\n  uninstall.sh\n  README.md\n  .gitignore\n</code></pre>"},{"location":"chapters/15/#the-install-script","title":"The Install Script","text":"<p>The install script creates symlinks from the development repository to <code>~/.claude/skills/</code>. Each symlink points to the entire skill directory, so when you update files in the development repository, the changes are immediately reflected in the installed skill without re-running the install script.</p> <pre><code>#!/usr/bin/env bash\n# install.sh \u2014 Install all skills via symlinks to ~/.claude/skills/\n# Usage: ./install.sh [--dry-run]\n\nset -euo pipefail\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\nSKILLS_DIR=\"${SCRIPT_DIR}/skills\"\nTARGET_DIR=\"${HOME}/.claude/skills\"\nDRY_RUN=false\n\n# Parse arguments\nfor arg in \"$@\"; do\n  case $arg in\n    --dry-run) DRY_RUN=true ;;\n    *) echo \"Unknown argument: $arg\"; exit 1 ;;\n  esac\ndone\n\n# Create the target directory if it doesn't exist\nif [ ! -d \"${TARGET_DIR}\" ]; then\n  if [ \"${DRY_RUN}\" = false ]; then\n    mkdir -p \"${TARGET_DIR}\"\n    echo \"Created directory: ${TARGET_DIR}\"\n  else\n    echo \"[dry-run] Would create: ${TARGET_DIR}\"\n  fi\nfi\n\n# Count skills for summary\ninstalled=0\nskipped=0\nupdated=0\n\n# Install each skill directory\nfor skill_dir in \"${SKILLS_DIR}\"/*/; do\n  skill_name=\"$(basename \"${skill_dir}\")\"\n  target_link=\"${TARGET_DIR}/${skill_name}\"\n\n  # Verify the skill has a SKILL.md\n  if [ ! -f \"${skill_dir}/SKILL.md\" ]; then\n    echo \"WARNING: ${skill_name}/ has no SKILL.md \u2014 skipping\"\n    ((skipped++)) || true\n    continue\n  fi\n\n  if [ -L \"${target_link}\" ]; then\n    # Symlink already exists \u2014 check if it points to the right place\n    existing_target=\"$(readlink \"${target_link}\")\"\n    if [ \"${existing_target}\" = \"${skill_dir%/}\" ]; then\n      echo \"  [ok]      ${skill_name} (already installed)\"\n      ((skipped++)) || true\n    else\n      if [ \"${DRY_RUN}\" = false ]; then\n        ln -sfn \"${skill_dir%/}\" \"${target_link}\"\n        echo \"  [updated] ${skill_name} (relinked from ${existing_target})\"\n      else\n        echo \"  [dry-run] Would relink ${skill_name}\"\n      fi\n      ((updated++)) || true\n    fi\n  elif [ -d \"${target_link}\" ]; then\n    echo \"WARNING: ${target_link} is a real directory, not a symlink \u2014 skipping\"\n    echo \"         Remove it manually if you want to install via symlink.\"\n    ((skipped++)) || true\n  else\n    if [ \"${DRY_RUN}\" = false ]; then\n      ln -s \"${skill_dir%/}\" \"${target_link}\"\n      echo \"  [new]     ${skill_name}\"\n    else\n      echo \"  [dry-run] Would install ${skill_name}\"\n    fi\n    ((installed++)) || true\n  fi\ndone\n\necho \"\"\necho \"Install complete: ${installed} new, ${updated} updated, ${skipped} unchanged/skipped\"\necho \"Skills directory: ${TARGET_DIR}\"\n</code></pre>"},{"location":"chapters/15/#the-uninstall-script","title":"The Uninstall Script","text":"<pre><code>#!/usr/bin/env bash\n# uninstall.sh \u2014 Remove skill symlinks from ~/.claude/skills/\n# Usage: ./uninstall.sh [skill-name]\n#        ./uninstall.sh --all\n\nset -euo pipefail\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\nSKILLS_DIR=\"${SCRIPT_DIR}/skills\"\nTARGET_DIR=\"${HOME}/.claude/skills\"\n\nif [ \"${1:-}\" = \"--all\" ]; then\n  for skill_dir in \"${SKILLS_DIR}\"/*/; do\n    skill_name=\"$(basename \"${skill_dir}\")\"\n    target_link=\"${TARGET_DIR}/${skill_name}\"\n    if [ -L \"${target_link}\" ]; then\n      rm \"${target_link}\"\n      echo \"Removed: ${skill_name}\"\n    fi\n  done\nelif [ -n \"${1:-}\" ]; then\n  target_link=\"${TARGET_DIR}/${1}\"\n  if [ -L \"${target_link}\" ]; then\n    rm \"${target_link}\"\n    echo \"Removed: ${1}\"\n  else\n    echo \"No symlink found for: ${1}\"\n    exit 1\n  fi\nelse\n  echo \"Usage: ./uninstall.sh [skill-name] | --all\"\n  exit 1\nfi\n</code></pre>"},{"location":"chapters/15/#the-skill-listing-system","title":"The Skill Listing System","text":"<p>Three mechanisms exist for listing available skills. Understanding all three helps you choose the right tool for each context.</p>"},{"location":"chapters/15/#1-the-skills-command","title":"1. The <code>/skills</code> Command","text":"<p>The most direct way to see what skills are loaded in the current session is to type <code>/skills</code> in the Claude Code prompt. Claude responds with a formatted list of all loaded skills, their descriptions, and their trigger phrases.</p> <p>This is the runtime view \u2014 it shows exactly what Claude Code knows about right now.</p>"},{"location":"chapters/15/#2-list-skillssh","title":"2. <code>list-skills.sh</code>","text":"<p>For shell-level inspection (outside a Claude Code session), a listing script reads the <code>~/.claude/skills/</code> directory directly and extracts key metadata from each <code>SKILL.md</code>:</p> <pre><code>#!/usr/bin/env bash\n# list-skills.sh \u2014 List all installed skills with names and descriptions\n\nSKILLS_DIR=\"${HOME}/.claude/skills\"\n\nif [ ! -d \"${SKILLS_DIR}\" ]; then\n  echo \"No skills directory found at ${SKILLS_DIR}\"\n  exit 1\nfi\n\necho \"Installed Skills\"\necho \"================\"\necho \"\"\n\nfor skill_dir in \"${SKILLS_DIR}\"/*/; do\n  skill_name=\"$(basename \"${skill_dir}\")\"\n  skill_md=\"${skill_dir}/SKILL.md\"\n\n  if [ ! -f \"${skill_md}\" ]; then\n    echo \"  ${skill_name} (no SKILL.md)\"\n    continue\n  fi\n\n  # Extract name and description from YAML frontmatter\n  # Using Python for reliable YAML parsing\n  python3 - \"${skill_md}\" &lt;&lt;'PYEOF'\nimport sys, re\n\npath = sys.argv[1]\nwith open(path) as f:\n    content = f.read()\n\n# Extract YAML frontmatter\nfm_match = re.match(r'^---\\n(.*?)\\n---', content, re.DOTALL)\nif not fm_match:\n    print(\"  (no frontmatter)\")\n    sys.exit(0)\n\nfm = fm_match.group(1)\n\nname_match = re.search(r'^name:\\s*(.+)$', fm, re.MULTILINE)\ndesc_match = re.search(r'^description:\\s*\\|?\\n?(.*?)(?=\\n\\w|\\Z)', fm, re.DOTALL)\n\nname = name_match.group(1).strip() if name_match else \"(unnamed)\"\ndesc = desc_match.group(1).strip().split('\\n')[0] if desc_match else \"(no description)\"\n\nprint(f\"  {name}\")\nprint(f\"    {desc}\")\nprint()\nPYEOF\n\ndone\n</code></pre>"},{"location":"chapters/15/#3-mcp-server-listing","title":"3. MCP Server Listing","text":"<p>When Claude Code is configured with an MCP (Model Context Protocol) server that exposes the skills directory as a resource, skills can be listed and managed through the MCP interface. This is the most advanced listing mechanism and enables programmatic skill management from external tools.</p> <p>The MCP server for skills exposes: - A resource for each installed skill (<code>skills://skill-name</code>) - A tool for listing all skills (<code>list_skills</code>) - A tool for reading a specific skill's SKILL.md (<code>read_skill</code>)</p> <p>Configuration in <code>.claude/mcp-config.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"skills\": {\n      \"command\": \"node\",\n      \"args\": [\"~/.claude/mcp-skills-server/index.js\"],\n      \"env\": {\n        \"SKILLS_DIR\": \"~/.claude/skills\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"chapters/15/#managing-the-30-skill-limit","title":"Managing the 30-Skill Limit","text":"<p>Claude Code loads skills at session start by reading all SKILL.md files in the skills directory. The practical limit for effective skill loading is approximately 30 skills \u2014 beyond this, the skill catalog occupies enough context that it competes with the actual work.</p> <p>The 30-Skill Limit is a Practical Constraint</p> <p>This is not a hard technical limit enforced by Claude Code, but a practical limit on how many skill descriptions can fit in the context window while leaving sufficient room for actual work. If you install 50 skills, all 50 are technically loaded, but the skill descriptions consume a significant fraction of the available context, reducing the quality of responses.</p>"},{"location":"chapters/15/#strategic-skill-management","title":"Strategic Skill Management","text":"<p>The goal is to maintain a set of installed skills that is as small as possible while covering your current work. Three strategies help:</p> <p>1. Profile-based installation. Maintain multiple profiles in your skills repository \u2014 one for textbook work, one for data engineering, one for job application portfolios. Each profile has its own install script that installs only the skills relevant to that context.</p> <pre><code># install-textbook-profile.sh\nTEXTBOOK_SKILLS=(\n  \"microsim-generator\"\n  \"book-metrics-generator\"\n  \"dag-validator\"\n  \"concept-classifier\"\n  \"course-description-assessor\"\n  \"learning-graph-generator\"\n  \"survey-to-chart-data\"\n)\n</code></pre> <p>2. Project-local for active development. Skills you are actively developing belong in <code>.claude/skills/</code> in your project directory, not in <code>~/.claude/skills/</code>. This prevents half-finished skills from cluttering your global catalog.</p> <p>3. Archive inactive skills. Move skills you have not used in 30+ days to an <code>archive/</code> directory within your skills repository. They remain version-controlled and can be re-installed via symlink when needed, but they do not consume context.</p>"},{"location":"chapters/15/#skill-priority-matrix","title":"Skill Priority Matrix","text":"Skill Type Install Globally? Notes Daily-use utilities Yes Always have these available Project-specific No \u2014 use .claude/skills/ Scope to the project In-development No \u2014 use .claude/skills/ Too unstable for global Domain-specific Only when working in domain Use profiles Archived / rarely used No Restore when needed"},{"location":"chapters/15/#verifying-installation","title":"Verifying Installation","text":"<p>After running the install script, verify the installation by checking the symlinks and reading one SKILL.md:</p> <pre><code># Verify symlinks are correctly created\nls -la ~/.claude/skills/\n\n# Check that a specific skill's SKILL.md is readable\ncat ~/.claude/skills/book-metrics-generator/SKILL.md | head -20\n\n# Count installed skills\nls ~/.claude/skills/ | wc -l\n</code></pre> <p>Then start a new Claude Code session and type <code>/skills</code> to confirm the skill catalog loaded correctly.</p>"},{"location":"chapters/15/#the-claudemd-file-and-skill-discovery-context","title":"The <code>CLAUDE.md</code> File and Skill Discovery Context","text":"<p>Claude Code also reads <code>~/.claude/CLAUDE.md</code> and <code>.claude/CLAUDE.md</code> in the project directory at session start. These files can provide skill-related context that does not belong in any individual SKILL.md \u2014 for example, project-wide conventions, default output directories, or which skill profile is active.</p> <p>A typical <code>CLAUDE.md</code> entry for skill context:</p> <pre><code>## Active Skill Profile\n\nThis project uses the textbook skill profile. The following skills are\ninstalled and available:\n- book-metrics-generator (/book-metrics)\n- dag-validator (/validate-dag)\n- microsim-generator (/microsim)\n\nOutput conventions for this project:\n- Generated MicroSims go in: docs/sims/\n- Metrics reports go in: reports/\n- Learning graphs go in: data/\n</code></pre> <p>This CLAUDE.md entry is not required for skill discovery \u2014 Claude Code will discover the skills through the filesystem scan regardless. But it provides useful context that helps Claude route ambiguous requests correctly when a user says \"generate a sim\" without using the exact trigger phrase.</p>"},{"location":"chapters/15/#confirming-installation-after-setup","title":"Confirming Installation After Setup","text":"<p>After running the install script, three verification steps confirm everything is working:</p> <p>Step 1: Verify the symlinks exist</p> <pre><code>ls -la ~/.claude/skills/ | grep \"^l\"\n</code></pre> <p>Every installed skill should appear as a symlink (lines starting with <code>l</code>). If a skill appears as a regular directory, it was manually copied rather than symlinked \u2014 updates to the source repository will not propagate.</p> <p>Step 2: Verify the SKILL.md is readable through the symlink</p> <pre><code>head -10 ~/.claude/skills/book-metrics-generator/SKILL.md\n</code></pre> <p>If this command fails with \"No such file or directory,\" the symlink is broken \u2014 the target directory was moved or deleted.</p> <p>Step 3: Start a new session and list skills</p> <pre><code>/skills\n</code></pre> <p>Claude Code will list all loaded skills. If an expected skill is missing from the list, return to step 1 and check the symlink.</p>"},{"location":"chapters/15/#summary","title":"Summary","text":"<p>The filesystem is the registry. Skills are discovered by reading <code>~/.claude/skills/</code> at session start \u2014 there is no manifest file to maintain, no service to query, no configuration to update. The symlink pattern separates development from deployment: you develop in a version-controlled repository and install via symlinks that update automatically when you edit files. The 30-skill practical limit requires strategic thinking about which skills are globally installed versus project-local, and profile-based install scripts let you switch between context-appropriate skill sets quickly. Verification after installation follows three steps: check symlinks with <code>ls -la</code>, verify the SKILL.md is readable through the link, and confirm discovery with <code>/skills</code> in a new session.</p>"},{"location":"chapters/16/","title":"Chapter 16: Testing and Debugging","text":"<p>Skills are not software in the traditional sense \u2014 there are no unit tests, no CI pipelines, no type checkers. A skill is a natural language document that directs Claude's behavior, and its \"correctness\" is measured by whether Claude behaves as intended when the skill is active. Testing is therefore fundamentally observational: you invoke the skill, watch what happens, and iterate.</p> <p>This chapter covers the manual testing workflow for skills, the most common failure modes and how to diagnose them, techniques for debugging with session logs, and the test-iterate-refine loop that turns a rough skill into a reliable one.</p>"},{"location":"chapters/16/#the-manual-testing-workflow","title":"The Manual Testing Workflow","text":"<p>The testing loop for a skill has five steps:</p> <ol> <li>Install the skill (or verify it is already installed via symlink)</li> <li>Start a fresh session \u2014 skills are loaded at session start</li> <li>Invoke the trigger \u2014 use the exact slash command or phrase the skill defines</li> <li>Observe the behavior \u2014 does Claude do what the SKILL.md specifies?</li> <li>Edit and repeat \u2014 update SKILL.md, restart session, try again</li> </ol> <p>The most important discipline in skill testing is starting a fresh session for each iteration. Skills are loaded once at session start. If you edit a SKILL.md while a session is running, the running session sees the old version. This is a frequent source of confusion: you fix the skill, try it again, and the problem persists \u2014 because you are still in the old session.</p> <pre><code>flowchart LR\n    A[Edit SKILL.md] --&gt; B[Start new session]\n    B --&gt; C[Invoke trigger]\n    C --&gt; D{Behaves correctly?}\n    D --&gt;|Yes| E[Test edge cases]\n    D --&gt;|No| F[Diagnose failure mode]\n    F --&gt; A\n    E --&gt; G{Edge cases pass?}\n    G --&gt;|Yes| H[Skill is ready]\n    G --&gt;|No| F</code></pre>"},{"location":"chapters/16/#test-case-design","title":"Test Case Design","text":"<p>Before testing, define what \"correct behavior\" looks like for each trigger. A test case has three parts:</p> <p>Input: The exact message or command the user will type, including any parameters.</p> <p>Expected behavior: What steps the skill should take. Not the output verbatim, but the structural behavior \u2014 which tools are called, in what order, what is written to disk, what is reported to the user.</p> <p>Pass criteria: The specific observable signals that indicate the test passed.</p>"},{"location":"chapters/16/#example-test-cases-for-book-metrics-generator","title":"Example Test Cases for <code>book-metrics-generator</code>","text":"<pre><code>Test 1: Basic invocation with no arguments\nInput: /book-metrics\nExpected: Skill reads mkdocs.yml from current directory, scans all chapter\n          files, produces metrics.json in project root, reports summary table\nPass criteria:\n  - metrics.json exists in project root after invocation\n  - Summary table shows total_chapters, avg_word_count, overall_score\n  - No Python errors in session output\n  - Execution time &lt; 30 seconds\n\nTest 2: Missing mkdocs.yml\nInput: /book-metrics (run from a directory with no mkdocs.yml)\nExpected: Skill reports \"No mkdocs.yml found\" and asks user to specify path\nPass criteria:\n  - Skill does NOT crash or produce a partial metrics.json\n  - User receives a clear error message with suggested resolution\n\nTest 3: Project with some missing chapter files\nInput: /book-metrics (run from a project where 3 chapters are in nav but missing)\nExpected: Skill reports missing chapters as errors in the issues array\nPass criteria:\n  - metrics.json includes issues array with file-not-found entries\n  - Summary shows complete_chapters count correctly excluding missing files\n  - Report is complete (does not stop early due to missing file)\n\nTest 4: Empty project (only mkdocs.yml, no chapter files)\nInput: /book-metrics\nExpected: Skill reports 0 chapters complete, 0 word count, score of 0\nPass criteria:\n  - metrics.json is valid JSON with zeroed summary fields\n  - Skill does not raise Python exceptions\n  - Recommendations section lists \"Create chapter files\" as first action\n</code></pre> <p>Designing test cases before you write the SKILL.md is a useful discipline \u2014 it forces you to specify the skill's behavior precisely, which makes the workflow section of SKILL.md easier to write.</p>"},{"location":"chapters/16/#common-failure-modes","title":"Common Failure Modes","text":""},{"location":"chapters/16/#failure-mode-1-skill-not-discovered","title":"Failure Mode 1: Skill Not Discovered","text":"<p>Symptom: You type <code>/book-metrics</code> and Claude responds as if it is a general request \u2014 it tries to improvise rather than executing the structured workflow.</p> <p>Diagnosis checklist:</p> <pre><code># 1. Is the skill directory in the right place?\nls ~/.claude/skills/book-metrics-generator/\n\n# 2. Does the SKILL.md exist?\nls ~/.claude/skills/book-metrics-generator/SKILL.md\n\n# 3. Is the trigger spelled correctly in the frontmatter?\nhead -20 ~/.claude/skills/book-metrics-generator/SKILL.md\n\n# 4. Are you in a new session? (Old sessions do not reload skills)\n# Start a new Claude Code session and try again\n\n# 5. How many skills are installed? (&gt;30 may cause context issues)\nls ~/.claude/skills/ | wc -l\n</code></pre> <p>The most common cause of skill-not-discovered is a stale session. Start a new session and try again before investigating further.</p> <p>Less common causes: - The <code>triggers</code> list in frontmatter uses wrong YAML syntax (missing dash, wrong indentation) - The trigger string contains special characters that do not match how the user typed the command - The skills directory has wrong permissions (<code>chmod 755 ~/.claude/skills/</code>) - A symlink is broken (the target directory was moved)</p> <pre><code># Check for broken symlinks\nfind ~/.claude/skills -maxdepth 1 -type l ! -exec test -e {} \\; -print\n</code></pre>"},{"location":"chapters/16/#failure-mode-2-wrong-skill-routing","title":"Failure Mode 2: Wrong Skill Routing","text":"<p>Symptom: The trigger fires, but a different skill activates \u2014 or Claude partially executes the wrong skill's workflow.</p> <p>Diagnosis: Two skills have overlapping triggers. Check for trigger conflicts:</p> <pre><code># Extract all triggers from all SKILL.md files\ngrep -h \"  - \" ~/.claude/skills/*/SKILL.md | sort | uniq -d\n</code></pre> <p>If you find duplicate triggers, either change one of the trigger phrases or use a more specific trigger for the skill that should take precedence.</p> <p>Resolution pattern: Make triggers specific to the skill's domain. Instead of <code>/analyze</code>, use <code>/analyze-learning-graph</code>. Instead of <code>/generate</code>, use <code>/generate-microsim</code>. Specificity prevents routing conflicts.</p>"},{"location":"chapters/16/#failure-mode-3-quality-gate-too-strict","title":"Failure Mode 3: Quality Gate Too Strict","text":"<p>Symptom: The skill produces output but then blocks the user with \"Score: 58/100 \u2014 below minimum threshold. Correct the following issues before proceeding.\" The issues are minor or debatable.</p> <p>Diagnosis: The scoring rubric assigns too many points to criteria that are either hard to meet or not actually important. Common examples: - Requiring a minimum word count that excludes legitimate shorter content - Requiring a specific section title that the author named differently - Counting admonitions too aggressively (penalizing chapters that are procedural and do not need callout boxes)</p> <p>Resolution: Lower the threshold or recalibrate the rubric. The rule of thumb: a gate threshold should be calibrated so that content you would be comfortable publishing passes, and content you would be embarrassed by fails. Run the scoring skill against 5-10 real artifacts you consider \"acceptable\" and set the threshold at the 20th percentile of those scores.</p>"},{"location":"chapters/16/#failure-mode-4-quality-gate-too-loose","title":"Failure Mode 4: Quality Gate Too Loose","text":"<p>Symptom: The skill passes content that is clearly incomplete or broken. A chapter with 200 words and no code blocks gets a score of 71 and proceeds.</p> <p>Diagnosis: The scoring rubric awards too many points for criteria that are easy to satisfy regardless of quality. Common examples: - Awarding 15 points simply for having a title (every file has a title) - Awarding 10 points for having any code block (even a one-liner)</p> <p>Resolution: Weight the rubric toward criteria that distinguish good content from minimal content. Word count, concept coverage, and diagram presence are better discriminators than the presence of any heading or any code block.</p>"},{"location":"chapters/16/#failure-mode-5-python-helper-fails-silently","title":"Failure Mode 5: Python Helper Fails Silently","text":"<p>Symptom: The skill's workflow says \"run transform.py\" and reports success, but the output file is wrong or missing. Claude does not notice.</p> <p>Diagnosis: The Python script exited 0 even though it failed. Check the script's error handling:</p> <pre><code># BAD \u2014 silent failure\ntry:\n    result = do_work()\n    with open(output_path, 'w') as f:\n        json.dump(result, f)\nexcept Exception:\n    pass  # This swallows all errors!\n\n# GOOD \u2014 explicit failure with non-zero exit\nimport sys\ntry:\n    result = do_work()\n    with open(output_path, 'w') as f:\n        json.dump(result, f)\nexcept Exception as e:\n    print(f\"Error: {e}\", file=sys.stderr)\n    sys.exit(1)\n</code></pre> <p>Also check that the skill's SKILL.md workflow reads the output file after running the script and validates it, rather than assuming success:</p> <pre><code>### Step 3: Run the transformation\nExecute: python3 transform.py &lt;input&gt; &lt;output&gt;\n\n### Step 4: Verify the output\nRead the output file and confirm it is valid JSON.\nIf the file is missing or empty, report the error and stop.\nDO NOT proceed to Step 5 if the output file is not valid.\n</code></pre>"},{"location":"chapters/16/#failure-mode-6-skill-produces-unexpected-output-format","title":"Failure Mode 6: Skill Produces Unexpected Output Format","text":"<p>Symptom: The skill runs and produces output, but the format differs from what the workflow specified. Field names are different, the structure is nested differently, or the output is prose instead of JSON.</p> <p>Diagnosis: The workflow instructions are ambiguous about the output format. Claude is making reasonable choices that differ from your intent.</p> <p>Resolution: Show, do not tell. Include an example of the exact output format in the SKILL.md:</p> <pre><code>## Output Format\n\nProduce a JSON object exactly matching this structure:\n\n```json\n{\n  \"score\": 84,\n  \"passed\": true,\n  \"dimensions\": {\n    \"completeness\": 90,\n    \"density\": 78,\n    \"diagrams\": 85\n  },\n  \"issues\": [],\n  \"recommendations\": []\n}\n</code></pre> <p>Do not add fields not shown above. Do not rename fields. Do not wrap the JSON in prose \u2014 output raw JSON only. <pre><code>Explicit examples eliminate ambiguity more effectively than any amount of prose description.\n\n---\n\n## Debugging with Session Logs\n\nClaude Code session logs capture every action taken during a session: tool calls, file reads, Bash executions, and Claude's reasoning. These logs are invaluable for diagnosing skill failures after the fact.\n\n### Enabling Session Logging\n\nSession logging is configured in `~/.claude/settings.json`:\n\n```json\n{\n  \"session_logging\": {\n    \"enabled\": true,\n    \"log_directory\": \"~/.claude/logs/\",\n    \"include_tool_calls\": true,\n    \"include_reasoning\": false\n  }\n}\n</code></pre></p>"},{"location":"chapters/16/#reading-a-session-log","title":"Reading a Session Log","text":"<p>A session log for a failed skill invocation might look like this (simplified):</p> <pre><code>[2026-02-20 14:23:11] User: /book-metrics\n[2026-02-20 14:23:11] Skill matched: book-metrics-generator (trigger: /book-metrics)\n[2026-02-20 14:23:12] Tool: Read \u2014 path: ./mkdocs.yml\n[2026-02-20 14:23:12] Tool result: (file content, 2847 chars)\n[2026-02-20 14:23:13] Tool: Bash \u2014 command: python3 ~/.claude/skills/book-metrics-generator/metrics.py ./mkdocs.yml\n[2026-02-20 14:23:14] Tool result: exit_code=1, stderr=\"FileNotFoundError: docs/chapters/06/index.md\"\n[2026-02-20 14:23:14] Claude response: \"I encountered an error running the metrics script...\"\n</code></pre> <p>This log shows exactly which tool failed and what error it produced. The skill's Python script exited 1 because it tried to read a chapter file that does not exist. The fix: update <code>metrics.py</code> to handle missing files gracefully rather than crashing.</p>"},{"location":"chapters/16/#diagnostic-questions-to-ask-the-log","title":"Diagnostic Questions to Ask the Log","text":"<p>When a skill fails, read the session log and answer:</p> <ol> <li>Was the correct skill triggered? (Look for \"Skill matched:\")</li> <li>Which step failed? (Find the first tool call that returned an error)</li> <li>What was the exact error message? (Read the <code>stderr</code> or <code>Tool result</code>)</li> <li>Did Claude attempt recovery? (Look for tool calls after the error)</li> <li>Did Claude report the correct error to the user? (Compare the log's final Claude response to the actual error)</li> </ol>"},{"location":"chapters/16/#the-test-iterate-refine-loop-in-practice","title":"The Test-Iterate-Refine Loop in Practice","text":"<p>A realistic debugging session for a new skill might progress through several iterations:</p> <p>Iteration 1: First invocation - Trigger fires correctly, skill loads - Python helper crashes with <code>ModuleNotFoundError: No module named 'yaml'</code> - Fix: add <code>import sys; sys.path.insert(0, ...)</code> or switch from <code>yaml</code> to <code>json</code> for config parsing - Edit SKILL.md to note the dependency, update the Python script</p> <p>Iteration 2: Second invocation - Python helper runs, output JSON is produced - But Claude reports \"4 issues found\" when there are actually 0 issues - Log shows Claude read a cached version of the output file from before the script ran - Fix: in SKILL.md workflow, add explicit instruction: \"Read the output file AFTER running the script, not before\"</p> <p>Iteration 3: Third invocation - Correct output reported for the happy path - Test edge case: empty project directory - Skill crashes \u2014 Python script throws <code>ZeroDivisionError</code> when computing average word count of 0 chapters - Fix: add guard clause in metrics.py: <code>avg = total / count if count &gt; 0 else 0</code></p> <p>Iteration 4: Fourth invocation - Happy path passes, empty project passes - Test edge case: project with circular symlinks in docs directory - Skill hangs indefinitely \u2014 <code>os.walk()</code> follows symlinks into an infinite loop - Fix: pass <code>followlinks=False</code> to <code>os.walk()</code></p> <p>Iteration 5: Fifth invocation - All test cases pass - Run against a real project for acceptance testing - Skill produces plausible metrics for a known project</p> <p>This progression \u2014 happy path, then each failure mode, then edge cases, then acceptance \u2014 is the standard pattern for validating a skill before publishing it.</p>"},{"location":"chapters/16/#edge-cases-to-test-for-every-skill","title":"Edge Cases to Test for Every Skill","text":"Edge Case What to Test Empty input Does the skill fail gracefully or crash? Missing file Does it report a clear error or throw an unhandled exception? Very large input Does it handle a 10,000-line CSV without timing out? Special characters Does it handle Unicode, paths with spaces, or single quotes? Conflicting triggers Does <code>/analyze</code> route correctly when two skills share a prefix? No permissions Does it fail gracefully if it cannot write to the output directory? Malformed JSON Does it handle corrupt input files with a useful error message? Already-exists output Does it ask before overwriting or silently overwrite?"},{"location":"chapters/16/#summary","title":"Summary","text":"<p>Testing skills is a manual, observational discipline. The workflow is: install, start a fresh session, invoke the trigger, observe, edit, repeat. The most common failures are stale sessions, overlapping triggers, miscalibrated quality gates, silent Python failures, and ambiguous output format specifications. Session logs provide a deterministic record of what happened during a failed invocation. The test-iterate-refine loop progresses from happy path to failure modes to edge cases to acceptance testing against real projects. Skills are ready for publication when they pass all documented test cases reliably across fresh sessions.</p>"},{"location":"chapters/17/","title":"Chapter 17: Publishing and Distribution","text":"<p>Building a skill for personal use and publishing it for others to use are fundamentally different activities. A personal skill can be rough around the edges \u2014 it knows your file paths, your naming conventions, your context. A published skill must be discoverable by a stranger, installable in under five minutes, and robust enough to handle environments it has never encountered.</p> <p>This chapter covers the full lifecycle of publishing a skill: packaging for sharing, GitHub-based distribution, versioning, documentation requirements, the install script pattern for a collection, license considerations, and how to build a skill collection that others will actually use.</p>"},{"location":"chapters/17/#what-publishing-means-for-a-skill","title":"What \"Publishing\" Means for a Skill","text":"<p>Publishing a skill means making it available for other Claude Code users to install and use. The primary distribution mechanism is GitHub: a repository containing one or more skills, with an install script that creates the symlinks and an uninstall script that removes them.</p> <p>There is no central registry or marketplace. Discovery happens through: - Personal recommendation and sharing - GitHub search (repository names, topics, README content) - Community lists maintained by skill authors - References in textbooks and tutorials (like this one)</p> <p>This means your README and repository description are your primary marketing. If they do not clearly communicate what the skill does and who it is for, it will not be used.</p>"},{"location":"chapters/17/#repository-structures-for-publishing","title":"Repository Structures for Publishing","text":""},{"location":"chapters/17/#single-skill-repository","title":"Single-Skill Repository","text":"<p>The simplest publishable unit is a repository that contains exactly one skill.</p> <pre><code>my-book-metrics-skill/\n  SKILL.md                  # The skill definition\n  metrics.py                # Python helper\n  validate_schema.py        # Validation helper\n  examples/\n    sample-mkdocs.yml       # Example input\n    sample-metrics.json     # Example output\n  tests/\n    test-cases.md           # Documented test cases\n  install.sh                # One-line install script\n  uninstall.sh              # One-line uninstall script\n  LICENSE                   # MIT or Apache 2.0\n  README.md                 # Required \u2014 see below\n  .gitignore\n</code></pre>"},{"location":"chapters/17/#skill-collection-repository","title":"Skill Collection Repository","text":"<p>A collection repository contains multiple related skills with a shared install script.</p> <pre><code>claude-textbook-skills/\n  skills/\n    book-metrics-generator/\n      SKILL.md\n      metrics.py\n      README.md             # Per-skill README\n    dag-validator/\n      SKILL.md\n      validate_dag.py\n      README.md\n    microsim-generator/\n      SKILL.md\n      template.html\n      generate.py\n      README.md\n    concept-classifier/\n      SKILL.md\n      classify.py\n      categories.json\n      README.md\n  install.sh                # Installs all skills or selected skills\n  install-profile.sh        # Profile-based install (textbook, data, etc.)\n  uninstall.sh\n  LICENSE\n  README.md                 # Collection overview\n  CHANGELOG.md              # Version history\n  .gitignore\n</code></pre>"},{"location":"chapters/17/#documentation-requirements-for-published-skills","title":"Documentation Requirements for Published Skills","text":"<p>A published skill needs two levels of documentation: the SKILL.md (which Claude reads) and the README.md (which humans read). These are not the same document.</p>"},{"location":"chapters/17/#skillmd-requirements-for-publication","title":"SKILL.md Requirements for Publication","text":"<p>The SKILL.md for a published skill must be more explicit than a personal skill because the author cannot assume a shared understanding of conventions:</p> <pre><code>---\nname: book-metrics-generator\ndescription: |\n  Analyzes a MkDocs textbook project and produces a structured metrics report.\n  Reads mkdocs.yml for project structure, scans all chapter files, and outputs\n  metrics.json with chapter completeness, content density, and quality scores.\ntriggers:\n  - /book-metrics\n  - /textbook-audit\n  - /course-metrics\nallowed-tools:\n  - Read\n  - Bash\n  - Write\n  - Glob\nversion: 2.1.0\nauthor: Your Name\nrepository: https://github.com/yourname/claude-textbook-skills\nrequires-python: \"&gt;=3.9\"\ndependencies: []\n---\n</code></pre> <p>Note the addition of <code>author</code>, <code>repository</code>, <code>requires-python</code>, and <code>dependencies</code> fields. These are not required by Claude Code for skill execution, but they are essential for users who need to debug installation issues or evaluate whether the skill is compatible with their environment.</p>"},{"location":"chapters/17/#the-readme-for-a-single-skill","title":"The README for a Single Skill","text":"<p>A well-structured per-skill README answers four questions in order:</p> <ol> <li>What does this skill do? (One sentence)</li> <li>Who is it for? (Target user)</li> <li>How do I install it? (Exact commands)</li> <li>How do I use it? (Invocation examples with expected output)</li> </ol> <pre><code># book-metrics-generator\n\nAnalyzes a MkDocs textbook project and produces a structured quality\nmetrics report in JSON format.\n\n## Who This Is For\n\nAuthors building intelligent textbooks with the MkDocs Material theme who want\nan objective, automated assessment of their course's completeness and quality.\n\n## Installation\n\n### Prerequisite\n\nPython 3.9 or later must be installed and available as `python3`.\n\n### Install via symlink (recommended)\n\n```bash\n# Clone the collection\ngit clone https://github.com/yourname/claude-textbook-skills.git ~/claude-skills\n\n# Run the install script\ncd ~/claude-skills &amp;&amp; ./install.sh book-metrics-generator\n</code></pre>"},{"location":"chapters/17/#manual-install","title":"Manual install","text":"<pre><code># Create the skill directory\nmkdir -p ~/.claude/skills/book-metrics-generator\n\n# Copy files\ncp skills/book-metrics-generator/* ~/.claude/skills/book-metrics-generator/\n</code></pre> <p>Start a new Claude Code session. Type <code>/book-metrics</code> to verify installation.</p>"},{"location":"chapters/17/#usage","title":"Usage","text":""},{"location":"chapters/17/#basic-invocation","title":"Basic invocation","text":"<p>From your textbook project directory:</p> <pre><code>/book-metrics\n</code></pre> <p>The skill reads <code>mkdocs.yml</code> from the current directory, scans all chapter files, and writes <code>metrics.json</code> to the project root.</p>"},{"location":"chapters/17/#specify-a-path","title":"Specify a path","text":"<pre><code>/book-metrics path/to/my-textbook/mkdocs.yml\n</code></pre>"},{"location":"chapters/17/#example-output","title":"Example output","text":"<pre><code>{\n  \"summary\": {\n    \"total_chapters\": 12,\n    \"complete_chapters\": 10,\n    \"avg_word_count\": 2847,\n    \"overall_score\": 78\n  },\n  \"issues\": [\n    {\n      \"severity\": \"warning\",\n      \"file\": \"docs/chapters/06/index.md\",\n      \"issue\": \"Word count (847) below minimum (2000)\"\n    }\n  ]\n}\n</code></pre>"},{"location":"chapters/17/#configuration","title":"Configuration","text":"<p>The skill reads scoring thresholds from <code>~/.claude/skills/book-metrics-generator/config.json</code> if present. See <code>config.json.example</code> for available settings.</p>"},{"location":"chapters/17/#troubleshooting","title":"Troubleshooting","text":"<p>\"No mkdocs.yml found\" \u2014 Run the skill from your textbook project root.</p> <p>\"ModuleNotFoundError\" \u2014 The Python helper uses only standard library modules. If you see this error, verify you are running Python 3.9+: <code>python3 --version</code></p> <p>Skill not recognized \u2014 Start a new Claude Code session. Skills are loaded at session start; editing SKILL.md in an existing session has no effect. <pre><code>### The README for a Skill Collection\n\nThe collection README is the entry point for users who find your repository. It must communicate the collection's purpose, list the included skills, and provide a clear installation path.\n\n```markdown\n# Claude Textbook Skills\n\nA collection of Claude Code skills for authors building intelligent textbooks\nwith MkDocs Material and the Dan McCreary methodology.\n\n## Included Skills\n\n| Skill | Trigger | Description |\n|-------|---------|-------------|\n| book-metrics-generator | `/book-metrics` | Audit textbook completeness and quality |\n| dag-validator | `/validate-dag` | Validate learning graphs for cycles and orphans |\n| microsim-generator | `/microsim` | Generate p5.js interactive simulations |\n| concept-classifier | `/classify-concept` | Classify concepts by cognitive type |\n| survey-to-chart-data | `/survey-to-chart` | Convert survey CSV to Chart.js data |\n\n## Quick Install\n\n```bash\ngit clone https://github.com/yourname/claude-textbook-skills.git ~/claude-skills\ncd ~/claude-skills &amp;&amp; ./install.sh\n</code></pre></p> <p>Start a new Claude Code session and type <code>/skills</code> to see all installed skills.</p>"},{"location":"chapters/17/#requirements","title":"Requirements","text":"<ul> <li>Claude Code (any recent version)</li> <li>Python 3.9+ available as <code>python3</code></li> <li>macOS or Linux (Windows via WSL)</li> </ul>"},{"location":"chapters/17/#install-individual-skills","title":"Install Individual Skills","text":"<pre><code>./install.sh book-metrics-generator\n./install.sh dag-validator\n</code></pre>"},{"location":"chapters/17/#profiles","title":"Profiles","text":"<p>Install only the skills you need:</p> <pre><code>./install-profile.sh textbook    # book-metrics, dag-validator, microsim-generator\n./install-profile.sh data        # survey-to-chart-data, concept-classifier\n</code></pre>"},{"location":"chapters/17/#contributing","title":"Contributing","text":"<p>See CONTRIBUTING.md for guidelines on adding skills to this collection. <pre><code>---\n\n## Versioning Conventions\n\nSkills use semantic versioning with three components: `MAJOR.MINOR.PATCH`.\n\n| Component | When to increment | Example |\n|-----------|-------------------|---------|\n| MAJOR | Breaking change to the skill's interface (changed trigger names, changed output schema) | 1.x.x \u2192 2.0.0 |\n| MINOR | New capability added, backwards-compatible (new trigger, new optional parameter) | x.1.x \u2192 x.2.0 |\n| PATCH | Bug fix, documentation improvement, performance improvement | x.x.1 \u2192 x.x.2 |\n\nThe version field in SKILL.md frontmatter must match the version in CHANGELOG.md and in any Git tags.\n\n### CHANGELOG.md\n\nMaintain a CHANGELOG.md in the collection root using the Keep a Changelog format:\n\n```markdown\n# Changelog\n\nAll notable changes to this skill collection are documented here.\n\n## [2.1.0] - 2026-02-20\n\n### Added\n- book-metrics-generator: Added `diagram_density` metric\n- book-metrics-generator: New `/textbook-audit` trigger alias\n\n### Fixed\n- dag-validator: Handle empty graph (0 concepts) without crashing\n- concept-classifier: Improve classification confidence for procedural concepts\n\n## [2.0.0] - 2026-01-15\n\n### Breaking Changes\n- book-metrics-generator: Output schema changed \u2014 `quality_score` field\n  renamed to `overall_score`. Update any downstream scripts that read metrics.json.\n\n### Added\n- New skill: survey-to-chart-data\n\n## [1.3.2] - 2025-12-01\n\n### Fixed\n- dag-validator: Correctly detect cycles in graphs with self-referencing nodes\n</code></pre></p>"},{"location":"chapters/17/#git-tagging","title":"Git Tagging","text":"<p>Tag releases in Git so users can install a specific version:</p> <pre><code>git tag -a v2.1.0 -m \"Add diagram density metric, fix empty graph crash\"\ngit push origin v2.1.0\n</code></pre> <p>Users can then clone a specific version:</p> <pre><code>git clone --branch v2.1.0 https://github.com/yourname/claude-textbook-skills.git\n</code></pre>"},{"location":"chapters/17/#the-install-script-for-a-collection","title":"The Install Script for a Collection","text":"<p>The collection install script must handle several cases cleanly: installing all skills, installing a single skill by name, dry-run mode for previewing what will happen, and updating existing symlinks when the repository is pulled.</p> <pre><code>#!/usr/bin/env bash\n# install.sh \u2014 Install Claude Code skills from this collection\n# Usage:\n#   ./install.sh              \u2014 Install all skills\n#   ./install.sh skill-name   \u2014 Install one skill\n#   ./install.sh --list       \u2014 List available skills\n#   ./install.sh --dry-run    \u2014 Preview without making changes\n\nset -euo pipefail\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\nSKILLS_SRC=\"${SCRIPT_DIR}/skills\"\nSKILLS_DEST=\"${HOME}/.claude/skills\"\nDRY_RUN=false\nLIST_ONLY=false\nSPECIFIC_SKILL=\"\"\n\n# Parse arguments\nfor arg in \"$@\"; do\n  case $arg in\n    --dry-run) DRY_RUN=true ;;\n    --list) LIST_ONLY=true ;;\n    --help|-h)\n      echo \"Usage: ./install.sh [skill-name] [--dry-run] [--list]\"\n      exit 0\n      ;;\n    -*) echo \"Unknown flag: $arg\"; exit 1 ;;\n    *) SPECIFIC_SKILL=\"$arg\" ;;\n  esac\ndone\n\n# List mode\nif [ \"${LIST_ONLY}\" = true ]; then\n  echo \"Available skills:\"\n  for d in \"${SKILLS_SRC}\"/*/; do\n    name=\"$(basename \"$d\")\"\n    if [ -f \"${d}/SKILL.md\" ]; then\n      desc=$(python3 -c \"\nimport re, sys\nwith open('${d}/SKILL.md') as f: c = f.read()\nm = re.search(r'description:\\s*\\|?\\n?\\s*(.+)', c)\nprint(m.group(1).strip() if m else '(no description)')\n\" 2&gt;/dev/null || echo \"(no description)\")\n      printf \"  %-35s %s\\n\" \"$name\" \"$desc\"\n    fi\n  done\n  exit 0\nfi\n\n# Create destination directory\n[ -d \"${SKILLS_DEST}\" ] || { [ \"${DRY_RUN}\" = false ] &amp;&amp; mkdir -p \"${SKILLS_DEST}\"; }\n\ninstall_skill() {\n  local skill_name=\"$1\"\n  local src=\"${SKILLS_SRC}/${skill_name}\"\n  local dest=\"${SKILLS_DEST}/${skill_name}\"\n\n  if [ ! -d \"${src}\" ]; then\n    echo \"ERROR: Skill '${skill_name}' not found in ${SKILLS_SRC}\"\n    return 1\n  fi\n\n  if [ ! -f \"${src}/SKILL.md\" ]; then\n    echo \"WARNING: ${skill_name}/ has no SKILL.md \u2014 skipping\"\n    return 0\n  fi\n\n  if [ -L \"${dest}\" ]; then\n    if [ \"${DRY_RUN}\" = false ]; then\n      ln -sfn \"${src}\" \"${dest}\"\n      echo \"  [updated] ${skill_name}\"\n    else\n      echo \"  [dry-run] Would update symlink: ${skill_name}\"\n    fi\n  elif [ -d \"${dest}\" ]; then\n    echo \"  [skip]    ${skill_name} (real directory exists at destination \u2014 manual install?)\"\n  else\n    if [ \"${DRY_RUN}\" = false ]; then\n      ln -s \"${src}\" \"${dest}\"\n      echo \"  [new]     ${skill_name}\"\n    else\n      echo \"  [dry-run] Would create symlink: ${skill_name}\"\n    fi\n  fi\n}\n\n# Install specific skill or all skills\nif [ -n \"${SPECIFIC_SKILL}\" ]; then\n  install_skill \"${SPECIFIC_SKILL}\"\nelse\n  echo \"Installing all skills to ${SKILLS_DEST}...\"\n  for d in \"${SKILLS_SRC}\"/*/; do\n    install_skill \"$(basename \"$d\")\"\n  done\nfi\n\necho \"\"\necho \"Done. Start a new Claude Code session to activate installed skills.\"\n[ \"${DRY_RUN}\" = true ] &amp;&amp; echo \"(Dry run \u2014 no changes made)\"\n</code></pre>"},{"location":"chapters/17/#license-considerations","title":"License Considerations","text":"<p>Skills are code and documentation combined. The license you choose affects how others can use, modify, and redistribute your skills.</p> License Use when... MIT You want maximum adoption. Anyone can use, modify, fork, and sell derivatives with minimal restriction. Apache 2.0 Similar to MIT but includes explicit patent protection. Good choice if your skills implement novel methods. CC BY 4.0 Skills that are primarily documentation rather than code. Requires attribution. CC BY-SA 4.0 Requires that derivatives use the same license. Good for community-oriented skill collections. Proprietary Skills that encode competitive advantage. Do not publish to GitHub without careful consideration. <p>For most open skill collections, MIT is the appropriate choice: it maximizes adoption and places minimal burden on users.</p> <p>The LICENSE file must be at the repository root. GitHub will display it prominently.</p>"},{"location":"chapters/17/#building-a-skill-collection","title":"Building a Skill Collection","text":"<p>A skill collection is more than a directory of SKILL.md files. A well-built collection has a coherent purpose, clear quality standards across all skills, shared helper utilities, and a governance model that allows it to grow without losing consistency.</p>"},{"location":"chapters/17/#defining-collection-scope","title":"Defining Collection Scope","text":"<p>Before adding skills to a collection, define what the collection is for. A collection with a clear scope is easier to discover and easier to trust:</p> <ul> <li>Textbook skills \u2014 skills for authors using MkDocs Material and intelligent textbook patterns</li> <li>Data engineering skills \u2014 skills for CSV/JSON transformation, schema validation, pipeline automation</li> <li>Analysis skills \u2014 skills for scoring and auditing content quality</li> <li>MicroSim skills \u2014 skills for generating interactive educational simulations</li> </ul> <p>Resist the temptation to add a skill to a collection just because it is convenient. A \"textbook skills\" collection that also includes a Slack message formatter and a Docker compose generator is not a textbook skills collection.</p>"},{"location":"chapters/17/#shared-utilities","title":"Shared Utilities","text":"<p>Skills in a collection often share utility code: JSON validation, frontmatter parsing, file discovery patterns. Factor these into shared utility scripts:</p> <pre><code>skills/\n  _shared/\n    frontmatter.py     # Parse YAML frontmatter from Markdown\n    validate_json.py   # Validate JSON against a schema file\n    file_utils.py      # Common file discovery patterns\n</code></pre> <p>Each skill that uses shared utilities imports from <code>_shared/</code>:</p> <pre><code>import sys\nfrom pathlib import Path\n\n# Add shared utilities to path\nsys.path.insert(0, str(Path(__file__).parent.parent / '_shared'))\nfrom frontmatter import parse_frontmatter\nfrom validate_json import validate_against_schema\n</code></pre>"},{"location":"chapters/17/#collection-quality-standards","title":"Collection Quality Standards","text":"<p>Document the standards that all skills in the collection must meet. This serves as both a contribution guide and a self-imposed quality bar:</p> <pre><code>## Collection Quality Standards\n\nAll skills in this collection must meet these criteria before merging:\n\n### Required Files\n- [ ] SKILL.md with complete frontmatter (name, description, triggers, allowed-tools, version)\n- [ ] README.md with installation instructions and usage examples\n- [ ] At least one example input and expected output in examples/\n\n### Code Quality\n- [ ] Python helpers exit non-zero on failure\n- [ ] Python helpers validate output before writing to disk\n- [ ] No hardcoded absolute paths \u2014 use Path(__file__).parent for relative paths\n- [ ] All file paths that may contain spaces are quoted in shell commands\n\n### Testing\n- [ ] At least 3 documented test cases in tests/test-cases.md\n- [ ] Happy path test passes\n- [ ] Empty/missing input test passes\n- [ ] Manual testing confirmed in a clean Claude Code session\n\n### Documentation\n- [ ] SKILL.md workflow has numbered steps\n- [ ] Error handling behavior is documented for all failure modes\n- [ ] Version is incremented correctly per semantic versioning rules\n</code></pre>"},{"location":"chapters/17/#summary","title":"Summary","text":"<p>Publishing a skill means making it work for someone who does not know your conventions, your file structure, or your context. The requirements are: a README that answers \"what, who, how to install, how to use\"; a SKILL.md with explicit author, version, and dependency fields; semantic versioning with a CHANGELOG; an install script that handles all common installation scenarios; and a clear license. Skill collections add a shared scope definition, shared utility code, and documented quality standards that ensure consistency across all included skills. The filesystem-based distribution model via GitHub symlinks is simple, reliable, and makes it trivially easy to update skills by pulling the repository.</p>"},{"location":"sims/cognify-ecosystem/","title":"Cognify Skill Ecosystem","text":"<p>Explore how 19 production Agent Skills connect across the Cognify business intelligence pipeline. Click any node to see its outputs, upstream dependencies, and downstream consumers.</p>      Open Fullscreen    21 nodes \u00a0\u00b7\u00a0 28 edges \u00a0\u00b7\u00a0 6 phases \u00a0\u00b7\u00a0 vis-network <p> </p>"},{"location":"sims/cognify-ecosystem/#what-this-sim-shows","title":"What This Sim Shows","text":"<p>The graph maps the complete Cognify skill ecosystem \u2014 every skill that powers the ABM methodology from market discovery through ongoing operations. Nodes are colored by pipeline phase:</p> Phase Color Skills Discovery Blue market-intel-generator, market-sizing-calculator, competitor-mapper Analysis Purple buyer-persona-builder, pain-point-analyzer, decision-mapper Strategy Cyan value-prop-generator, positioning-matrix, pricing-strategist Execution Orange outreach-sequencer, content-personalizer, objection-handler, crm-field-mapper Operations Green pipeline-tracker, win-loss-analyzer, territory-planner, campaign-optimizer, roi-calculator, qbr-generator Ecosystem White Custom Skill Developer (this guide), Skill Quality Analyzer <p>Edges show data flow \u2014 which skill's output feeds into which skill's input. This is pipeline orchestration in practice: skills don't call each other directly, they chain through file contracts.</p>"},{"location":"sims/cognify-ecosystem/#how-to-read-the-graph","title":"How to Read the Graph","text":"<ul> <li>Click a node to see its detail panel: description, outputs, and connection counts</li> <li>Use phase filter buttons at the top to isolate pipeline stages</li> <li>Drag nodes to rearrange the layout</li> <li>Scroll to zoom in and out</li> <li>Stats bar at the bottom shows totals for nodes, edges, and phases</li> </ul> <p>Architecture insight</p> <p>The graph reveals a clear funnel pattern: Discovery skills feed Analysis, which feeds Strategy, which feeds Execution, which feeds Operations. The Ecosystem layer (this guide + the analyzer) sits outside the funnel \u2014 it's meta-tooling that improves the skills themselves.</p> <p>Related</p> <p>Chapter 8 covers meta-skill routing patterns. Chapter 11 covers pipeline orchestration \u2014 the technique that makes this ecosystem work. The Skill Quality Analyzer scores each node against a 100-point rubric.</p>"},{"location":"sims/quality-score-calculator/","title":"Skill Quality Score Calculator","text":"<p>Use the sliders below to score your Agent Skill across seven dimensions. The total score updates in real time along with a letter grade and a verdict. Load a preset to see how a minimal, good, or production-ready skill compares.</p>      Open Fullscreen    <p> </p>"},{"location":"sims/quality-score-calculator/#scoring-dimensions","title":"Scoring Dimensions","text":"Dimension Max Points What It Checks Frontmatter Compliance 20 Required YAML fields: name, description, version, author, category, tags Structure Compliance 20 All required top-level sections present and in order Workflow Quality 15 Step clarity, error handling paths, logical flow Quality Scoring System 15 Internal success metrics, rubrics, self-evaluation criteria Token Efficiency 10 Concise prompts \u2014 no bloat, repetition, or over-specification Ecosystem Quality 10 Tool declarations, cross-skill integration, discovery metadata Cross-Platform Readiness 10 Works in Claude.ai, Claude Code CLI, and API without modification"},{"location":"sims/quality-score-calculator/#grade-scale","title":"Grade Scale","text":"Score Grade Verdict 85 \u2013 100 A Production ready. Ship it. 70 \u2013 84 B Solid foundation. Address the gaps before publishing. 55 \u2013 69 C Needs work. Focus on the lowest-scoring dimensions. 40 \u2013 54 D Significant gaps. Review the guide chapters for each weak area. 0 \u2013 39 F Start with Chapter 3: Your First Skill."},{"location":"sims/routing-decision-tree/","title":"Meta-Skill Routing Simulator","text":"<p>When a meta-skill receives a request, it does not load every reference file into context. It inspects the request, extracts keywords, and routes to exactly one guide \u2014 loading only what is needed.</p> <p>This simulator lets you see that decision process in real time.</p>        \u26f6 Open fullscreen"},{"location":"sims/routing-decision-tree/#how-it-works","title":"How it works","text":"<p>The meta-skill router follows three steps on every invocation:</p> <ol> <li> <p>Keyword extraction. The request is tokenized and stop words are removed. The remaining terms are matched against each route's keyword list.</p> </li> <li> <p>Route selection. The reference file with the most keyword hits is selected. All other guides are skipped entirely.</p> </li> <li> <p>Selective loading. Only the matched guide is injected into context before Claude generates its response.</p> </li> </ol>"},{"location":"sims/routing-decision-tree/#the-routing-table","title":"The routing table","text":"Keywords Reference file Library chart, bar, line, pie, donut <code>chartjs-guide.md</code> Chart.js timeline, dates, chronological <code>timeline-guide.md</code> vis-timeline network, graph, nodes, edges <code>vis-network-guide.md</code> vis-network map, geographic, location <code>map-guide.md</code> Leaflet flowchart, sequence, diagram <code>mermaid-guide.md</code> Mermaid simulation, physics, animation <code>p5-guide.md</code> p5.js venn, overlap, sets <code>venn-guide.md</code> Venn.js"},{"location":"sims/routing-decision-tree/#why-token-efficiency-matters","title":"Why token efficiency matters","text":"<p>Loading all seven reference guides on every invocation would cost roughly 5,600 tokens of context before Claude writes a single line of code. The router loads one guide \u2014 typically 650\u2013900 tokens \u2014 and skips the rest. That is an 85\u201390% reduction in reference overhead, which translates directly to faster responses and lower API costs at scale.</p>"},{"location":"sims/skill-anatomy-explorer/","title":"Skill Anatomy Explorer","text":"<p>Explore the structure of a <code>SKILL.md</code> file interactively. Click any section block in the diagram to reveal what that section does, whether it is required, its approximate token cost, and a real example.</p>      Open Fullscreen    9 sections \u00a0\u00b7\u00a0 5 required, 4 optional \u00a0\u00b7\u00a0 3-tier token model <p> </p>"},{"location":"sims/skill-anatomy-explorer/#what-this-sim-shows","title":"What This Sim Shows","text":"<p>The explorer maps the nine standard sections of a production <code>SKILL.md</code> file to a color-coded diagram. Each section is tagged with:</p> <ul> <li>Required vs. Optional \u2014 five sections are required for Claude Code to execute a skill reliably; four are best-practice additions</li> <li>Token cost estimate \u2014 the approximate tokens each section contributes to context</li> <li>Type \u2014 whether the section is structural metadata, instructional content, or a quality gate</li> </ul> <p>The file tree in the top-left panel shows where <code>SKILL.md</code> lives relative to the other directories in a skill package (<code>scripts/</code>, <code>references/</code>, <code>assets/</code>).</p>"},{"location":"sims/skill-anatomy-explorer/#the-3-tier-token-model","title":"The 3-Tier Token Model","text":"<p>The bottom strip of the sim visualizes the progressive disclosure design pattern for skills:</p> Tier When Loaded What It Contains Cost Tier 1 \u2014 Metadata Every session YAML frontmatter + H1 title ~100 tokens Tier 2 \u2014 Instructions On invocation Full SKILL.md body &lt; 5,000 tokens Tier 3 \u2014 Resources On demand Files in <code>references/</code> and <code>assets/</code> As needed <p>This design keeps skills lightweight in the global skill registry while making full workflow detail available exactly when Claude needs it.</p>"},{"location":"sims/skill-anatomy-explorer/#sections-at-a-glance","title":"Sections at a Glance","text":"Section Color Required Tokens YAML Frontmatter Blue Yes ~100 H1 Title White Yes ~5 Overview / Purpose Purple Yes ~80 When to Activate Green Yes ~120 Workflow Steps Orange Yes ~1,200 Output Files Summary Blue Yes ~150 Example Session Purple No ~300 Common Pitfalls Orange No ~200 Quality Scoring Rubric Green No ~400 <p>Design principle</p> <p>The five required sections are the minimum contract between the skill and Claude Code. The four optional sections are what separate a working skill from a production-quality skill. Add them when the skill will be used by others or invoked frequently.</p> <p>Related</p> <p>Chapter 4 covers the full SKILL.md structure in depth. Chapter 7 covers quality scoring rubric design. Chapter 9 covers token budget management and the progressive disclosure pattern.</p>"}]}